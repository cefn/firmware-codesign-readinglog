<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>fd973bec006ea0a0bf65c04c03890c23faac270ad76c1a35f836115d34fcd43a</job>
    <base_name>62ns</base_name>
    <doi>10.1016/j.datak.2004.12.008</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <region class="unknown" id="1">Data &amp; Knowledge Engineering 55 (2005) 327–342 www.elsevier.com/locate/datak</region>
      <title-group>
        <article-title class="DoCO:Title" id="2">What do the pictures mean? Guidelines for experimental evaluation of representation fidelity in diagrammatical conceptual modeling techniques</article-title>
      </title-group>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="3">Jeffrey Parsons a</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="4">Linda Cole</name>
        </contrib>
      </contrib-group>
      <region class="unknown" id="5">b</region>
      <region class="unknown" id="6">a Faculty of Business Administration, Memorial University of Newfoundland, St. John’s, NL, Canada A1B 3X5 b QueenÕs School of Business, QueenÕs University, Kingston, ON, Canada K7L 3N6 Received 14 December 2004; accepted 14 December 2004 Available online 8 January 2005</region>
      <abstract class="DoCO:Abstract" id="7">It is important to articulate the objectives and underlying assumptions behind a growing body of experimental research in conceptual modeling. We provide four guidelines for developing materials for experiments that evaluate conceptual modeling techniques, under the assumption that a primary purpose of conceptual modeling is to facilitate communication between analysts and users in validating domain knowledge during systems development. These guidelines assist in developing experimental materials that support meaningful tests of domain semantics. We present empirical evidence indicating the value of two of the guidelines. We also evaluate selected recent experiments on conceptual modeling with respect to the guidelines. Ó 2005 Elsevier B.V. All rights reserved.</abstract>
      <region class="unknown" id="10">Keywords: Conceptual modeling; Experimental design; UML * Corresponding author. E-mail addresses: <email id="8">jeffreyp@mun.ca</email> (J. Parsons), <email id="9">lmcole27@yahoo.ca</email> (L. Cole). 0169-023X/$ - see front matter Ó 2005 Elsevier B.V. All rights reserved. doi:10.1016/j.datak.2004.12.008</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="11">328</outsider>
      <outsider class="DoCO:TextBox" type="header" id="12">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="13" page="2" column="1">1. Introduction</h1>
        <region class="DoCO:TextChunk" id="22" page="2" column="1">Hundreds of methods and techniques for conceptual modeling in systems analysis and design have been proposed [<xref ref-type="bibr" rid="R19" id="14" class="deo:Reference">19</xref>], although few have seen widespread use. One criticism of modeling techniques is that they lack strong theoretical foundations [<xref ref-type="bibr" rid="R24" id="15" class="deo:Reference">24</xref>, <xref ref-type="bibr" rid="R25" id="16" class="deo:Reference">25</xref>]. As a result, it is difficult to understand if and why methods are useful in supporting specific objectives. This challenge has generated a recent resurgence of interest in research aimed at empirically evaluating modeling techniques, particularly those using diagrammatic notations to convey semantics of a real world domain. Broadly speaking, empirical evaluation on conceptual modeling techniques falls under the scope of a growing body of empirical research in software engineering (e.g., [<xref ref-type="bibr" rid="R12" id="17" class="deo:Reference">12</xref>, <xref ref-type="bibr" rid="R13" id="18" class="deo:Reference">13</xref>, <xref ref-type="bibr" rid="R23" id="19" class="deo:Reference">23</xref>, <xref ref-type="bibr" rid="R27" id="20" class="deo:Reference">27</xref>]). That body of work has tended to deal primarily with adapting empirical methods used in the social sci- ences for use in software engineering contexts, and with providing guidance to improve the quality of empirical software engineering research, which has often been carried out by researchers without adequate background in empirical methods and data analysis techniques. Adopting a specific focus on conceptual modeling, Wand and Weber [<xref ref-type="bibr" rid="R25" id="21" class="deo:Reference">25</xref>] proposed a framework for evaluating aspects of conceptual modeling approaches. Their framework clearly delin- eates four targets of evaluation: grammars, methods, scripts, and context. The Wand and Weber framework provides valuable guidance on the general structure of experimental (and other) studies to evaluate conceptual modeling methods. This paper provides a complementary set of guidelines for developing experimental materials and measures appropriate for research questions that assess the suitability of conceptual modeling techniques for expressing domain semantics.</region>
      </section>
      <section class="deo:Motivation">
        <h1 class="DoCO:SectionTitle" id="23" page="2" column="1">2. Motivation</h1>
        <region class="DoCO:TextChunk" id="42" page="2" column="1">Throughout this discussion, attention is limited to the use of methods for conceptual modeling (capturing semantics of the real world relevant to an information system). We adopt the position of Wand and Weber [ <xref ref-type="bibr" rid="R24" id="24" class="deo:Reference">24</xref>] that conceptual models represent aspects of the real world, as perceived by humans. Wand and Weber use this premise to argue for the suitability of ontology (the branch of philosophy dealing with what exists), and in particular the ontology of Mario Bunge [<xref ref-type="bibr" rid="R2" id="25" class="deo:Reference">2</xref>], as a basis for identifying what needs to be modeled to achieve ontological fidelity in a method. Using that approach, ontological deficits and overloads can be isolated and predictions made about the empirical impacts of these problems. Several experimental studies (e.g., [<xref ref-type="bibr" rid="R4" id="26" class="deo:Reference">4</xref>, <xref ref-type="bibr" rid="R5" id="27" class="deo:Reference">5</xref>, <xref ref-type="bibr" rid="R9" id="28" class="deo:Reference">9</xref>, <xref ref-type="bibr" rid="R26" id="29" class="deo:Reference">26</xref>]) have used this foundation to make and test predictions about methods. The premise adopted by Wand and Weber can be used to explore further the rationale for con- structing a model. Various researchers have identified that one of the major purposes of conceptual models is to facilitate communication between clients (or users) and analysts (or developers) (e.g., [<xref ref-type="bibr" rid="R10" id="30" class="deo:Reference">10</xref>, <xref ref-type="bibr" rid="R11" id="31" class="deo:Reference">11</xref>, <xref ref-type="bibr" rid="R14" id="32" class="deo:Reference">14</xref>, <xref ref-type="bibr" rid="R20" id="33" class="deo:Reference">20</xref>]). Analysts construct a (typically diagrammatic) conceptual model to document their understanding of a domain as developed through meetings with clients, examination of documents, and other activities. The model allows users to verify whether or not analyst interpretations reflect reality as perceived by users. From this perspective, research to evaluate modeling methods should focus on their capacity to facilitate this verification. Specifically, domain semantics expressed in scripts constructed using different grammars (or versions) may be easier or harder to comprehend based on whether or not the grammars (or versions of a grammar) adhere to<marker type="page" number="3"/><marker type="block"/> certain theoretical (e.g., ontological) precepts. Options for evaluation include: assessing the ability of developers to construct models that capture requirements (ÔwriteÕ tasks), and assessing the ability of readers of models to extract information contained in them (ÔreadÕ tasks) [<xref ref-type="bibr" rid="R25" id="37" class="deo:Reference">25</xref>]. We focus here on laboratory experiments that involve ÔreadÕ tasks. In addition, research can be classified according to whether comparisons are between different modeling formalisms (intergrammar and multi-grammar) or use different approaches to developing scripts (typically guided by some theoretical basis for alternate forms of representation) within versions of a single formalism (intragrammar) [<xref ref-type="bibr" rid="R25" id="38" class="deo:Reference">25</xref>]. Notably, recent research on ontological analysis of modeling methods has investigated, using an intragrammar approach, whether and how adherence to ontological principles affects the ability of a model to communicate information (e.g., [<xref ref-type="bibr" rid="R3" id="39" class="deo:Reference">3</xref>, <xref ref-type="bibr" rid="R4" id="40" class="deo:Reference">4</xref>, <xref ref-type="bibr" rid="R9" id="41" class="deo:Reference">9</xref>]). In the following, we focus on issues in intragrammar comparisons using alternate scripts (diagrams) motivated by different theoretical considerations.</region>
        <outsider class="DoCO:TextBox" type="header" id="35" page="3" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="36" page="3" column="1">329</outsider>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="43" page="3" column="1">3. Assumptions</h1>
        <region class="DoCO:TextChunk" id="54" page="3" column="1">The guidelines we propose are intended to support experiments that adhere to two fundamental assumptions about the role of experimentation in generating scientific knowledge. First is the assumption that experiments are conducted to determine the relationship between one or more independent variables and one or more dependent variables. Generally, this means controlling for the impact of other variables that might affect the dependent variables, either by ensuring homogeneity among participants on many ÔbackgroundÕ variables, or measuring background variables and accounting for their effects during data analysis. In other words, the primary advantage of experimentation over other empirical research methods is the high level of internal validity it affords (ensuring that variation in the dependent variables results from variation in the independent variable(s)). This sometimes comes at the expense of external validity, or generalization of results to settings different from those in the laboratory. Experimentation has recently been proposed as a needed, but underutilized strategy in empirical software engineering [ <xref ref-type="bibr" rid="R12" id="44" class="deo:Reference">12</xref>, <xref ref-type="bibr" rid="R27" id="45" class="deo:Reference">27</xref>]. In other empirical disciplines, discussion over the appropriate role of experimentation in knowledge creation is well-established. For example, in the academic marketing discipline, an active discussion known as the Calder–Lynch dialogue (after the main proponents), took place in the early 1980s [<xref ref-type="bibr" rid="R6" id="46" class="deo:Reference">6</xref>, <xref ref-type="bibr" rid="R7" hidden="1" id="47" class="deo:Reference">7</xref>, <xref ref-type="bibr" rid="R8" id="48" class="deo:Reference">8</xref>, <xref ref-type="bibr" rid="R15" id="49" class="deo:Reference">15</xref>, <xref ref-type="bibr" rid="R16" id="50" class="deo:Reference">16</xref>]. That debate centered on the relative importance of internal validity and external validity in experiments. In the context of that discussion, our guidelines are predicated on a view close to Calder et al. Namely, we believe the primary strength of experimental research is internal validity and that threats to internal validity interfere with the ability to draw conclusions about the theoretical propositions being tested in the experiment. We further believe that, in the absence of internal validity, external validity is of limited value. Second, we believe that an understanding of simple, theoretical causal relationships (involving one or a few independent variables) is needed before adopting more complex research designs. We further believe that current theoretical knowledge of effective conceptual modeling grammars is limited, due to the relatively small amount of prior theory-based experimental work in this area. In the context of the Calder–Lynch dialogue, this means focusing (in a single study) on the core variables of interest. Moreover, these variables should be chosen based on their role in the theory being tested, rather than introduced in an ad hoc fashion. Over time, as basic relationships are<marker type="page" number="4"/><marker type="block"/> understood, the theory can be adapted to account for additional factors that lead to a more complete model of complex phenomena.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="52" page="4" column="1">330</outsider>
        <outsider class="DoCO:TextBox" type="header" id="53" page="4" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="55" page="4" column="1">4. Guidelines</h1>
        <region class="DoCO:TextChunk" id="69" page="4" column="1">The guidelines consist of four criteria that should be addressed when making choices about key parameters of experimental design––independent variables, dependent measures, participants, and procedures––for experiments comparing the effectiveness of alternate intragrammar scripts in communicating domain semantics. Unlike existing guidelines for empirical software engineering research, which focus on more basic issues such as appropriate use of data analysis techniques (e.g., [ <xref ref-type="bibr" rid="R13" id="56" class="deo:Reference">13</xref>]), our guidelines deal with specific issues in the choice of independent variables, dependent variables, tasks, and participants. We justify the guidelines, but do not claim that the list is exhaustive. First, alternative scripts generated to manipulate one or more independent variables (e.g., con- formance or non-conformance to some ontological guidelines) should be informationally equivalent with respect to the dependent variable(s) [<xref ref-type="bibr" rid="R11" id="57" class="deo:Reference">11</xref>]. That is, it should be possible to answer questions correctly with any of the representational forms used as treatments in an experimental study. Otherwise, internal validity is threatened, since differences in information content may confound attempts to measure differences in comprehension of alternate semantically equivalent representations. To illustrate, if one form provides enough information to answer selected questions correctly, while a second form does not, it would not be surprising to find that participants receiving the first form outperform those receiving the second form on those questions. Put simply, if one set of materials contains the answers to certain questions while a second set does not, we can only expect the former group to perform better (or no worse) than the latter on those questions. Information equivalence between diagrams need not be total, but must apply with respect to questions constituting the dependent measures. This does not mean that information inequivalence cannot be a variable of interest in conceptual modeling experiments. In contrast, it may well be that different grammars (or versions of a grammar) derived from a theoretical base may lead to informationally inequivalent diagrams when used as intended. However, in our view such differences and their implications (e.g., in terms of the quality of conceptual models that can be constructed using the grammar) are best examined in diagram construction (write) tasks, rather than diagram interpretation (read) tasks. Second, dependent variables should measure performance only with respect to semantics contained in the script(s). Therefore, questions should be limited to those that test comprehension of explicit semantics expressed in a script. Other kinds of questions, notably the problem solving questions used by Gemino [<xref ref-type="bibr" rid="R9" id="58" class="deo:Reference">9</xref>] and Bodart et al. [<xref ref-type="bibr" rid="R1" id="59" class="deo:Reference">1</xref>], are intended to measure a deeper level of domain understanding in which information in a script is incorporated into an existing knowledge schema [<xref ref-type="bibr" rid="R17" id="60" class="deo:Reference">17</xref>, <xref ref-type="bibr" rid="R18" id="61" class="deo:Reference">18</xref>]. Therefore, such questions address other, more complex, issues than communicating/vali- dating domain semantics expressed in a script. Since there is presently not a good understanding of how to effectively represent domain semantics in a script, such dependent measures are not consistent with our assumption regarding the need to understand simple causal relationships before studying more complex questions. In the next section, we present an example that illustrates this guideline and its implications in more detail.<marker type="page" number="5"/><marker type="block"/> Third, when selecting participants, subject matter experts (SMEs) should not be used. This rec- ommendation sacrifices external validity, since a central role of conceptual models to facilitate communication between analysts and users (SMEs). However, given the research objective of assessing the degree to which (and ease with which) information can be extracted from a script constructed according to different theoretically motivated rules, it is critical that participants can answer questions by using only that script, rather than by using background knowledge. Using SMEs to test the capacity of scripts to convey semantics clearly can confound experimental results and thereby threaten internal validity. To illustrate, if we construct a script related to the enrollment of students in courses at a university and conduct an experiment with student subjects (SMEs in that setting), there is no way of distinguishing whether answers are based on information provided in the scripts, or simply on the participantsÕ experience at their own university. This can improperly weaken or strengthen the effects of the treatment, depending on whether or not the participantsÕ knowledge is consistent with the semantics in the script. Clearly, tests involving SMEs are needed in order to provide external validity to fully understand issues in using conceptual modeling techniques to communicate and verify requirements. However, such tests are most valuable after a reasonably comprehensive and controlled understanding of how different representations facilitate or inhibit the expression of semantics has been developed. We contend that such understanding does not yet exist, and that the design of some prior studies may have inhibited the development of cumulative knowledge in this area. Fourth, in experimental procedures for studies testing the representation capacity of alternative forms of representation, scripts should be available to participants as they answer questions. Given that we are interested in testing the ability of the diagrams to convey semantics (rather than, say, ease of memorizing diagram content), there is no reason to remove diagrams in experiments. Moreover, since scripts would be available in a real-world communication/validation situation, there is some external validity to this protocol. Some have advocated removing diagrams on the basis that an objective of studies may be to test learning [<xref ref-type="bibr" rid="R1" id="65" class="deo:Reference">1</xref>, <xref ref-type="bibr" rid="R11" id="66" class="deo:Reference">11</xref>]. Those studies were based on the work of Mayer [<xref ref-type="bibr" rid="R17" id="67" class="deo:Reference">17</xref>, <xref ref-type="bibr" rid="R18" id="68" class="deo:Reference">18</xref>], whose interest lay primarily in understanding if and how different kinds of diagrammatic models (e.g., showing a machineÕs parts versus showing a machineÕs parts and its steps of operation) help in learning about a domain in the sense of incorporating knowledge from a model into existing knowledge structures. However, the objective of testing learning is difficult to reconcile with the role of conceptual models in facilitating communication with users/ clients who know the domain, and would not learn from reviewing diagrams. Testing learning may lead to important future uses for conceptual models, but we believe our understanding of how methods convey domain semantics remains sketchy to the point that considerable work is needed in understanding more basic issues first. We think it is incumbent on researchers who advocate the removal of diagrams prior to taking measures of dependent variables to offer a strong rationale for that protocol.</region>
        <outsider class="DoCO:TextBox" type="header" id="63" page="5" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="64" page="5" column="1">331</outsider>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="70" page="5" column="1">5. Applying the guidelines</h1>
        <region class="DoCO:TextChunk" id="92" page="5" column="1">In this section, we illustrate how failing to adhere to the first three of the four guidelines in Sec- tion 4 can interfere with the results obtained in experimental studies. <marker type="page" number="6"/><marker type="block"/>  <xref ref-type="fig" rid="F1" id="74" class="deo:Reference">Fig. 1</xref> contains two alternative UML class diagram segments illustrating possible ways of depicting associations or relationships. <xref ref-type="fig" rid="F1(a)" id="75" class="deo:Reference">Fig. 1(a)</xref> attaches an association class (with attributes) to an association between two entity classes. <xref ref-type="fig" rid="F1(b)" id="76" class="deo:Reference">Fig. 1(b)</xref> depicts two unrelated associations between the same classes. These representations are not informationally equivalent, according to the first of our four guidelines. Thus, a question such as ‘‘Is the chair of a committee a member of that committee?’’ can be answered correctly (given appropriate generally accepted rules for reading diagrams) strictly from the information provided in <xref ref-type="fig" rid="F1(a)" id="77" class="deo:Reference">Fig. 1(a)</xref>, but not (using the same rules) from the information in <xref ref-type="fig" rid="F1(b)" id="78" class="deo:Reference">Fig. 1(b)</xref>. If (a) and (b) were contained in treatments given to two experimental groups, it would not be surprising or interesting to find that the group given <xref ref-type="fig" rid="F1(a)" id="79" class="deo:Reference">Fig. 1(a)</xref> outperformed the group given <xref ref-type="fig" rid="F1(b)" id="80" class="deo:Reference">Fig. 1(b)</xref> with respect to this or similar questions. The second guideline is illustrated via Figs. 1 and 2 jointly: namely, questions should be re- stricted to those that can be answered from the semantics conveyed by the diagram. Using <xref ref-type="fig" rid="F1(b)" id="81" class="deo:Reference">Fig. 1(b)</xref> to illustrate, consider a problem solving question such as ‘‘From the information in the diagram, what options are available to a committee if the chair resigns?’’ Answering such a question in a meaningful way depends on interpreting the words in a diagram based on background knowledge. Simply replacing the relevant terms with terms from <xref ref-type="fig" rid="F2" id="82" class="deo:Reference">Fig. 2</xref> shows that such questions cannot test the semantics conveyed by the diagram structure (what options are available to a beta if a delta association between an alpha and a beta is terminated?). Figs. 1 and 2 also illustrate the importance of eschewing the use of subject matter experts in tests of the ability of alternate forms to convey semantics (the third of our guidelines). One could argue plausibly that almost any participant in an experimental study is a sufficient SME to answer a question such as ‘‘Is the chair of a committee a member of that committee?’’ simply by applying a common sense interpretation of the words, regardless of whether this semantics is conveyed by the diagram structure (in <xref ref-type="fig" rid="F1" id="83" class="deo:Reference">Fig. 1</xref>). This could mitigate any effect of the treatment (i.e., the impact of differences in the way semantics are conveyed by the diagram structure). A simple way to eliminate potential confounds associated with using SMEs is simply to replace semantically laden<marker type="page" number="7"/><marker type="block"/> words with non-words, as in <xref ref-type="fig" rid="F2" id="91" class="deo:Reference">Fig. 2</xref>. By doing that, prior domain knowledge cannot interfere with the interpretation of the semantics conveyed in the representation embodied by the diagram. For instance, domain knowledge cannot be used to correctly guess an answer to the question ‘‘Is the delta of a beta also a gamma of that beta?’’ Other approaches used in prior studies have either manipulated or measured prior knowledge to some degree.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="72" page="6" column="1">332</outsider>
        <outsider class="DoCO:TextBox" type="header" id="73" page="6" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <region class="unknown" id="85" page="6" column="1">member of Associate Committee (a) 1..* 0.. Membership Chair: [Y/N] Datejoined: Date 1..* member of 0..* (b) Associate Committee 1 chair of 0..*</region>
        <region class="DoCO:FigureBox" id="F1">
          <caption class="deo:Caption" id="86" page="6" column="1">Fig. 1. Information inequivalence.</caption>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="87" page="7" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="88" page="7" column="1">333</outsider>
        <region class="unknown" id="89" page="7" column="1">1..* gamma of 0..* Alpha Beta 1 delta of 0..*</region>
        <region class="DoCO:FigureBox" id="F2">
          <caption class="deo:Caption" id="90" page="7" column="1">Fig. 2. Removing the influence of prior knowledge.</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="93" page="7" column="1">6. Empirical evidence</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="94" page="7" column="1">6.1. General issues</h2>
          <region class="DoCO:TextChunk" id="98" page="7" column="1">We collected data on the effect of following or violating the second and third guidelines in our framework as part of a larger study examining the effects of alternate ways of representing the ontological concept of property precedence on UML diagram comprehension (additional information about property precedence is given in [<xref ref-type="bibr" rid="R22" id="95" class="deo:Reference">22</xref>] and the complete study design is provided in [<xref ref-type="bibr" rid="R21" id="96" class="deo:Reference">21</xref>]). We report here only the results relevant to assessing these guidelines. To assess the second guideline, we asked questions dealing with comprehension of diagram semantics, as well as problem solving questions of the type used by Gemino [<xref ref-type="bibr" rid="R9" id="97" class="deo:Reference">9</xref>]. To assess the third guideline, we used alternate forms of the experimental material. In the first, we used English words from a domain with which participants had some familiarity (subject matter expertise). In the second version, we substituted Greek letters so that participants were able to answer questions only on the semantics conveyed by the diagram structure (no subject matter expertise).</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="99" page="7" column="1">6.2. Task domain and material</h2>
          <region class="DoCO:TextChunk" id="128" page="7" column="1">Since the objectives of this study were very focused, we decided not to test entire diagrams. In- stead, we used two diagram segments. <xref ref-type="fig" rid="F3" id="100" class="deo:Reference">Fig. 3</xref> contains one of the diagram segments used. <xref ref-type="fig" rid="F3" id="101" class="deo:Reference">Fig. 3</xref> models logical associations (flight and reservation) between physical objects (passengers and<marker type="page" number="8"/><marker type="block"/> aircraft). The second segment in the study dealt with an application involving reserving tables for customers at a restaurant. To test whether background knowledge interfered with the communication of diagram semantics, we constructed alternative representations for which the class and association labels were ‘‘semantically void.’’ <xref ref-type="fig" rid="F4" id="107" class="deo:Reference">Fig. 4</xref> contains the segment corresponding to that in <xref ref-type="fig" rid="F3" id="108" class="deo:Reference">Fig. 3</xref>. In <xref ref-type="fig" rid="F4" id="109" class="deo:Reference">Fig. 4</xref>, answers to any questions about the diagrams can be based only on the semantics conveyed via the constructs and structure of the diagram, whereas answers to questions about the segment in <xref ref-type="fig" rid="F3" id="110" class="deo:Reference">Fig. 3</xref> might be influenced by background knowledge about the general subject domain. The dependent variables in the study consisted of: (1) the number of correct answers to questions about the semantics conveyed in a diagram segment, and (2) the number of ‘‘reasonable’’ answers to problem solving questions based on the diagram segments. <xref ref-type="table" rid="T1" id="111" class="deo:Reference">Table 1</xref> contains the comprehension questions asked regarding the diagram segments in <xref ref-type="fig" rid="F3" id="112" class="deo:Reference">Fig. 3</xref>. <xref ref-type="table" rid="T2" id="113" class="deo:Reference">Table 2</xref> contains the problem solving questions for the same diagram segments. The ‘‘reasonableness’’ of an answer to a problem solving question was judged with respect to whether a participant provided an answer that made sense given the diagram segment. We propose two research questions based on the guidelines above. First, does an experimental treatment produce differential effects in comprehension versus problem-solving questions, as<marker type="page" number="9"/><marker type="block"/> found in earlier studies (e.g., [<xref ref-type="bibr" rid="R1" id="127" class="deo:Reference">1</xref>])? Second, are responses to questions affected by background knowledge of a domain as primed by the labels (words) used on a diagram, rather than by the semantics conveyed by the diagram structure? The experimental materials consisted of four diagram segments: two subject domains (restaurant and airline) and alternate representation of the concept of ontological precedence in which we were interested (ÔgoodÕ and ÔbadÕ), each with a series of questions. In addition, we also varied the use of segments that carried semantics in the words used in class and association labels (ÔSemanticÕ segments) with the use of segments that used Greek letters as labels and were there- fore semantically void (ÔVoidÕ segments). Two versions of the material were developed. In version A, the ordering of segments was: Void-Bad; Void-Good; Semantic-Good; Semantic-Bad. In version B, the ordering of segments was: Void-Good; Void-Bad; Semantic-Bad; Semantic- Good. In both versions, the Void cases were presented first to eliminate the potential influence of semantics conveyed by the use of words on subsequent interpretation of semantically void segments.</region>
          <region class="unknown" id="103" page="7" column="1">Passenger Reservation Flight Aircraft 1..* 1..* 1..* 1..* 1..* 1..1 • Name • Seat No. • Flight No. • Aircraft ID • DoB • Food Req • Dept_Airport • Type • Aeroplan # • Arr_Airport • Capacity • Date • Dept_Time • Arr_Time</region>
          <region class="DoCO:FigureBox" id="F3">
            <caption class="deo:Caption" id="104" page="7" column="1">Fig. 3. Sample diagram segment used in the study.</caption>
          </region>
          <outsider class="DoCO:TextBox" type="page_nr" id="105" page="8" column="1">334</outsider>
          <outsider class="DoCO:TextBox" type="header" id="106" page="8" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
          <region class="unknown" id="115" page="8" column="1">Chi Alpha Gamma Theta 1..* 1..* 1..* 1..* 1..* 1..1</region>
          <region class="DoCO:FigureBox" id="F4">
            <caption class="deo:Caption" id="117" page="8" column="1">Fig. 4. Semantically void equivalent of <xref ref-type="fig" rid="F3" id="116" class="deo:Reference">Fig. 3</xref>.</caption>
          </region>
          <region class="unknown" id="124" page="8" column="1"> <xref ref-type="table" rid="T1" id="118" class="deo:Reference">Table 1</xref> Sample comprehension questions Semantic segment (<xref ref-type="fig" rid="F3" id="119" class="deo:Reference">Fig. 3</xref>) Semantically void segment (<xref ref-type="fig" rid="F4" id="120" class="deo:Reference">Fig. 4</xref>) Can a passenger have a reservation with an Can a Chi be associated with an Alpha that does not aircraft that does not involve a specific flight? involve a specific Gamma? Does the diagram specify an upper limit on the Does the diagram specify an upper limit on the number number of reservations associated with a specific flight? of Alphas associated with a specific Gamma? <xref ref-type="table" rid="T2" id="121" class="deo:Reference">Table 2</xref> Sample problem-solving questions Semantic segment (<xref ref-type="fig" rid="F3" id="122" class="deo:Reference">Fig. 3</xref>) Semantically void segment (<xref ref-type="fig" rid="F4" id="123" class="deo:Reference">Fig. 4</xref>) Under what circumstances would a passenger fail Under what circumstances would a Chi fail to board to board the aircraft associated with the reserved the Theta associated with the alpha-ed Gamma? List flight? List all of the reasons can you think of for this all of the reasons can you think of for this How would a decision by the airline has to standardize How would a decision by the Lambda has to the types of aircraft it flies affect passengers and their standardize the types of Thetas it has affect Chis and reservations? Give as many examples as you can think of their Alphas? Give as many examples as you can think of</region>
          <outsider class="DoCO:TextBox" type="header" id="125" page="9" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
          <outsider class="DoCO:TextBox" type="page_nr" id="126" page="9" column="1">335</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="129" page="9" column="1">6.3. Participants and procedure</h2>
          <region class="DoCO:TextChunk" id="130" page="9" column="1">Eighty undergraduate business students with no information systems experience participated in this study. Students were recruited via solicitation in an introductory marketing course. A short training session introduced the basics of class diagram notation, and students participated in a self-test of their understanding of the concepts covered. After the self-test, the administrator re- viewed the answers to the questions with the participants. Next, each participant was randomly assigned to version A or B (as described above) of the material. Participants were told the purpose of the study was to test how people read information from a certain type of diagram used in information systems development. Booklets containing instructions, the diagram segments, and questions for each segment, were distributed. Participants were given unlimited time to complete the exercise and the administrator collected each booklet as it was completed.</region>
        </section>
      </section>
      <section class="deo:Results">
        <h1 class="DoCO:SectionTitle" id="131" page="9" column="1">7. Results</h1>
        <region class="DoCO:TextChunk" id="151" page="9" column="1">To test whether problem solving questions produce different patterns of answers than comprehension questions, we compared dependent measures across treatments using independent sam- ples t-tests. <xref ref-type="table" rid="T3" id="132" class="deo:Reference">Table 3</xref> shows a general pattern of support for ÔgoodÕ representations (those following a particular ontological prescription) over ÔbadÕ representations (those violating the prescription) on two types of comprehension questions. Six of eight dependent measures show support in the direction hypothesized in the primary study. In particular, for both examples, participants shown the ontologically ÔgoodÕ representation outperformed those shown the ÔbadÕ representation on questions related to the existence of precedence (p &lt; 0.001 in three cases, p = 0.04 in the remaining case). On questions related to cardinalities involving preceding mutual properties, participants shown the ÔgoodÕ representation outperformed those shown the ÔbadÕ representation for the Airline example in both the Semantic and Void conditions (p &lt; 0.01). There was no statistical difference between groups on the Restaurant example.<marker type="page" number="10"/><marker type="block"/>  <xref ref-type="table" rid="T4" id="139" class="deo:Reference">Table 4</xref> shows corresponding results for problem solving questions. The scores show the mean number of ‘‘reasonable’’ suggestions generated under varying conditions. The pattern of signifi- cance is quite different from that in <xref ref-type="table" rid="T3" id="140" class="deo:Reference">Table 3</xref>. In particular, the treatment had no substantive effect on the number of correct answers to problem-solving questions. In only one of the eight cases (questions about the possible effects of standardization in the Airline example), participants in the ontologically ÔbadÕ group outperformed those in the ÔgoodÕ group (p = 0.004). These results confirm that comprehension and problem-solving questions can produce different patterns of results, as had been found in earlier research [<xref ref-type="bibr" rid="R1" id="141" class="deo:Reference">1</xref>, <xref ref-type="bibr" rid="R9" id="142" class="deo:Reference">9</xref>]. More importantly, they indicate that effects of a treatment on the semantics conveyed by diagram structure (as demonstrated by comprehension questions) can be masked when problem solving questions are asked. Next, we examined evidence in relation to the third guideline—that subject matter experts should not be used. In our study, participants could bring background (common-sense) knowledge to bear in answering questions about both the restaurant and airline examples when English labels were used on the diagram segments. However, when Greek letters were used, participants had to rely solely on semantics conveyed by the diagram structure and symbols. Thus, by looking<marker type="page" number="11"/><marker type="block"/> at differences in responses to comprehension questions between Semantic and Void materials, we can determine the extent to which the use of words may invoke background knowledge and affect responses. Table 5 contains the t-test results of this comparison. The top half of Table 5 shows that, under the ontologically ÔgoodÕ form of representation, the presence or absence of English words that could affect answers has little impact on the results (in other words, the use of Greek letters does not seem to have a substantive negative effect on scores). In only one of the four cases (existence of precedence in the Restaurant example) did participants shown the Semantic segments outperform those shown the Void segments (p = 0.02). However, under the ÔbadÕ form of representation, in three of the four cases the use of English words (ÔsemanticÕ version) leads to significantly higher scores than when Greek letters are used. Specifically, in the Restaurant example, both existence (p = 0.02) and cardinality (p = 0.01) questions; in the Airline example, existence questions (p = 0.01). This means participants receiving semantically void materials are unable to use background knowledge to answer questions correctly. However, the difference between Semantic and Void segments is significant only when the ontological representation is ÔbadÕ (in that case, the words help). This suggests participants<marker type="page" number="12"/><marker type="block"/> are using knowledge primed by the words in the diagram, rather than the semantics conveyed by the diagramÕs structure, to answer the questions. Table 6 contains t-test results analogous to those of Table 5, using problem solving questions as dependent measures. As can be seen, participants scored consistently higher on the ÔsemanticÕ segments than on the ÔvoidÕ segments (p &lt; 0.001 in all eight cases). However, unlike the results for the comprehension questions in Table 5, there is no difference in the pattern between the version of material that followed the ontological prescription and the version that violated the prescription.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="134" page="10" column="1">336</outsider>
        <outsider class="DoCO:TextBox" type="header" id="135" page="10" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <region class="unknown" id="138" page="10" column="1"> <xref ref-type="table" rid="T3" id="136" class="deo:Reference">Table 3</xref> Results for diagram comprehension questions Diagram Question type Semantics Mean SD Sig. segment Good Bad Good Bad Restaurant Existence of precedence [one question] Semantic 0.93 0.60 0.267 0.496 &lt;0.001 Cardinalities related to preceding mutual 2.38 2.38 0.838 0.806 n.s. properties [three questions] Airline Existence of precedence [two questions] 1.28 0.98 0.679 0.800 0.04 Cardinalities related to preceding mutual 0.80 0.55 0.405 0.504 &lt;0.01 properties [one question] Restaurant Existence of precedence [one question] Void 0.75 0.38 0.44 0.49 &lt;0.001 Cardinalities related to preceding mutual 2.10 1.93 0.900 0.949 n.s. properties [three questions] Airline Existence of precedence [two questions] 1.33 0.60 0.701 0.391 &lt;0.001 Cardinalities related to preceding mutual 0.83 0.58 0.385 0.501 &lt;0.01 properties [one question] <xref ref-type="table" rid="T4" id="137" class="deo:Reference">Table 4</xref> Results for problem solving questions Diagram segment Problem solving question Semantics Mean SD Sig. Good Bad Good Bad Restaurant Reasons for being late Semantic 2.50 2.48 1.664 1.853 n.s. Ways of increasing capacity 1.53 1.65 1.176 1.122 n.s. Airline Reasons for missing association 2.23 2.27 1.833 1.723 n.s. Effect of standardization 0.88 1.53 0.723 1.176 0.004 Restaurant Reasons for being late Void 0.18 0.23 0.501 0.660 n.s. Ways of increasing capacity 0.28 0.38 0.506 0.586 n.s. Airline Reasons for missing association 0.13 0.18 0.404 0.446 n.s Effect of standardization 0.08 0.08 0.267 0.267 n.s.</region>
        <outsider class="DoCO:TextBox" type="header" id="144" page="11" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="145" page="11" column="1">337</outsider>
        <region class="unknown" id="146" page="11" column="1">Table 5 Comprehension results for semantic vs. void diagram segments Diagram Question type Representation Mean SD Sig. segment Semantic Void Semantic Void Restaurant Existence of precedence [one question] Good 0.93 0.75 0.267 0.439 0.02 Cardinalities related to preceding mutual 2.38 2.10 0.838 0.900 n.s. properties [three questions] Airline Existence of precedence [two questions] 1.28 1.33 0.679 0.701 n.s. Cardinalities related to preceding mutual 0.80 0.83 0.405 0.385 n.s. properties [one question] Restaurant Existence of precedence [one question] Bad 0.60 0.38 0.496 0.490 0.02 Cardinalities related to preceding mutual 2.38 1.93 0.807 0.944 0.01 properties [three questions] Airline Existence of precedence [two questions] 0.98 0.60 0.800 0.591 0.01 Cardinalities related to preceding mutual 0.55 0.58 0.503 0.500 n.s. properties [one question]</region>
        <region class="unknown" id="148" page="11" column="1">Table 6 Problem solving results for semantic vs. void diagram segments Diagram segment Problem solving question Representation Mean SD Sig. Semantic Void Semantic Void Restaurant Reasons for being late Good 2.50 0.18 1.664 0.501 &lt;0.001 Ways of increasing capacity 1.53 0.28 1.176 0.506 &lt;0.001 Airline Reasons for missing association 2.23 0.13 1.833 0.404 &lt;0.001 Effect of standardization 0.88 0.08 0.723 0.267 &lt;0.001 Restaurant Reasons for being late Bad 2.48 0.23 1.853 0.660 &lt;0.001 Ways of increasing capacity 1.65 0.38 1.122 0.586 &lt;0.001 Airline Reasons for missing association 2.17 0.18 1.723 0.446 &lt;0.001 Effect of standardization 1.53 0.08 1.176 0.267 &lt;0.001</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="149" page="12" column="1">338</outsider>
        <outsider class="DoCO:TextBox" type="header" id="150" page="12" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
      </section>
      <section class="deo:Discussion">
        <h1 class="DoCO:SectionTitle" id="152" page="12" column="1">8. Discussion</h1>
        <region class="DoCO:TextChunk" id="154" page="12" column="1">Based on a review of prior experimental research on conceptual modeling techniques and the assumption that it is necessary to gain deeper understanding of how humans extract real-world semantics from modeling formalisms, we proposed four criteria that we argue should be followed in such research. The first criterion restates Gemino and WandÕs [<xref ref-type="bibr" rid="R11" id="153" class="deo:Reference">11</xref>] claim that information equivalence is critical in studies that compare alternate formalisms. The second states that questions should focus on measuring comprehension (semantics conveyed by modeling constructs in a script). We showed empirically how, for a given manipulation, the pattern of results can differ for comprehension questions versus problem solving questions that rely on reasoning beyond information contained in a script (by using background knowledge associated with the domain embed- ded in the words attached to script elements). In our example, a treatment that produced significant support for the value of an ontologically ÔgoodÕ representation in comprehension questions produced no effect for problem solving questions. We speculate this is because participants either used background knowledge of the domain to answer the questions (when shown the Semantic segments) or were unable to answer the questions effectively (when shown the Void segments). The third criterion states that subject matter experts should not be used in studies. We showed empirically that domain familiarity can affect results under some treatment conditions. In particular, an ontologically ÔgoodÕ representation neutralizes the effect of domain familiarity on measures of comprehension (compared to the effect of domain familiarity on measures of comprehension given an ontologically ÔbadÕ representation). The final criterion states that scripts should be available to participants during a study, since developing a good understanding of how humans extract information from scripts should precede studies of how easy or difficult it is to learn (remember) information from a script that is no longer available to review.</region>
      </section>
      <section class="deo:Evaluation">
        <h1 class="DoCO:SectionTitle" id="155" page="12" column="1">9. Evaluation of some previous research using the framework</h1>
        <region class="DoCO:TextChunk" id="156" page="12" column="1">Table 7 presents a selection of relevant prior experimental research in conceptual modeling. These studies were chosen because each deals with evaluation of some ontologically derived predictions about effective conceptual modeling practices. Each study addresses intragrammar comparisons of scripts based on alternate forms of representation (the independent variable), and assesses the ability of users to use the representation in various ways (the dependent variables). In general, we found a high level of variability in the extent to which the studies adhered to our guidelines.</region>
        <outsider class="DoCO:TextBox" type="header" id="157" page="13" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="158" page="13" column="1">339</outsider>
        <region class="DoCO:TextChunk" id="166" confidence="possible" page="13" column="1">Table 7 Evaluation of selected prior research using the framework References Task Information Answerable Potential impact Models equivalence from script of prior knowledge a available Bodart et al. [<xref ref-type="bibr" rid="R1" id="159" class="deo:Reference">1</xref>] E1: Reconstruct diagram n/a b High 2 Domains N E2: Comprehension test High High High and Low N E3: Problem solving test High Low N Burton-Jones and Meso [<xref ref-type="bibr" rid="R3" id="160" class="deo:Reference">3</xref>] Problem solving test Moderate Moderate Moderate Y Cloze test Moderate High Y Burton-Jones and Weber [<xref ref-type="bibr" rid="R4" id="161" class="deo:Reference">4</xref>] Comprehension test Unknown c Moderate 2 Domains Y Problem solving Moderate High and Low N Burton-Jones and Weber [<xref ref-type="bibr" rid="R5" id="162" class="deo:Reference">5</xref>] Comprehension test Moderate High High Y Gemino and Wand [<xref ref-type="bibr" rid="R10" id="163" class="deo:Reference">10</xref>] Comprehension test High Low Y Problem solving test Unknown c Low N Cloze test Moderate N Parsons [<xref ref-type="bibr" rid="R20" id="164" class="deo:Reference">20</xref>] Comprehension test High High None Y Weber [<xref ref-type="bibr" rid="R26" id="165" class="deo:Reference">26</xref>] Reconstruct diagram n/a b High Moderate N a Scores on this criterion were subjectively assigned by one of the authors (e.g., university domain = high, con- tracting = moderate, plant nursery = low, non-words = none). b Information equivalence not applicable in diagram reconstruction task. c Unable to determine from the paper (e.g., diagrams and/or questions were not included in the paper).</region>
        <region class="DoCO:TextChunk" id="167" page="13" column="1">The columns indicate the degree to which the studies comply with our guidelines. The comparison allows us to identify the strengths and weakness of previous research and demonstrates how the framework can be applied to compare a range of independent studies. The first column iden- tifies the study, and the second column briefly summarizes the experimental task. Each of the remaining columns represents a framework criterion as follows:</region>
        <region class="DoCO:TextChunk" id="168" confidence="possible" page="13" column="1">• Information equivalence: Did the diagrams used provide the information to answer the questions correctly under any of the representational forms used as treatments in the experimental study? • Answerable from script: Could the questions be answered without reasoning beyond the information presented in the script(s)? • Potential impact of prior knowledge: To what extent could subject matter expertise (formal or common-sense) confound results? • Script available: Could participants refer to scripts while answering the questions?</region>
        <region class="DoCO:TextChunk" id="172" page="13" column="1">To assess information equivalence, we analyzed the diagram segments (scripts) used for each treatment to determine if they contained the same information with respect to the dependent measures. We recognized that equivalence could be high, moderate, or low, based on the degree of similarity in the information content of scripts used in the treatments with respect to the dependent measures. In the case of diagram construction tasks, the issue of information equivalence was deemed to be not applicable. If scripts for each treatment were not included in the paper, <marker type="page" number="14"/><marker type="block"/> information equivalence was classified as Ôunknown.Õ To determine if questions could be answered from the scripts provided, we again analyzed the questions in reference to the script. The following coding scheme was used: if the answers to questions were contained in the script requiring no inference, the study was classified as ÔhighÕ with respect to this criterion; if the answer required some inference, it was as ÔmoderateÕ; if the answer was only vaguely based on the script, it was classified as ÔlowÕ. Note that, in several cases in which questions could not be fully answered from the script, researchers instructed participants to answer a question ‘‘from the information in the diagram’’. While this might mitigate the tendency to use other information, this is difficult to establish with confidence in the absence of some evidence about participantsÕ behavior in the study. To determine the potential impact of prior knowledge of the domain, we generally accepted the classification of examples used by authors who used multiple examples with different levels of domain familiarity (see footnote to Table 7). In cases having only one study, we made a subjective assessment of the relative level of familiarity of the domain that might reasonably be expected from participants (generally, university students). Note that most studies either manipulated or measured background knowledge in order to eliminate or assess its potential impacts on responses. Entries in the column ÔModels AvailableÕ were taken from authorsÕ descriptions of the procedures used in the study.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="170" page="14" column="1">340</outsider>
        <outsider class="DoCO:TextBox" type="header" id="171" page="14" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="173" page="14" column="1">10. Conclusions</h1>
        <region class="DoCO:TextChunk" id="175" page="14" column="1">Researchers involved in experiments to evaluate conceptual modeling techniques need to design materials, develop dependent measures, select subjects, and design procedures that enable the objectives of the experiment to be met. We believe there is a pressing need for more controlled experimental studies that will improve our theoretical understanding of how the modeling constructs used in diagramming techniques facilitate comprehension of domain semantics for the purposes of communication and validation. Such experimental work should inform future field research aimed at understanding the effectiveness of conceptual modeling grammars in practice. With respect to the guidelines proposed in this paper, Table 7 highlights a lack of consistency in the selection of materials, questions, subjects, and procedures in previous experimental research. This inhibits the development of a cumulative theoretical understanding of how semantics are conveyed in conceptual modeling grammars. The table demonstrates that applying the framework to prior research on conceptual modeling can help in comparing and integrating the findings from previous studies in terms of improving our understanding of how alternate representations facilitate verification of conceptual models. Moreover, Table 7 suggests there are other research questions for which the guidelines proposed here do not apply. We conclude that researchers need to pay careful attention to articulating their research questions and justifying their experimental design decisions accordingly. Finally, when viewed in the context of the framework introduced by Wand and Weber [<xref ref-type="bibr" rid="R25" id="174" class="deo:Reference">25</xref>], this paper justifies, and provides empirical evidence to support, more specific guidelines for the conduct of experimental studies. We see this work as part of an ongoing dialogue, and hope these guidelines spark further discussion among researchers conducting experiments in conceptual modeling, in much the same way as the Calder–Lynch debate did for experimental research in marketing.</region>
        <outsider class="DoCO:TextBox" type="header" id="176" page="15" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="177" page="15" column="1">341</outsider>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="178" page="15" column="1">References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="179" page="15" column="1">[1] F. Bodart, A. Patel, M. Sim, R. Weber, Should optional properties be used in conceptual modelling? A theory and three empirical tests, Information Systems Research 12 (4) (2001) 384–405.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="180" page="15" column="1">[2] M. Bunge, Treatise on Basic Philosophy, Ontology, vol.3, Reidel, Amsterdam, 1977.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="181" page="15" column="1">[3] A. Burton-Jones, P. Meso, How good are these UML diagrams? An empirical test of the Wand and Weber good decomposition model, in: L. Applegate, R. Galliers, J.I. DeGross (Eds.), Proceedings of Twenty-third International Conference on Information Systems, 23 (Barcelona, Spain, 2002) pp. 101–114.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="182" page="15" column="1">[4] A. Burton-Jones, R. Weber, Understanding relationships with attributes in entity relationship diagrams, in: P. De, J. DeGross (Eds.), Proceedings of the Twentieth International Conference on Information Systems, (Charlotte, NC, 1999) pp. 214–228.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="183" page="15" column="1">[5] A. Burton-Jones, R. Weber, Properties do not have properties: Investigating a questionable conceptual modeling practice, in: D. Batra, J. Parsons, V. Ramesh (Eds.), Proceedings of the Second Symposium on Research in Systems Analysis and Design (Miami, FL, 2003) 14 pp.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="184" page="15" column="1">[6] B.J. Calder, L.W. Phillips, A.M. Tybout, Designing research for application, Journal of Consumer Research 8 (Sept) (1981) 197–207.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="185" page="15" column="1">[7] B.J. Calder, L.W. Phillips, A.M. Tybout, The concept of external validity, Journal of Consumer Research 9 (Dec.) (1982) 240–244.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="186" page="15" column="1">[8] B.J. Calder, L.W. Phillips, A.M. Tybout, Beyond external validity, Journal of Consumer Research 10 (Dec.) (1982) 112–114.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="187" page="15" column="1">[9] A. Gemino, Empirical Methods for Comparing Systems Analysis Modeling Techniques, Ph.D. Thesis, Faculty of Commerce and Business Administration, University of British Columbia, 1999.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="188" page="15" column="1">[10] A. Gemino, Y. Wand, Comparing mandatory and optional properties in conceptual data modeling, in: P. Bowen, V. Mookerjee (Eds.), Proceedings of the Tenth Workshop on Information Technologies and Systems (Brisbane, Australia, 2000) pp. 97–102.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="189" page="15" column="1">[11] A. Gemino, Y. Wand, Foundations for Empirical Comparisons of Conceptual Modeling Techniques, in: D. Batra, J. Parsons, V. Ramesh (Eds.), Proceedings of the Second Symposium on Research in Systems Analysis and Design (Miami, FL, 2003) 29 pp.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="190" page="15" column="1">[12] N. Juristo, A.M. Moreno, Basics of Software Engineering Experimentation, Kluwer, Boston, 2001.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="191" page="15" column="1">[13] B.A. Kitchenham, S.L. Pfleeger, L.M. Pickard, P.W. Jones, D.C. Hoaglin, K. El Emam, J. Rosenberg, Preliminary guidelines for empirical research in software engineering, IEEE Transactions on Software Engineering 28 (8) (2002) 721–734.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="192" page="15" column="1">[14] C. Kung, A. Solvberg, Activity Modeling and Behavior Modeling, in: T. Olle, H. Sol, A. Verrijn-Stuart (Eds.), Information Systems Design Methodologies: Improving the Practice, North-Holland, Berlin, 1985, pp. 145–171.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="193" page="15" column="1">[15] J.G. Lynch Jr., On the external validity of experiments in consumer research, Journal of Consumer Research 9 (Dec.) (1982) 225–239.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="194" page="15" column="1">[16] J.G. Lynch Jr., The role of external validity in theoretical research, Journal of Consumer Research 10 (1983) 109– 114.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="195" page="15" column="1">[17] R.E. Mayer, Models for understanding, Review of Educational Research 59 (1) (1989) 43–64.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="196" page="15" column="1">[18] R.E. Mayer, J.K. Gallini, When is an illustration worth ten thousand words?, Journal of Educational Psychology 82 (4) (1990) 715–726.</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="197" page="15" column="1">[19] J. Oei, L. van Hemmen, E. Falkenberg, S. Brinkkemper, The MetaModel Hierarchy: A Framework for Information Systems Concepts and Techniques, Technical Report, Department of Informatics, Katholieke Universiteit 92 (17) (1992) pp. 1–30.</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="198" page="15" column="1">[20] J. Parsons, Effects of local versus global schema diagrams on verification and communication in conceptual data modeling, Journal of Management Information Systems 19 (3) (2003) 155–183.</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="199" page="15" column="1">[21] J. Parsons, L. Cole, Properties of properties: An experimental examination of property precedence in conceptual modeling, in: S. Hartmann, J. Roddick, (Eds.), Conceptual Modelling 2004: First Asia-Pacific Conference on Conceptual Modelling, Conferences in Research and Practice in Information Technology (31) (Jan., 2004) pp. 101– 110.</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="202" page="16" column="1">[22] J. Parsons, Y. Wand, Property-based Semantic Reconciliation of Heterogeneous Information Sources, in: S. Spaccapietra, S. March, Y. Kambayashi (Eds.), Lecture Notes in Computer Science, vol. 2503 (ER-2002), Springer Verlag, 2002, pp. 351–364.</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="203" page="16" column="1">[23] J. Singer, N.G. Vinson, Ethical issues in empirical studies of software engineering, IEEE Transactions on Software Engineering 28 (12) (2002) 1171–1180.</ref>
          <ref rid="R24" class="deo:BibliographicReference" id="204" page="16" column="1">[24] Y. Wand, R. Weber, On the ontological expressiveness of information systems analysis and design grammars, Journal of Information Systems 3 (1993) 217–237.</ref>
          <ref rid="R25" class="deo:BibliographicReference" id="205" page="16" column="1">[25] Y. Wand, R. Weber, Information systems and conceptual modeling: a research agenda, Information Systems Research 13 (4) (2002) 363–376.</ref>
          <ref rid="R26" class="deo:BibliographicReference" id="206" page="16" column="1">[26] R. Weber, Are attributes entities? A study of database designersÕ memory structures, Information Systems Research 7 (2) (1996) 137–162.</ref>
          <ref rid="R27" class="deo:BibliographicReference" id="209" page="16" column="1">[27] C. Wohlin, P. Runeson, M. Ho  ̈ st, M.C. Ohlsson, Bjo  ̈ rn Regnell, A. Wessl  ́n, Experimentation in Software Engineering: An Introduction, Kluwer, Boston, 2000. <marker type="block"/> Jeffrey Parsons received the BComm (honours) degree from Memorial University of Newfoundland in 1985, and a Ph.D. from the University of British Columbia in 1992. He was on the faculty at the University of Manitoba for two years, was a visiting faculty member at the University of Pennsylvania, and is currently Professor and Associate Dean (Research) in the Faculty of Business Administration at Memorial University of Newfoundland. He co-chaired the 2001 Workshop on Information Technologies and Systems. His research is funded by the Natural Sciences and Engineering Research Council of Canada and the Social Sciences and Humanities Research Council of Canada. His work has appeared in journals such as ACM Transactions on Database Systems, IEEE Transactions on Software Engineering, Communications of the ACM, Journal of Management Information Systems, and Management Science. His current research interests</ref>
          <ref class="deo:BibliographicReference" id="210" confidence="possible" page="16" column="1">include artifact reuse in systems development, conceptual modeling, database design, and recommender systems. He is a member of the IEEE Computer Society, Association for Computing Machinery, Institute for Operations Research and Managment Science, and Association for Information Systems.</ref>
          <ref class="deo:BibliographicReference" id="211" page="16" column="1">Linda Cole received her BComm (honours) degree from Memorial University of Newfoundland in 2002. She is currently an M.Sc. candidate in management information systems at QueenÕs School of Business, QueenÕs University. Her research interests include requirements analysis and innovation adoption.</ref>
        </ref-list>
        <outsider class="DoCO:TextBox" type="page_nr" id="200" page="16" column="1">342</outsider>
        <outsider class="DoCO:TextBox" type="header" id="201" page="16" column="1">J. Parsons, L. Cole / Data &amp; Knowledge Engineering 55 (2005) 327–342</outsider>
        <region class="DoCO:FigureBox" id="Fx208">
          <image class="DoCO:Figure" src="62ns.page_016.image_01.png" thmb="62ns.page_016.image_01-thumb.png"/>
        </region>
        <region class="DoCO:FigureBox" id="Fx212">
          <image class="DoCO:Figure" src="62ns.page_016.image_02.png" thmb="62ns.page_016.image_02-thumb.png"/>
        </region>
      </section>
    </body>
  </article>
</pdfx>
