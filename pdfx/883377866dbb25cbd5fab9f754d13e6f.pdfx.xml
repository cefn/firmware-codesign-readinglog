<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>9065123f3fe8a3d18d934a24b77ba1642e9f63c46867c5d55c6006d343114653</job>
    <base_name>62p6</base_name>
    <doi confidence="possible" alt_doi="http://dx.doi.org/10.1093/acprof:oso/9780195387384.003.0003">http://dx.doi.org/10.1002/pam.10129</doi>
    <warning>Original PDF was found to be an image-based/possible OCR document. Output quality may be degraded.</warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <region class="unknown" id="1">ii:.</region>
      <title-group>
        <article-title class="DoCO:Title" id="2">EXPERIMENTAL AND QUASI-EXPERIMENTAL DESIGNS FOR GENERALIZED CAUSALINFERENCE</article-title>
      </title-group>
      <region class="DoCO:TextChunk" id="3" confidence="possible">William R. Shadish Trru UNIvERSITYop MEvPrrts</region>
      <contrib-group class="DoCO:ListOfAuthors"><contrib contrib-type="author"><name id="4">Thomas D. Cook</name></contrib><contrib contrib-type="author"><name id="10">Donald T. Campbell</name></contrib></contrib-group>
      <region class="unknown" id="5">** .jr-*"-</region>
      <region class="unknown" id="6">NonrrrwpsrERN UNrvPnslrY</region>
      <outsider class="DoCO:TextBox" type="sidenote" id="7">iLli" '"+.'- , ,</outsider>
      <outsider class="DoCO:TextBox" type="sidenote" id="8">fr</outsider>
      <region class="unknown" id="11">HOUGHTON MIFFLIN COMPANY Boston New York</region>
      <region class="unknown" id="12">Experiments and Generalized Causal lnference</region>
      <region class="unknown" id="13">Ex.per'i'ment English from Old French from Latin experimentum, from experiri, to try; in Indo-European Roots.] n. Abbr. exp., expt, 1. a. A test under controlled conditions that is made to demonstratea known truth, examine the validity of a hypothesis, or determine the efficacyof something previously untried' b. The processof conducting such a test; experimentation. 2' An innovative act or procedure: "Democracy is only an experiment in gouernment" (.V{illiam Ralph lnge).</region>
      <region class="unknown" id="14">Cause (k6z): [Middle English from Old French from Latin causa' teason, purpose.] n. 1. a. The producer of an effect, result, or consequence. b. The one, such as a person, an event' or a condition, that is responsible for an action or a result. v. 1. To be the causeof or reason for; result in. 2. To bring about or compel by authority or force.</region>
      <region class="DoCO:TextChunk" id="15" confidence="possible">o MANv historians and philosophers,the increasedemphasison experimentation in the 15th and L7th centuriesmarked the emergence of modern science Galileo's from its '1.6'!.2 roots treatrse in natural Bodies philosophy Tbat Stay Atop (Hacking, 'Water, or 1983). Moue Drake in It as (1981) usheringin cites modern experimental science,but earlier claims can be made favoring \Tilliam Gilbert's1,600 study Onthe Loadstoneand MagneticBodies, Leonardoda Vinci's (1,452-1.51.9) many investigations, and perhapseventhe Sth-century B.C.philoso- pher Empedocles,who used various empirical demonstrationsto argue against Parmenides(Jones, '1.969a, 1'969b).In the everyday senseof the term, humans have beenexperimentingwith different ways of doing things from the earliestmo- ments of their history. Suchexperimentingis as natural a part of our life as trying a new recipe or a different way of starting campfires.</region>
      <region class="DoCO:FigureBox" id="Fx16">
        <image class="DoCO:Figure" src="62p6.page_002.image_02.png" thmb="62p6.page_002.image_02-thumb.png"/>
      </region>
      <outsider class="DoCO:TextBox" type="header" id="17">z 1. EXeERTMENTs AND GENERALTzED cAUsAL INFERENcE</outsider>
      <region class="unknown" id="18">| I</region>
      <region class="DoCO:TextChunk" id="19">However, the scientific revolution of the 1.7thcentury departed in three ways from the common use of observation in natural philosophy atthat time. First, it in- creasingly used observation to correct errors in theory. Throughout historg natural philosophers often used observation in their theories, usually to win philosophical arguments by finding observations that supported their theories. However, they still subordinated the use of observation to the practice of deriving "first theories from principles," starting points that humans know to be true by our nature or by divine revelation (e.g., the assumedproperties of the four basic elements of fire, water, earth, and air in Aristotelian natural philosophy). According to some accounts,this subordination of evidenceto theory degenerated in the 17th "The century: Aristotelian principle of appealing to experiencehad degenerated among philosophers into dependenceon reasoning supported by casual examples and the refutation of opponents by pointing to apparent exceptions not carefully '1,98"1., examined" (Drake, p. xxi).'Sfhen some 17th-century scholarsthen beganto use observation to correct apparent errors in theoretical and religious first principles, they came into conflict with religious or philosophical authorities, as in the case of the Inquisition's demands that Galileo recant his account of the earth re- volving around the sun. Given such hazards,the fact that the new experimental science tipped the balance toward observation and ^way from dogma is remarkable. By the time Galileo died, the role of systematicobservation was firmly entrenched as a central feature of science,and it has remained so ever since (Harr6,1981). Second,before the 17th century, appeals to experiencewere usually basedon passive observation of ongoing systemsrather than on observation of what happens after a system is deliberately changed. After the scientific revolution in the L7th centurS the word experiment (terms in boldface in this book are defined in the Glossary) came to connote taking a deliberate action followed by systematic observationof what occurred afterward. As Hacking (1983) noted of FrancisBa- "He con: taught that not only must we observenature in the raw, but that we must 'twist also the lion's tale', that is, manipulate our world in order to learn its se- crets" (p. U9). Although passiveobservation revealsmuch about the world, ac- tive manipulation is required to discover some of the world's regularities and possibilities (Greenwood,, 1989). As a mundane example, stainless steel does not occur naturally; humans must manipulate it into existence.Experimental science came to be concerned with observing the effects of such manipulations. Third, early experimenters realized the desirability of controlling extraneous influences that might limit or bias observation. So telescopeswere carried to higher points at which the air was clearer, the glass for microscopeswas ground ever more accuratelg and scientistsconstructed laboratories in which it was possible to use walls to keep out potentially biasing ether waves and to use (eventually sterilized) test tubes to keep out dust or bacteria. At first, thesecontrols were developed for astronomg chemistrg and physics, the natural sciences in which interest in sciencefirst bloomed. But when scientists started to use experiments in areas such as public health or education, in which extraneous influences are harder to control (e.g., Lind , 1,753lr,they found that the controls used in natural</region>
      <region class="DoCO:FigureBox" id="Fx20">
        <image class="DoCO:Figure" src="62p6.page_003.image_03.png" thmb="62p6.page_003.image_03-thumb.png"/>
      </region>
      <outsider class="DoCO:TextBox" type="header" id="21">EXPERTMENTS AND CAUSATTON I I</outsider>
      <region class="DoCO:TextChunk" id="22" confidence="possible">science in the laboratoryworked poorly in thesenew applications. So they developed new methodsof dealingwith extraneous influence,such as random assignment (Fisher, 1,925) or addinga nonrandomized control group (Coover&amp; Angell, 1.907). As theoreticaland observational experience accumulated across theseset- tings and topics,more sources of bias were identifiedand more methodswere developed to copewith them (Dehue, 2000). TodaSthe key featurecommonto all experiments is still to deliberately vary something so asto discover what happens to something else later-to discover the effectsof presumed causes. As laypersons we do this, for example,to assess what happens to our blood pressure if we exercise more, to our weight if we diet less, or ro our behaviorif we read a self-helpbook. However,scientificexperimenta- tion has developed increasinglyspecialized substance, language,and tools, including the practiceof field experimentation in the socialsciences that is the primary focus of this book. This chapter begins to explore these matters by (1) discussing the natureof causation that experiments test, (2) explainingthe spe- cializedterminology(e.g.,randomizedexperiments, quasi-experiments) that de- scribessocial experiments, (3) introducing the problem of how to generalize causalconnections from individual experiments, and (4) briefly situatingthe ex- perimentwithin a largerliteratureon the nature of science.</region>
      <region class="unknown" id="23">EXPERIMENTS AN D CAUSATION</region>
      <region class="DoCO:TextChunk" id="24" confidence="possible">A sensible discussion of experiments requiresboth a vocabularyfor talking about causation and an understanding of key concepts that underliethat vocabulary.</region>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="25" confidence="possible" page="4" column="1">DefiningCause, Effect, and Causal Relationships</h2>
        <region class="DoCO:TextChunk" id="26" confidence="possible" page="4" column="1">Most peopleintuitively recognize causal relationships in their daily lives.For instance, you may say that another automobile's hitting yours was a causeof the damage to your car; that the number of hours you spentstudyingwas a causeof your testgrades; or that the amountof food a friend eatswas a cause of his weight. You may evenpoint to more complicated causal relationships, noting that a low test gradewas demoralizing, which reducedsubsequent studying,which caused evenlower grades. Here the same variable(low grade)can be both a cause and an effect,and there can be a reciprocal relationship betweentwo variables (low gradesand not studying)that cause each other. Despitethis intuitive familiarity with causalrelationsbips, a precise definition of cause and effecthaseludedphilosophers for centuries.lIndeed,the definitions</region>
        <region class="unknown" id="27" page="4" column="1">1. Our analysisrefldctsthe useof the word causationin ordinary language, not the more detaileddiscussions of causeby philosophers. Readers interested in suchdetail may consult a host of works that we reference in this chapter,includingCook and</region>
        <region class="DoCO:FigureBox" id="Fx28">
          <image class="DoCO:Figure" src="62p6.page_004.image_04.png" thmb="62p6.page_004.image_04-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="29" page="5" column="1">4 | 1. EXPERTMENTS AND GENERALTZED CAUSAL INFERENCE</outsider>
        <region class="DoCO:TextChunk" id="30" confidence="possible" page="5" column="1">of terms suchas cause and, effectdependpartly on eachother and on the causal relationshipin Locke said: "That which which both producesany are embedded. simpleor So the complexidea,we 17th-century philosopherJohn denoteby the generalnamecaLtse, and that which is produce d, effect" (1,97 s, p. 32fl and also: " A cAtrse is that which makesany other thing, either simpleidea, substance, or mode,beginto be; and an effectis that, which had its beginning from someother thing" (p. 325).Since then, otherphilosophers and scientists have givenus useful definitionsof the threekey ideas--cause, effect, and causal relationship-that are more specific and that betterilluminatehow experiments work. We would not defend any of theseas the true or correctdefinition,giventhat the latter haseluded philosophers for millennia;but we do claign that theseideashelp to clarify the scientific practiceof probing causes.</region>
        <region class="DoCO:TextChunk" id="31" confidence="possible" page="5" column="1">Cause 'We Considerthe causeof a forest fire. know that fires start in differentways-a match tossedfrom a ca\ a lightning strike, or a smolderingcampfire,for example. None of thesecauses is necessary because a forest fire can start evenwhen, say'a match is not present.Also, none of them is sufficientto start the fire. After all, a match must stay "hot" long enoughto start combustion;it must contact combustible material suchas dry leaves; there must be oxygenfor combustionto occur; and the weather must be dry enoughso that the leavesare dry and the match is not dousedby rain. So the match is part of a constellation of conditions without which a fire will not result,althoughsomeof these conditionscan be usually takenfor granted,suchasthe availabilityof oxygen.A lightedmatchis, rhere- fore, what Mackie (1,974)called an inus condition-"an insufficient but non- redundantpart of an unnecessary but sufficient condition" (p. 62; italicsin original). It is insufficientbecause a match cannot start a fire without the other conditions. It is nonredundant only if it adds something fire-promoting that is uniquelydifferent from what the other factors in the constellation (e.g., oxygen, dry leaves) contributeto startinga fire; after all,it would beharderro saywhether the match causedthe fire if someone else simultaneously tried startingit with a cigarettelighter.It is part of a sufficientcondition to start a fire in combination with the full constellationof factors.But that condition is not necessary because thereare other setsof conditionsthat can also start fires. A research exampleof an inus condition concerns a new potentialtreatment for cancer. In the late 1990s,a teamof researchers in Bostonheaded by Dr. Judah Folkman reportedthat a new drug calledEndostatinshrank tumors by limiting their blood supply (Folkman, 1996).Other respected researchers could not repli- catethe effectevenwhen usingdrugsshippedto them from Folkman's lab. Scien- tists eventuallyreplicatedthe resultsafter they had traveledto Folkman'slab to learnhow to properlymanufacture, transport,store,and handlethe drug and how to inject it in the right location at the right depth and angle.One observer labeled thesecontingencies the "in-our-hands" phenomenon,meaning "even we don't</region>
        <region class="DoCO:FigureBox" id="Fx32">
          <image class="DoCO:Figure" src="62p6.page_005.image_05.png" thmb="62p6.page_005.image_05-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="33" page="6" column="1">EXPERIMENTS AND CAUSATIONI S</outsider>
        <region class="DoCO:TextChunk" id="42" page="6" column="1">know which details are important, so it might take you some time to work it out" (Rowe, L999, p.732). Endostatin was an inus condition. It was insufficientcause by itself, and its effectiveness required it to be embedded in a larger set of conditions that were not even fully understood by the original investigators. Most causesare more accurately called inus conditions. Many factors are usually required for an effectto occur, but we rarely know all of them and how they relate to each other. This is one reason that the causal relationships we discussin this book are not deterministic but only increasethe probability that an effect will occur (Eells,1,991,; Holland, 1,994).It also explains why a given causalrelationship will occur under some conditions but not universally across time, space,hu- -"r pop,rlations, or other kinds of treatments and outcomes that are more or less related io those studied. To different {egrees, all causal relationships are context dependent,so the generalizationof experimental effects is always at issue.That is *hy *. return to such generahzations throughout this book. <marker type="block"/> 'We can better understand what an effect is through a counterfactual model 'l'973' that goes back at least to the 18th-century philosopher David Hume (Lewis, p. SSel. A counterfactual is something that is contrary to fact. In an experiment, ie obseruewhat did happez when people received a treatment. The counterfactual is knowledge of what would haue happened to those same people if they simultaneously had not receivedtreatment. An effect is the difference betweenwhat did happen and what would have happened. 'We cannot actually observe a counterfactual. Consider phenylketonuria (PKU), a genetically-based metabolic disease that causes mental retardation unless treated during the first few weeks of life. PKU is the absenceof an enzyme that would otherwise prevent a buildup of phenylalanine, a substance toxic to the nervous system. Vhen a restricted phenylalanine diet is begun early and main- tained, reiardation is prevented. In this example, the causecould be thought of as the underlying genetic defect, as the enzymatic disorder, or as the diet. Each implies a difierenicounterfactual. For example, if we say that a restricted phenylalanine diet causeda decrease in PKU-basedmental retardation in infants who are phenylketonuric 'h"d at birth, the counterfactual is whatever would have happened t'h.r. sameinfants not receiveda restricted phenylalanine diet. The samelogic applies to the genetic or enzymatic version of the cause. But it is impossible for theseu.ry ,"-i infants simultaneously to both have and not have the diet, the genetic disorder, or the enzyme deficiency. So a central task for all cause-probing research is to create reasonable ap- proximations to this physically impossible counterfactual. For instance, if it were ethical to do so, we might contrast phenylketonuric infants who were given the diet with other phenylketonuric infants who wer€ not given the diet but who were similar in many ways to those who were (e.g., similar face) gender,age, socioeco- nomic status, health status). Or we might (if it were ethical) contrast infants who<marker type="page" number="7"/><marker type="block"/> were not on the diet for the first 3 months of their lives with those same infants after they were put on the diet starting in the 4th month. Neither of these ap- proximations is a true counterfactual. In the first case,the individual infants in the treatment condition are different from those in the comparison condition; in the second case, the identities are the same, but time has passedand many changes other than the treatment have occurred to the infants (including permanent damage done by phenylalanine during the first 3 months of life). So two central tasks in experimental design are creating a high-quality but necessarily imperfect source of counterfactual inference and understanding how this source differs from the treatment condition. This counterfactual reasoning is fundarnentally qualitative because causal inference, even in experiments, is fundamentally qualitative (Campbell, 1975; Shadish, 1995a; Shadish 6c Cook, 1,999). However, some of these points have been formalized by statisticiansinto a specialcasethat is sometimescalled Rubin's "1.974,'1.977,1978,79861. CausalModel (Holland, 1,986;Rubin, This book is not about statistics, so we do not describethat model in detail ('West,Biesanz,&amp; Pitts [2000] do so and relate it to the Campbell tradition). A primary emphasisof Rubin's model is the analysis of causein experiments, and its basic premisesare consistent with those of this book.2 Rubin's model has also been widely used to analyze causal inference in case-control studies in public health and medicine (Holland 6c Rubin, 1988), in path analysisin sociology (Holland,1986), and in a paradox that Lord (1967) introduced into psychology (Holland 6c Rubin, 1983); and it has generatedmany statistical innovations that we cover later in this book. It is new enough that critiques of it are just now beginning to appear (e.g., Dawid, 2000; Pearl, 2000). tUfhat is clear, however, is that Rubin's is a very general model with obvious and subtle implications. Both it and the critiques of it are required material for advanced students and scholars of cause-probingmethods.<marker type="block"/> How do we know if cause and effect are related? In a classic analysis formalized by the 19th-century philosopher John Stuart Mill, a causal relationship exists if (1) the causeprecededthe effect, (2) the causewas related to the effect,and (3) we can find no plausible alternative explanation for the effect other than the cause. These three characteristics mirror what happens in experiments in which (1) we manipulate the presumed cause and observe an outcome afterward; (2) we see whether variation in the cause is related to variation in the effect; and (3) we use various methods during the experiment to reduce the plausibility of other explanations for the effect, along with ancillary methods to explore the plausibility of those we cannot rule out (most of this book is about methods for doing this).</region>
        <region class="unknown" id="35" page="6" column="1">Effect</region>
        <region class="DoCO:FigureBox" id="Fx37">
          <image class="DoCO:Figure" src="62p6.page_006.image_06.png" thmb="62p6.page_006.image_06-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="38" page="7" column="1">I 6 1. EXPERIMENTS AND GENERALIZED</outsider>
        <region class="unknown" id="39" page="7" column="1">I CAUSAL INFERENCE</region>
        <region class="unknown" id="41" page="7" column="1">Causal Relationship</region>
        <region class="unknown" id="43" page="7" column="1">2. However, Rubin's model is not intended to say much about the matters of causal generalization that we address in this book.</region>
        <region class="DoCO:FigureBox" id="Fx44">
          <image class="DoCO:Figure" src="62p6.page_007.image_07.png" thmb="62p6.page_007.image_07-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="45" page="8" column="1">EXPERTMENTS AND CAUSATTON | 7 I</outsider>
        <region class="unknown" id="46" page="8" column="1">Henceexperiments are well-suitedto studyingcausalrelationships. No other sci- the characteristics of causalrelationships so well. Mill's analysis also pointsto the weakness of other methods. In many correlational studies, for example,it is impossible to know which of two variablescamefirst, so defending a causal relationshipbetween them is precarious. Understanding this logic of causalrelationships and how its key terms,suchas causeand effect,are defined helpsresearchers to critique cause-probing studies.</region>
      </section>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="47" confidence="possible" page="8" column="1">Causation, Correlation, and Confounds</h2>
        <region class="DoCO:TextChunk" id="48" confidence="possible" page="8" column="1">A well-known maxim in research is: Correlation does not proue causation. This is so because we may not know which variable came first nor whether alternative explanations for the presumed effectexist. For example, supposeincome and education are correlated.Do you have to have a high income before you can aff.ordto pay for education,or do you first have to get a good education before you can get a better paying job? Each possibility may be true, and so both need investigation.But until those investigationsare completed and evaluatedby the scholarly communiry a simple correlation doesnot indicate which variable came first. Correlations also do little to rule out alternative explanations for a relationship between two variables such as education and income. That relationship may not be causal at all but rather due to a third variable (often called a confound), such as intelligence or family so- cioeconomicstatus,that causes both high education and high income. For example, if high intelligencecauses success in education and on the job, then intelligent people would have correlatededucation and incomes,not because education causes income (or vice versa) but because both would be causedby intelligence.Thus a central task in the study of experiments is identifying the different kinds of confounds that can operate in a particular researcharea and understanding the strengthsand weaknesses associated with various ways of dealing with them</region>
      </section>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="49" confidence="possible" page="8" column="1">Manipulable and Nonmanipulable Causes</h2>
        <region class="unknown" id="50" page="8" column="1">In the intuitive understanding of experimentation that most peoplehave,it makes sense to say, "Let's see what happens if we requirewelfarerecipients to work"; but it makesno sense to say, "Let's seewhat happens if I change this adult maleinto a three-year-old girl." And so it is alsoin scientific experiments. Experiments explore the effectsof things that can be manipulated, such as the dose of a medicine,the amount of a welfarecheck,the kind or amount of psychotherapy or the number of childrenin a classroom. Nonmanipulable events (e.g.,the explosionof a super- nova) or ages, their raw genetic material,or their biological sex)cannotbe causes in experiments because we cannotdeliberately vary them to seewhat then happens. Consequently, most scientists and philosophers agree that it is much harderto discover the effects of nonmanipulable causes.</region>
        <region class="DoCO:FigureBox" id="Fx51">
          <image class="DoCO:Figure" src="62p6.page_008.image_08.png" thmb="62p6.page_008.image_08-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="52" page="9" column="1">I</outsider>
        <region class="DoCO:TextChunk" id="53" confidence="possible" page="9" column="1">8 1. EXeERTMENTS ANDGENERALTzED cAUsAL TNFERENcE |</region>
        <region class="DoCO:TextChunk" id="57" page="9" column="1">To be clear,we are not arguing that all causes must be manipulable-only that experimental causes must be so. Many variables that we correctly think of as causes are not directly manipulable. Thus it is well establishedthat a geneticdefect causes PKU even though that defect is not directly manipulable.'We can investigatesuch causesindirectly in nonexperimental studiesor even in experimentsby manipulating biological processesthat prevent the gene from exerting its influence, as through the use of diet to inhibit the gene'sbiological consequences. Both the nonmanipulable gene and the manipulable diet can be viewed as causes-both covary with PKU-basedretardation, both precedethe retardation, and it is possibleto explore other explanations for the gene'sand the diet's effectson cognitive function- ing. However, investigating the manipulablc diet as a causehas two important advantages over considering the nonmanipulable genetic problem as a cause.First, only the diet provides a direct action to solve the problem; and second,we will see that studying manipulable agents allows a higher quality source of counterfactual inferencethrough such methods as random assignment.\fhen individuals with the nonmanipulable genetic problem are compared with personswithout it, the latter are likely to be different from the former in many ways other than the genetic defect. So the counterfactual inference about what would have happened to those with the PKU genetic defect is much more difficult to make. Nonetheless,nonmanipulable causesshould be studied using whatever means are availableand seemuseful. This is true because such causes eventuallyhelp us to find manipulable agents that can then be used to ameliorate the problem at hand. The PKU example illustrates this. Medical researchers did not discover how to treat PKU effectively by first trying different diets with retarded children. They first discovered the nonmanipulable biological features of retarded children affected with PKU, finding abnormally high levels of phenylalanine and its associated metabolic and genetic problems in those children. Those findings pointed in certain ameliorative directions and away from others, leading scientiststo experiment with treatments they thought might be effective and practical. Thus the new diet resulted from a sequenceof studies with different immediate purposes, with different forms, and with varying degreesof uncertainty reduction. Somewere experimental, but others were not. Further, analogue experiments can sometimes be done on nonmanipulable causes,that is, experiments that manipulate an agent that is similar to the cause of interest. Thus we cannot change a person's race, but we can chemically induce skin pigmentation changes in volunteer individuals-though such analogues do not match the reality of being Black every day and everywhere for an entire life. Similarly past events,which are normally nonmanipulable, sometimesconstitute a natural experiment that may even have been randomized, as when the 1'970 Vietnam-era draft lottery was used to investigate a variety of outcomes (e.g., An- grist, Imbens, &amp; Rubin, 1.996a;Notz, Staw, &amp; Cook, l97l). Although experimenting on manipulable causesmakes the job of discovering their effectseasier,experiments are far from perfect means of investigating causes. <marker type="page" number="10"/><marker type="block"/> Sometimesexperiments modify the conditions in which testing occurs in a way that reducesthe fit between those conditions and the situation to which the results are to be generalized.Also, knowledge of the effects of manipulable causestells nothing about how and why those effectsoccur. Nor do experiments answer many other questions relevant to the real world-for example, which questions are worth asking, how strong the need for treatment is, how a cause is distributed through societg whether the treatment is implemented with theoretical fidelitS and what value should be attached to the experimental results. In additioq, in experiments,we first manipulate a treatment and only then ob- serveits effects;but in some other studieswe first observean effect, such as AIDS, and then search for its cause, whether manipulable or not. Experiments cannot help us with that search. Scriven (1976) likens such searchesto detective work in which a crime has been committed (..d., " robbery), the detectives observea particular pattern of evidencesurrounding the crime (e.g.,the robber wore a baseball cap and a distinct jacket and used a certain kind of Bun), and then the detectives searchfor criminals whose known method of operating (their modus operandi or m.o.) includes this pattern. A criminal whose m.o. fits that pattern of evidence then becomesa suspect to be investigated further. Epidemiologists use a similar method, the case-control design (Ahlbom 6c Norell, 1,990),in which they observe a particular health outcome (e.g., an increasein brain tumors) that is not seen in another group and then attempt to identify associatedcauses(e.g., increasedcell phone use). Experiments do not aspire to answer all the kinds of questions, not even all the types of causal questions, that social scientistsask.</region>
        <region class="DoCO:FigureBox" id="Fx55">
          <image class="DoCO:Figure" src="62p6.page_009.image_09.png" thmb="62p6.page_009.image_09-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="56" page="10" column="1">I EXPERIMENTS AND CAUSATIONI 9</outsider>
      </section>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="58" confidence="possible" page="10" column="1">Causal Description and Causal Explanation</h2>
        <region class="unknown" id="59" page="10" column="1">The uniquestrengthof experimentation is in describingthe consequences attrib- utableto deliberately varyinga treatment.'We call this causaldescription. In contrast, experiments do lesswell in clarifying the mechanisms through which and the conditionsunder which that causalrelationshipholds-what we call causal explanation.For example,most childrenvery quickly learnthe descriptive causal relationshipbetween flicking a light switch and obtainingillumination in a room. However,few children (or evenadults)can fully explain why that light goeson. To do so, they would haveto decompose the treatment(the act of flicking a light switch)into its causallyefficacious insulatedcircuit) and its nonessential features(e.g.,whetherthe switch is thrown by hand or a motion detector).They would haveto do the samefor the effect (eitherincandescent or fluorescentlight can be produced,but light will still be produced whether the light fixture is recessed or not). For full explanation,they would then have to show how the causallyefficacious parts of the treatmentinfluencethe causally affectedparts of the outcomethrough identified mediating processes (e.g., the</region>
        <region class="DoCO:FigureBox" id="Fx60">
          <image class="DoCO:Figure" src="62p6.page_010.image_10.png" thmb="62p6.page_010.image_10-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="61" page="11" column="1">I</outsider>
        <region class="DoCO:TextChunk" id="62" confidence="possible" page="11" column="1">1O I T. CXPTRIMENTS AND GENERALIZED CAUSAL INFERENCE</region>
        <region class="DoCO:TextChunk" id="63" confidence="possible" page="11" column="1">passageof electricity through the circuit, the excitation of photons).3 ClearlS the causeof the light going on is a complex cluster of many factors. For those philosophers who equate cause with identifying that constellation of variables that necessarily inevitably and infallibly results in the effect (Beauchamp,1.974),talk of cause is not warranted until everything of relevanceis known. For them, there is no causal description without causal explanation. Whatever the philosophic merits of their position, though, it is not practical to expect much current social science to achieve such complete explanation. The practical importance of causal explanation is brought home when the switch fails to make the light go on and when replacing the light bulb (another easily learned manipulation) fails to solva the problem. Explanatory knowledge then offers clues about how to fix the problem-for example, by detecting and re- pairing a short circuit. Or if we wanted to create illumination in a place without lights and we had explanatory knowledge, we would know exactly which features of the cause-and-effect relationship are essentialto create light and which are irrelevant. Our explanation might tell us that there must be a source of electricity but that that source could take several different molar forms, such as abattery, a generator, a windmill, or a solar array. There must also be a switch mechanism to close a circuit, but this could also take many forms, including the touching of two bare wires or even a motion detector that trips the switch when someone enters the room. So causal explanation is an important route to the generalization of causal descriptions becauseit tells us which features of the causal relationship are essentialto transfer to other situations. This benefit of causal explanation helps elucidate its priority and prestige in all sciences and helps explain why, once a novel and important causal relationship is discovered, the bulk of basic scientific effort turns toward explaining why and how it happens. Usuallg this involves decomposing the causeinto its causally effective parts, decomposing the effects into its causally affected parts, and identifying the processes through which the effective causal parts influence the causally affected outcome parts. These examplesalso show the close parallel between descriptive and explanatory causation and molar and molecular causation.aDescriptive causation usually concerns simple bivariate relationships between molar treatments and molar outcomes, molar here referring to a package that consistsof many different parts. For instance, we may find that psychotherapy decreases depression,a simple descriptive causal relationship benveen a molar treatment package and a molar outcome. However, psychotherapy consists of such parts as verbal interactions, placebo-</region>
        <region class="unknown" id="64" page="11" column="1">3. However, the full explanationa physicistwould offer might be quite different from this electrician's explanation,perhapsinvoking the behaviorof subparticles. This difference how complicated is the notion of explanationand how it can quickly becomequite complex once one shifts levelsof analysis. 4. By molar, we mean somethingtaken as a whole rather than in parts. An analogyis to physics,in which molar might refer to the propertiesor motions of masses, as distinguished from those of molecules or atomsthat make up thosemasses.</region>
        <region class="DoCO:FigureBox" id="Fx65">
          <image class="DoCO:Figure" src="62p6.page_011.image_11.png" thmb="62p6.page_011.image_11-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="66" page="12" column="1">EXPERIMENTS AND CAUSATION I 11 I</outsider>
        <region class="DoCO:TextChunk" id="67" page="12" column="1">generating procedures, setting characteristics,time constraints, and payment for services.Similarly, many depression measuresconsist of items pertaining to the physiological,cognitive, and affectiveaspectsof depression.Explan atory causation breaks thesemolar causesand effectsinto their molecular parts so as to learn, say, that the verbal interactions and the placebo featuresof therapy both causechanges in the cognitive symptoms of depression,but that payment for servicesdoes not do so even though it is part of the molar treatment package. If experiments are less able to provide this highly-prized explanatory causal knowledge, why.are experimentsso central to science,especiallyto basic social science,in which theory and explanation are often the coin of the realm? The answer is that the dichotomy ber'*reen descriptive and explanatory causation is lessclear in scientific practice than in abstract discussions about causation.First, many causal ex- planatironsconsist of chains of descriptivi causal links in which one event causesthe next. Experiments help to test the links in each chain. Second,experiments help distinguish betweenthe validity of competing explanatory theories, for example, by testing competing mediating links proposed by those theories. Third, some experiments test whether a descriptive causal relationship varies in strength or direction under Condition A versus Condition B (then the condition is a moderator variable that explains the conditions under which the effect holds). Fourth, some experimentsadd quantitative or qualitative observations of the links in the explanatory chain (medi- ator variables) to generateand study explanations for the descriptive causal effect. Experiments are also prized in applied areas of social science,in which the identification of practical solutions to social problems has as great or even greater priority than explanations of those solutions. After all, explanation is not always required for identifying practical solutions. Lewontin (1997) makes this point about the Human Genome Project, a coordinated multibillion-dollar research program ro map the human genome that it is hoped eventually will clarify the genetic causesof diseases. Lewontin is skeptical about aspectsof this search: '!ilhat is involvedhereis the difference between explanation and intervention. Many disorders can be explained by the failureof the organism to makea normal protein,a failurethat is the consequence of a genemutation.But interuentionrequires that the normalproteinbe providedat the right placein the right cells,at the right time and in the right amount,or elsethat an alternative way be found to providenormal cellular function.'What is worse, it might evenbenecessary to keepthe abnormalproteinaway from the cellsat criticalmoments. None of theseobjectives is served by knowing the "1,997, DNA sequence of the defective gene. (Lewontin, p.29) Practical applications are not immediately revealedby theoretical advance.In- stead, to reveal them may take decadesof follow-up work, including tests of simple descriptive causal relationships. The same point is illustrated by the cancer drug Endostatin, discussedearlier. Scientistsknew the action of the drug occurred through cutting off tumor blood supplies; but to successfullyuse the drug to treat cancersin mice required administering it at the right place, angle, and depth, and those details were not part of the usual scientific explanation of the drug's effects.</region>
        <region class="DoCO:FigureBox" id="Fx68">
          <image class="DoCO:Figure" src="62p6.page_012.image_12.png" thmb="62p6.page_012.image_12-thumb.png"/>
        </region>
        <region class="unknown" id="69" page="13" column="1">12 I 1. EXPERTMENTS AND GENERALTZED CAUSAL TNFERENCE I</region>
        <region class="unknown" id="70" page="13" column="1">In the end,then,causal descriptions and causal explanations are in delicate bal- ancein experiments.'$7hat experiments do bestis to improvecausaldescriptions; they do lesswell at explainingcausalrelationships. But most experiments can be designed to providebetterexplanations than is typicallythe case today. Further,in focusingon causaldescriptions, experiments often investigate molar eventsthat may be less strongly related to outcomesthan are more molecularmediating processes, especially those processes that are closerto the outcomein the explanatory chain. However,many causaldescriptions are still dependable and strong enoughto be useful,to be worth making the building blocks around which important policiesand theoriesare created. Just considerthe dependability of such causal statements as that schooldesegregation causes white flight, or that outgroup threat causes ingroup cohesion, or that psychotherapy improves mental health,or that diet reduces the retardationdueto PKU. Suchdependable causal relationships are usefulto policymakers, practitioners, and scientists alike.</region>
      </section>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="71" page="13" column="1">MODERN DESCRIPTIONS OF EXPERIMENTS</h1>
        <region class="DoCO:TextChunk" id="72" page="13" column="1">Some of the terms used in describing modern experimentation (seeTable L.L) are unique, clearly defined, and consistently used; others are blurred and inconsis- tently used. The common attribute in all experiments is control of treatment (though control can take many different forms). So Mosteller (1990, p. 225) writes, "fn an experiment the investigator controls the application of the treat- ment"l and Yaremko, Harari, Harrison, and Lynn (1,986,p.72) write, "one or more independent variables are manipulated to observe their effects on one or more dependentvariables." However, over time many different experimental sub- types have developed in responseto the needs and histories of different sciences 'Winston ('Winston, 1990; 6c Blais, 1.996\.</region>
        <region class="DoCO:TextChunk" id="73" confidence="possible" page="13" column="1">TABLE 1.1 TheVocabulary of Experiments</region>
        <region class="DoCO:TextChunk" id="74" confidence="possible" page="13" column="1">Experiment: A studyin whichan intervention is deliberately introduced to observe its effects. Randomized Experiment: An experiment in whichunitsareassigned to receive the treatment or an alternative condition by a random process such as the toss of a coinor a table of random numbers. Quasi-Experiment: An experiment in whichunitsarenot assigned to conditions randomly. Natural Experiment: Not really an experiment because the cause usually cannot be manipulated; a study that contrasts a naturally occurring event such as an earthquake with a comoarison condition. Correlational Study: Usually synonymous with nonexperimental or observational study; a study thatsimply observes the size and direction of a relationship among variables.</region>
        <region class="DoCO:FigureBox" id="Fx75">
          <image class="DoCO:Figure" src="62p6.page_013.image_13.png" thmb="62p6.page_013.image_13-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="76" page="14" column="1">I MODERN DESCRIPTIONS OF EXPERIMENTS I tr</outsider>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="77" confidence="possible" page="14" column="1">Randomized Experiment</h2>
          <region class="DoCO:TextChunk" id="78" confidence="possible" page="14" column="1">The most clearlydescribed variant is the randomizedexperiment, widely credited to Sir RonaldFisher (1,925,1926).It was first usedin agriculture but laterspread to other topic areasbecause it promisedcontrol over extraneous sources of variation without requiringthe physicalisolationof the laboratory.Its distinguishing featureis clear and important-that the varioustreatmentsbeingcontrasted(in- cludingno treatmentat all) are assigned to experimental units' by chance, for example,by cointossor useof a table of random numbers. If implemented correctlS ,"rdo- assignment creates two or more groupsof units that are probabilistically similarto .".h other on the average.6 Hence, any outcomedifferences that are observed between those groupsat the end,ofa study arelikely to be dueto treatment' not to differences between the groupsthat alreadyexistedat the start of the study. Further,when certainassumptions are met, the randomizedexperiment yields an estimateof the sizeof a treatmenteffectthat has desirablestatisticalproperties' along with estimates of the probability that the true effectfalls within a defined confidence interval.Thesefeaturesof experiments are so highly prized that in a research area suchas medicinethe randomizedexperimentis often referredto as the gold standardfor treatmentoutcomeresearch.' Closelyrelatedto the randomizedexperimentis a more ambiguousand in- consistently used term, true experiment.Someauthorsuseit synonymously with randomized experiment(Rosenthal &amp; Rosnow,1991'). Others useit more generally to refer to any studyin which an independent variableis deliberately 'We manipulated (Yaremkoet al., 1,9861and a dependent variableis assessed. shall not usethe term at all givenits ambiguity and given that the modifier true seems to imply restricted claims to a singlecorrectexperimental method.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="79" confidence="possible" page="14" column="1">Quasi-Experiment</h2>
          <region class="DoCO:TextChunk" id="80" confidence="possible" page="14" column="1">Much of this book focuseson a class of designsthat Campbell and Stanley (1,963) popularized as quasi-experiments.s Quasi-experiments share with all other</region>
          <region class="unknown" id="81" page="14" column="1">5. Units can be people,animals,time periods,institutions,or almost anything else.Typically in field experimentation they are peopleor someaggregate of people,such as classrooms or work sites.In addition, a little thought showsthat random assignment of units to treatmentsis the sameas assignment of treatmentsto units, so are frequendyusedinterchangeably' 6. The word probabilisticallyis crucial, as is explainedin more detail in Chapter 8. 7. Although the rerm randomized experiment is used this way consistently acrossmany fields and in this book, the closely related term random experiment in a different way to indicate experiments for which the outcomecannor be predictedwith &amp; Tanis, 1988). 8. Campbell (1957) first but changedterminologyvery quickly; Rosenbaum (1995a\ and Cochran (1965\ referto theseas observational studies,a term we avoid because many peopleuseit to refer to correlationalor nonexperimental studies,as well. Greenberg and usequdsi-etcperiment to refer to studiesthat randomly assigngroups conditions,but we would considerthesegroup- randomizedexperiments (Murray' 1998).</region>
          <region class="DoCO:FigureBox" id="Fx82">
            <image class="DoCO:Figure" src="62p6.page_014.image_14.png" thmb="62p6.page_014.image_14-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="83" page="15" column="1">I</outsider>
          <outsider class="DoCO:TextBox" type="sidenote" id="84" page="15" column="1">14</outsider>
          <region class="unknown" id="85" page="15" column="1">I 1. EXPERIMENTS AND GENERALIZED CAUSAL INFERENCE I</region>
          <outsider class="DoCO:TextBox" type="sidenote" id="86" page="15" column="1">t</outsider>
          <outsider class="DoCO:TextBox" type="sidenote" id="87" page="15" column="1">rl</outsider>
          <region class="DoCO:TextChunk" id="88" page="15" column="1">experiments a similar purpose-to test descriptivecausal hypothesesabout manipulable causes-as well as many structural details, such as the frequent presenceof control groups and pretest measures,to support a counterfactual inference about what would have happened in the absenceof treatment. But, by definition, quasi- experiments lack random assignment. Assignment to conditions is by means of self- selection,by which units choosetreatment for themselves, or by meansof adminis- trator selection,by which teachers,bureaucrats,legislators,therapists,physicians, or others decide which persons should get which treatment. Howeveq researchers who use quasi-experimentsmay still have considerablecontrol over selectingand schedulingmeasures,over how nonrandom assignmentis executed,over the kinds of comparison groups with which treatment,groups are compared, and over some aspectsof how treatment is scheduled.As Campbell and Stanleynote:</region>
          <region class="DoCO:TextChunk" id="89" confidence="possible" page="15" column="1">There are many natural socialsettingsin which the research person can introduce something like experimental design into his scheduling of data collectionprocedures (e.g.,the uhen and to whom of measurement), even though he lacksthe full control over the scheduling of experimental stimuli (the when and to wltom of exposure and the ability to randomize exposures) which makesa true experiment possible. Collec- tively,such situationscan be regarded as quasi-experimental designs. (Campbell&amp; StanleS 1,963, p. 34)</region>
          <region class="DoCO:TextChunk" id="90" page="15" column="1">In quasi-experiments,the causeis manipulable and occurs before the effect is measured. However, quasi-experimental design features usually create less compelling support for counterfactual inferences. For example, quasi-experimental control groups may differ from the treatment condition in many systematic(non- random) ways other than the presenceof the treatment Many of theseways could be alternative explanations for the observed effect, and so researchershave to worry about ruling them out in order to get a more valid estimate of the treatment effect. By contrast, with random assignmentthe researcherdoes not have to think as much about all these alternative explanations. If correctly done, random assignment makes most of the alternatives less likely as causes of the observed treatment effect at the start of the study. In quasi-experiments,the researcherhas to enumeratealternative explanations one by one, decide which are plausible, and then use logic, design, and measurement to assess whether each one is operating in a way that might explain any ob- servedeffect. The difficulties are that thesealternative explanations are never completely enumerable in advance, that some of them are particular to the context being studied, and that the methods neededto eliminate them from contention will vary from alternative to alternative and from study to study. For example, suppose two nonrandomly formed groups of children are studied, a volunteer treatment group that gets a new reading program and a control group of nonvolunteerswho do not get it. If the treatment group does better, is it becauseof treatment or be- causethe cognitive development of the volunteerswas increasingmore rapidly even before treatment began? (In a randomized experiment, maturation rates would</region>
          <region class="DoCO:FigureBox" id="Fx91">
            <image class="DoCO:Figure" src="62p6.page_015.image_15.png" thmb="62p6.page_015.image_15-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="92" page="16" column="1">MODERN DESCRIPTIONS OF EXPERIMENTS | 1s</outsider>
          <region class="DoCO:TextChunk" id="93" confidence="possible" page="16" column="1">havebeenprobabilistically equal in both groups.)To assess this alternative, the researcher might add multiple pretests to revealmaturationaltrend beforethe treatment, and then comparethat trend with the trend after treatment. Another alternative explanationmight bethat the nonrandomcontrol group in- cludedmoredisadvantaged children who had lessaccess to booksin their homesor who had parentswho read to them lessoften. (In a randomizedexperiment'both groupswould havehad similar proportionsof suchchildren.)To assess this alter- nativi, the experimenter may measure the number of books at home,parentaltime spentreadingtochildren,and perhaps trips to libraries. Then the researcher would seeif thesevariablesdiffered acrosstreatment and control groups in the hypothe- sizeddirection that could explain the observedtreatment effect. Obviously,as the number of plausiblealternativeexplapationsincreases, the designof the quasi. experimentbecomes more intellectually demandingand complex---especially because we are nevercertainwe haveidentifiedall the alternative explanations. The efforts of the quasi-experimenter start to look like affemptsto bandagea wound that would have beenlesssevere if random assignment had beenusedinitially. The ruling out of alternative hypotheses is closelyrelatedto a falsificationist logic popularized by Popper (1959).Poppernoted how hard it is to be sure that a g*.r"t conclusion(e.g.,,ll r*"ttr are white) is correct basedon a limited set of observations (e.g., all the swansI've seenwere white). After all, future observations may change(e.g.,some day I may seea black swan).So confirmation is logically difficult. By contrast,observing a disconfirminginstance (e.g., a black swan) is sufficient,in Popper's view, to falsify the generalconclusionthat all swansare white. Accordingly, nopper urged scientists to try deliberately to falsify the con- clusionsthey wiih to draw rather than only to seekinformation corroborating them. Conciusions that withstand falsificationare retainedin scientificbooks or journals and treated as plausible until better evidencecomes along. Quasi- experimentation is falsificationistin that it requiresexperimenters to identify a causalclaim and then to generate and examineplausiblealternativeexplanations that might falsify the claim. However,suchfalsificationcan neverbe as definitiveas Popperhoped.Kuhn (7962) pointed out that falsificationdepends on two assumptions that can never be fully tested.The first is that the causalclaim is perfectlyspecified. But that is neverih. ."r.. So many featuresof both the claim and the test of the claim are debatable-for example,which outcome is of interest,how it is measured, the conditionsof treatment,who needstreatment, and all the many other decisions that researchers must make in testingcausalrelationships. As a result, disconfir- mation often leadstheoriststo respecify part of their causaltheories.For example, they might now specifynovel conditionsthat must hold for their theory to be irue and that were derivedfrom the apparentlydisconfirmingobservations. Second, falsificationrequiresmeasures that are perfectlyvalid reflectionsof the theory being tested.However,most philosophersmaintain that all observationis theorv-laden. It is laden both with intellectualnuancesspecificto the partially</region>
          <region class="DoCO:FigureBox" id="Fx94">
            <image class="DoCO:Figure" src="62p6.page_016.image_16.png" thmb="62p6.page_016.image_16-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="sidenote" id="95" page="17" column="1">16</outsider>
          <region class="unknown" id="96" page="17" column="1">I 1. EXPERIMENTS AND GENERALIZED CAUSAL INFERENCE I</region>
          <region class="unknown" id="97" page="17" column="1">uniquescientificunderstandings of the theory held by the individual or group de- vising the test and also with the experimenters' extrascientific wishes,hopes, aspirations,and broadly shared cultural assumptionsand understandings. If measures are not independent of theories, how can they provideindependent theory tests,includingtestsof causaltheories? If the possibilityof theory-neutral observations is denied,with them disappears the possibilityof definitiveknowledge both of what seems to confirm a causalclaim and of what seems to disconfirmit. Nonetheless, a fallibilist versionof falsificationis possible. It argues that stud- iesof causalhypotheses can still usefullyimproveunderstanding of general trends despite ignorance of all the contingencies that might pertainto thosetrends.It ar- guesthat causalstudiesare usefulevenif w0 haveto respecify the initial hypoth- esisrepeatedly to accommodate new contingencies and new understandings. After all, those respecifications are usually minor in scope;they rarely involve wholesale overthrowing of generaltrendsin favor of completely oppositetrends. Fallibilist falsificationalso assumes that theory-neutralobservation is impossible but that observations can approacha more factlikestatuswhenthey havebeenre- peatedlymadeacross differenttheoreticalconceptions of a construct,across multiple kinds of measurements, and at multiple times.It alsoassumes that observations are imbued with multiple theories, not iust one, and that different operationalprocedures do not sharethe samemultiple theories. As a result,ob- servations that repeatedlyoccur despitedifferent theoriesbeing built into them havea special factlike statusevenif they can neverbe fully theory-neutral facts. In summary, then, fallible falsificationis more than just see- ing whether observationsdisconfirm a prediction. It involvesdiscoveringand judging the worth of ancillary assumptions about the restrictedspecificityof the causalhypothesis under test and also about the heterogeneity of theories,view- points, settings, and times built into the measures of the causeand effectand of any contingencies modifying their relationship. It is neitherfeasible nor desirable to rule out all possiblealternative interpre- tarionsof a causalrelationship.Instead,only plausiblealternatives constitutethe major focus.This serves partly to keep matterstractablebecause the number of possiblealternatives is endless. It also recognizes that many alternatives have no seriousempiricalor experiential support and so do not warrant special attention. However,the lack of supportcan sometimes be deceiving. For example, the cause of stomachulcerswas long thought to be a combination of and excessacid production. Few scientistsseriouslythought that ulcers were caused by a pathogen (e.g., virus,germ,bacteria) because it was assumed that an acid-filled stomachwould destroy all living organisms. However,in L982 Aus- 'Warren tralian researchers Barry Marshall and Robin discovered spiral-shaped bacteria,later name d Helicobacter pylori (H. pylori), in ulcerpatients'stomachs. rilfith this discovery, the previouslypossiblebut implausiblebecame plausible. By "1994, a U.S. National Institutesof Health Consensus Development Conference</region>
          <region class="DoCO:TextChunk" id="98" confidence="possible" page="17" column="1">concluded that H. pylori was the major causeof most peptic ulcers. So labeling ri-</region>
          <region class="DoCO:FigureBox" id="Fx99">
            <image class="DoCO:Figure" src="62p6.page_017.image_17.png" thmb="62p6.page_017.image_17-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="100" page="18" column="1">I MODERN DESCRTPTONS OFEXPERIMENTS I tt I</outsider>
          <region class="DoCO:TextChunk" id="101" confidence="possible" page="18" column="1">val hypothesesas plausible dependsnot just on what is logically possible but on social consensus, shared experienceand, empirical data. Because such factors are often context specific, different substantive areasde- velop their own lore about which alternatives are important enough to need to be controlled, even developing their own methods for doing so. In early psychologg for example, a control group with pretest observations was invented to control for the plausible alternative explanation that, by giving practice in answering test content, pretestswould produce gains in performance even in the absenceof a treatment effect (Coover 6c Angell, 1907). Thus the focus on plausibility is a two-edged sword: it reducesthe range of alternatives to be considered in quasi-experimental work, yet it also leavesthe resulting causal inference vulnerable to the discovery that an implausible-seemingalternative may later emerge as a likely causal agent.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="102" confidence="possible" page="18" column="1">NaturalExperiment</h2>
          <region class="DoCO:TextChunk" id="103" page="18" column="1">The term natural experiment describesa naturally-occurring contrast between a treatment and a comparisoncondition (Fagan, 1990; Meyer, 1995;Zeisel,1,973l. Often the treatments are not even potentially manipulable, as when researchers retrospectivelyexamined whether earthquakesin California causeddrops in property values (Brunette, 1.995; Murdoch, Singh, 6c Thayer, 1993). Yet plausible causal inferences about the effects of earthquakes are easy to construct and defend. After all, the earthquakesoccurred before the observations on property val- ues,and it is easyto seewhether earthquakesare related to properfy values. A useful source of counterfactual inference can be constructed by examining property values in the same locale before the earthquake or by studying similar localesthat did not experience an earthquake during the bame time. If property values dropped right after the earthquake in the earthquake condition but not in the comparison condition, it is difficult to find an alternative explanation for that drop. Natural experiments have recently gained a high profile in economics. Before the 1990s economists had great faith in their ability to produce valid causal in- ferencesthrough statistical adjustments for initial nonequivalence between treatment and control groups. But two studies on the effects of job training programs showed that those adjustments produced estimates that were not close to those generated from a randomized experiment and were unstable across tests of the model's sensitivity (Fraker 6c Maynard, 1,987; Lalonde, 1986). Hence, in their searchfor alternative methods, many economistscame to do natural experiments, such as the economic study of the effects that occurred in the Miami job market when many prisoners were releasedfrom Cuban jails and allowed to come to the United States(Card, 1990). They assumethat the releaseof prisoners (or the tim- ing of an earthquake) is independent of the ongoing processesthat usually affect unemployment rates (or housing values). Later we explore the validity of this assumption-of its desirability there can be little question.</region>
          <region class="DoCO:FigureBox" id="Fx104">
            <image class="DoCO:Figure" src="62p6.page_018.image_18.png" thmb="62p6.page_018.image_18-thumb.png"/>
          </region>
          <region class="unknown" id="105" page="19" column="1">18 I 1. EXPERIMENTS AND GENERALIZED CAUSAL INFERENCE</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="106" confidence="possible" page="19" column="1">Nonexperimental Designs</h2>
          <region class="unknown" id="107" page="19" column="1">The termscorrelationaldesign,passive observational design, and nonexperimental designrefer to situationsin which a presumedcauseand effect are identified and measuredbut in which other structural featuresof experiments are missing.Ran- dom assignment is not part of the design,nor are suchdesignelements as pretests and control groupsfrom which researchers might constructa usefulcounterfactual inference. Instead,relianceis placedon measuring alternativeexplanations individually and then statisticallycontrolling for them. In cross-sectional studies in which all the data aregatheredon the respondents at one time, the researcher may not even know if the causeprecedes the dffect. When thesestudiesare used for causalpurposes, the missingdesignfeatures can be problematic unless much is already known about which alternativeinterpretations are plausible,unlessthose that are plausiblecan be validly measured, and unless the substantive model used for statistical adjustment is well-specified. These are difficult conditionsto meetin the real world of research practice,and thereforemany commentators doubt the potentialof suchdesigns to supportstrongcausalinferences in most cases.</region>
          <region class="unknown" id="108" page="19" column="1">EXPERIMENTS AND THEGENERALIZATION OF CAUSAL CONNECTIONS</region>
          <region class="DoCO:TextChunk" id="109" confidence="possible" page="19" column="1">The strength of experimentation is its ability to illuminate causal inference. The weaknessof experimentation is doubt about the extent to which that causal rela- 'We tionship generalizes. hope that an innovative feature of this book is its focus on generalization. Here we introduce the general issuesthat are expanded in later chapters.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="110" confidence="possible" page="19" column="1">Most Experiments Are HighlyLocalBut Have</h2>
          <region class="unknown" id="111" page="19" column="1">GeneralAspirations</region>
          <region class="unknown" id="112" page="19" column="1">Most experiments are highly localizedand particularistic. They are almostalways conductedin a restrictedrange of settings, often just one, with a particular version of one type of treatmentrather than, say,a sampleof all possibleversions. Usually they have several measures-eachwith theoreticalassumptions that are differentfrom thosepresentin other measures-but far from a complete set of all possiblemeasures. Each experimentnearly always usesa convenientsampleof people rather than one that reflectsa well-described population; and it will inevitably be conducted at a particular point in time that rapidly becomes history. Yet readers of experimental results are rarelyconcerned with what happened in that particular,past,local study.Rather,they usuallyaim to learn eitherabout theoreticalconstructs of interestor about alarger policy.Theoristsoften want to</region>
          <region class="DoCO:FigureBox" id="Fx113">
            <image class="DoCO:Figure" src="62p6.page_019.image_19.png" thmb="62p6.page_019.image_19-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="114" page="20" column="1">EXeERTMENTS AND THEGENERALIZATION OFCAUSAL CONNECTIONS I t'</outsider>
          <region class="DoCO:TextChunk" id="115" page="20" column="1">connect experimental results to theories with broad conceptual applicability, which ,.q,rir., generalization at the linguistic level of constructs rather than at the level of the operations used to represent these constructs in a given experiment. They nearly always want to generallzeto more people and settings than are rep- resentedin a single experiment. Indeed, the value assignedto a substantive theory usually dependson how broad a rangeof phenomena the theory covers. SimilarlS policymakers may be interested in whether a causal relationship would hold iprobabilistically) across the many sites at which it would be implemented as a policS an inferencethat requires generalization beyond the original experimental stody contexr. Indeed, all human beings probably value the perceptual and cognitive stability that is fostered by generalizations. Otherwise, the world might appear as a btulzzing cacophony of isolqted instances requiring constant cognitive processingthat would overwhelm our limited capacities. In defining generalizationas a problem, we do not assumethat more broadly applicable resulti are always more desirable(Greenwood, 1989). For example, physicists -ho use particle accelerators to discover new elements may not expect that it would be desiiable to introduce such elementsinto the world. Similarly, social scientists sometimes aim to demonstrate that an effect is possible and to understand its mechanismswithout expecting that the effect can be produced more generally. For instance, when a "sleeper effect" occurs in an attitude change study involving per- suasivecommunications, the implication is that change is manifest after a time delay but not immediately so. The circumstancesunder which this effect occurs turn out to be quite limited and unlikely to be of any general interest other than to show that the theory predicting it (and many other ancillary theories) may not be wrong (Cook, Gruder, Hennigan &amp; Flay l979\.Experiments that demonstrate limited generalization may be just as valuable as those that demonstratebroad generalization. Nonetheless, a conflict seems to exist berweenthe localized nature of the causal knowledge that individual experiments provide and the more generalizedcausal goals that researchaspiresto attain. Cronbach and his colleagues(Cronbach et al., f gSO;Cronbach, 19821havemade this argument most forcefully and their works have contributed much to our thinking about causal generalization. Cronbach noted that each experiment consistsof units that receivethe experiences being contrasted, of the treaiments themselves , of obseruations made on the units, and of the settings in which the study is conducted. Taking the first letter from each of these four iords, he defined the acronym utos to refer to the "instances on which data are collected" (Cronb ach, "1.982,p. 78)-to the actual people,treatments' measures' and settingsthat were sampledin the experiment. He then defined two problems of generalizition: (1) generaliiing to the "domain about which [the] question is asked" (p.7g),which he called UTOS; and (2) generalizingto "units, treatments,variables, "nd r.r,ings not directly observed" (p. 831,*hi.h he called oUTOS.e</region>
          <region class="unknown" id="116" page="20" column="1">9. We oversimplify for pedagogical reasons.For example,Cronbach only usedcapital S, not small s, so that his system,eferred only to ,tos, not utos. He offered diverseand not always consistentdefinitions of UTOS and *UTOS, in particular. And he doesnot usethe word generalizationin the samebroad way we do here.</region>
          <region class="DoCO:FigureBox" id="Fx117">
            <image class="DoCO:Figure" src="62p6.page_020.image_20.png" thmb="62p6.page_020.image_20-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="118" page="21" column="1">I</outsider>
          <outsider class="DoCO:TextBox" type="sidenote" id="119" page="21" column="1">20</outsider>
          <region class="unknown" id="120" page="21" column="1">I 1. EXPERIMENTS AND GENERALIZED CAUSAL INFERENCE</region>
          <region class="unknown" id="121" page="21" column="1">Our theoryof causalgeneralization, outlinedbelowand presented in more detail in ChaptersLL through 13, melds Cronbach's thinking with our own ideas about generalization from previousworks (Cook, 1990, t99t; Cook 6c Camp- bell,1979), creatinga theory that is differentin modestways from both of these predecessors. Our theory is influencedby Cronbach's work in two ways.First, we follow him by describingexperiments consistently throughout this book as con- sistingof the elements of units, treatments, observations, and settingsrlo though we frequentlysubstitute personsfor units giventhat most field experimentation is conducted with humansas participants. :We also often substituteoutcomef.orob- seruations given the centrality of observations about outcomewhen examining causal relationships. Second, we acknowledge that researchers are ofteninterested in two kinds of.generalization about eachof thesefive elements, and that these two typesareinspiredbg but not identicalto, the two kinds of generalization that Cronbach defined. 'We call these construct validity generalizations (inferences about the constructs that research operations represent) and externalvalidity generalizations (inferences about whetherthe causalrelationship holds overvariation in persons, settings, treatment,and measurement variables).</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="122" confidence="possible" page="21" column="1">Construct Validity: Causal Generalization as Representation</h2>
          <region class="DoCO:TextChunk" id="127" page="21" column="1">The first causal generalization problem concerns how to go from the particular units, treatments, observations, and settings on which data are collected to the higher order constructs these instancesrepresent.These constructs are almost always couched in terms that are more abstract than the particular instancessam- pled in an experiment. The labels may pertain to the individual elementsof the experiment (e.g., is the outcome measured by a given test best described as intelligence or as achievement?).Or the labels may pertain to the nature of relationships among elements, including causal relationships, as when cancer treatments are classified as cytotoxic or cytostatic depending on whether they kill tumor cells directly or delay tumor growth by modulating their environment. Consider a randomized experiment by Fortin and Kirouac (1.9761. The treatment was a brief educational course administered by severalnurses,who gave a tour of their hospital and covered some basic facts about surgery with individuals who were to have elective abdominal or thoracic surgery 1-5to 20 days later in a single Montreal hospital. Ten specific outcome measureswere used after the surgery, such as an activities of daily living scaleand a count of the analgesics used to control pain. Now compare this study with its likely t^rget constructs-whether <marker type="page" number="22"/><marker type="block"/> patient education (the target cause)promotes physical recovery (the targ€t effect) "*ong surgical patients (the target population of units) in hospitals (the target univeise ofiettings). Another example occurs in basic research,in which the question frequently aiises as to whether the actual manipulations and measuresused in an experiment really tap into the specific cause and effect constructs specified by the theory. One way to dismiss an empirical challenge to a theory is simply to make the casethat the data do not really represent the concepts as they are specified in the theory. Empirical resnlts often force researchers to change their initial understanding of whaithe domain under study is. Sometimesthe reconceptuahzation leads to a more restricted inference about what has been studied. Thus the planned causal agent in the Fortin and Kirouac (I976),study-patie,nt education-might need to b! respecified as informational patient education if the information component of the treatment proved to be causally related to recovery from surgery but the tour of the hospital did not. Conversely data can sometimes lead researchersto think in terms o?,"rg., constructs and categoriesthat are more general than those with which they began a researchprogram. Thus the creative analyst of patient education studies mlght surmise that the treatment is a subclass of interventions that function by increasing "perceived control" or that recovery from surgery can be treated as a subclas of ;'p.tronal coping." Subsequentreaders of the study can even add their own interpietations, perhaps claiming that perceived control is really just a special caseof the even more general self-efficacy construct. There is a sobtie interplay over time among the original categories the researcherintended to represeni, the study as it was actually conducted, the study results, and subse- qrr..ri interpretations. This interplay can change the researcher'sthinking about what the siudy particulars actually achieved at a more conceptual level, as can feedback fromreaders. But whatever reconceptualizationsoccur' the first problem of causal generaltzationis always the same: How can we generalizefrom a sample of instancesand the data patterns associatedwith them to the particular target constructs they represent?</region>
          <region class="unknown" id="124" page="21" column="1">10. refer to time as a separate featureof experiments, following Campbell (79571and Cook and Campbell time can cut acrossthe other factorsindependently. Cronbachdid not includetime in his notational system, insteadincorporating time into scheduling of treatment),observations (e.g.,when measures are administered), or setting (e.g.,the historicalcontext of the experiment).</region>
          <region class="DoCO:FigureBox" id="Fx125">
            <image class="DoCO:Figure" src="62p6.page_021.image_21.png" thmb="62p6.page_021.image_21-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="126" page="22" column="1">EXnERTMENTs AND THE GENERALIZATIoN oF cAUsAL coNNEcrtoNS| ,, I</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="128" confidence="possible" page="22" column="1">External Validity: Causal Generalization as Extrapolation</h2>
          <region class="unknown" id="129" page="22" column="1">The of generalization is to infer whether a causalrelationship p.rrorrt, settings, treatments, and outcomes. For example, resultsof an the effectsof a kindergarten Head Startprogiam on the subsequent of poor African Memphis during the 1980smay want to know if a programwith partially overlapping socialdevelopment goals_would be aseffective in improvingthi mathematics test scores of poor Hispanicchildren in Dallas if this programwere to be implemented tomorrow. This exampl. again reminds us that generahzation is not a synonym for broader applicatiorr.H.r., generahzation is from one city to another city and</region>
          <region class="DoCO:FigureBox" id="Fx130">
            <image class="DoCO:Figure" src="62p6.page_022.image_22.png" thmb="62p6.page_022.image_22-thumb.png"/>
          </region>
          <region class="unknown" id="131" page="23" column="1">1. EXPERIMENTS AND GENERALIZED CAUSAL INFERENCE</region>
          <region class="unknown" id="132" page="23" column="1">from one kind of clienteleto anotherkind, but thereis no presumptionthat Dal-</region>
          <region class="DoCO:TextChunk" id="133" confidence="possible" page="23" column="1">las is somehow broader than Memphis or that Hispanic children constitute a</region>
          <region class="unknown" id="134" page="23" column="1">broader population than African American children. Of course,some generalizations are from narrow to broad. For example,a researcher who randomly samplesexperimentalparticipants from a national population may generalize (probabilistically) from the sampleto all the other unstudied members of that same population. Indeed,that is the rationale for choosingrandom selectionin the first place.Similarly when policymakers considerwhetherHead Start should be continuedon a national basis,they are not so interested in what happened in Memphis.They are more interested in what would happenon the average across the United States, as its many local programsstill differ from eachother despite efforts in the 1990sto standardize much of what happens to Head Startchildren and parents.But generalization can also go from the broad to the narrow. Cron- example of an experiment that studied differences between the performances of groups of and public schools.In this case, the concernof individual parentsis to know which type of schoolis better for their particular child, not for the whole group. \Thether from narrow to broad, broad to narroq or across units at about the samelevelof aggregation, all theseexamples of externalvalidity questions share the sameneed-to infer the extent to which the effect holds over variationsin persons, settings, treatments, or outcomes.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="135" confidence="possible" page="23" column="1">Approaches to MakingCausal Generalizations</h2>
          <region class="DoCO:TextChunk" id="136" page="23" column="1">\Thichever way the causal generalization issue is framed, experiments do not seem at first glance to be very useful. Almost invariablS a given experiment uses a limited set of operations to represent units, treatments, outcomes, and settings. This high degree of localization is not unique to the experiment; it also charac- terizes case studies, performance monitoring systems, and opportunistically- administered marketing questionnaires given to, say, a haphazard sample of respondents at local shopping centers (Shadish, 1995b). Even when questionnaires are administered to nationally representative samples, they are ideal for representing that particular population of persons but have little relevanceto citizens outside of that nation. Moreover, responsesmay also vary by the setting in which the interview took place (a doorstep, a living room, or a work site), by the time of day at which it was administered, by how each question was framed, or by the particular race, age,and gender combination of interviewers. But the fact that the experiment is not alone in its vulnerability to generalization issuesdoes not make it any less a problem. So what is it that justifies any belief that an experiment can achieve a better fit between the sampling particulars of a study and more general inferences to constructs or over variations in persons, settings, treatments, and outcomes?</region>
          <region class="DoCO:FigureBox" id="Fx137">
            <image class="DoCO:Figure" src="62p6.page_023.image_23.png" thmb="62p6.page_023.image_23-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="138" page="24" column="1">EXeERTMENTs AND THE GENERALtzATtoN oF cAUsAL coNNEcrtoNs I tt</outsider>
          <region class="unknown" id="139" page="24" column="1">Samplingand Causal Generalization The methodmost often recommended for achieving this closefit is the useof formal probabiliry samplingof instances of units, treatments, observations, or settings (Rossi,Vlright, &amp; Anderson,L983). This presupposes that we have clearly deiineated populationsof eachand that we can samplewith known probability from within eachof effect,this entailsthe random selection of instances, to be carefullydistinguished from random assignment discussed earlier in this chapter. Randomselection involvesselecting cases by chanceto repre- sentthat popuiation,whereas random assignment involvesassigning cases to multiple conditions. In cause-probing research that is not experimental, random samples of indi- viduals"r. oft.n nr.d. Large-scale longitudinalsurveys such asthe PanelStudyof the National Longitudinal Surveyare usedto represent the populationof the United States-or certainagebrackets within it-and measures Lf pot.ntial causes and effectsare then relatedto each other using time lags in ,nr^"r.rr.-ent and statistical controlsfor group nonequivalence. All this is donein hopesof a randomizedexperiment achieves. However,cases of random ielection from a broad population followed by random assignment from within this population are much rarer (seeChapter 12 for examples). Also rare arestudies oi t".rdotn selection followed by a quality quasi-experiment. Such experiments require a high levelof resources and a degree of logisticalcontrol that is iarely feasible, so many researchers prefer to rely on an implicit set of nonsta- tistical heuristics for generalization that we hope to make more explicit and sys- tematicin this book. Random selection occurs even more rarely with settings than with people.Consider the outcomes observed in an experiment. How ofterrlre they raniomly sampled?'We grant that the domain samplingmodel of classical test iheory (Nunnally 6c that the itemsusedto measure a sampledfrom a domain of all possible items. However,in actual researchers ever randomly constructing measures. Nor do they do so when choosing ma- nipulationsor settings. For instance, many settings will not agreeto be sampled, "rid ,o1n. of the settings that agreeto be randomly sampled will almostcertainly not agree to be randomlyassigned to treatments, no definitivelist of poisible treatments is most obvious in areasin which treat-*,, are being discovered and developed rapidly, such as in AIDS research. In general, then, random samplingis alwaysdesirable, but it is only rarely and con- tingently "However, feasible. formal sampling methodsare not the only option. Two informal, purposive samplingmethodrare sometimes useful-purposive sampling of heterogeneousinstances and typical instances. In the former case'the aim is to includeinrLni.r chosendeliberately to reflect diversity on presumptively important dimensions, even though the sampleis not formally random. In the latter</region>
          <region class="DoCO:FigureBox" id="Fx140">
            <image class="DoCO:Figure" src="62p6.page_024.image_24.png" thmb="62p6.page_024.image_24-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="141" confidence="possible" page="25" column="1">24 I .l. TxpEnIMENTS AND GENERALIZED CAUSAL INFERENCE</region>
          <region class="DoCO:TextChunk" id="146" page="25" column="1">case,the aim is to explicate the kinds of units, treatments, observations, and settings to which one most wants to generalize andthen to selectat least one instance of each class that is impressionistically similar to the class mode. Although these purposive sampling methods are more practical than formal probability sampling, they are not backed by a statistical logic that justifies formal generalizations.Nonetheless, they are probabty the most commonly used of all sampling methods for facilitating generalizations. A task we set ourselvesin this book is to explicate such methods and to describe how they can be used more often than is the casetoday. However, sampling methods of any kind are insufficient to solve either problem of generalization. Formal probability sampling requires specifying a target population from which sampling then takes place, but defining such populations is difficult for some targets of generalization such as treatments. Purposive sampling of heterogeneousinstancesis differentially feasible for different elementsin a study; it is often more feasible to make measuresdiverse than it is to obtain diverse settings, for example. Purposive sampling of typical instancesis often feasible when target modes, medians, or means are known, but it leaves questions about generalizations to a wider range than is typical. Besides, as Cronbach points out, most challenges to the causal generalization of an experiment typically emerge after a study is done. In such cases,sampling is relevant only if the in- stancesin the original study were sampled diversely enough to promote responsible reanalysesof the data to seeif a treatment effect holds acrossmost or all of the targets about which generahzation has been challenged. But packing so many sourcesof variation into a single experimental study is rarely practical and will almost certainly conflict with other goals of the experiment. Formal sampling methods usually offer only a limited solution to causal generalizationproblems. A theory of generalizedcausal inference needsadditional tools. <marker type="page" number="26"/><marker type="block"/> 2. Ruling Out lrreleuancies.They identify those things that are irrelevant because they do not change a generalization. 3. Making Discriminations. They clarify k.y discriminations that limit generalization. 4. Interpolation and Extrapolation. They make interpolations to unsampled values within the range of the sampled instances and, much more difficult, they explore extrapolations beyond the sampled range. 5 . Causal Explanation. They develop and test explanatory theories about the pattern of effects,causes,and mediational processes that are essentialto the transfer of a causalrelationship. In this book, we want to show how scientistscan and do use thesefive principles to draw generalizedconclusions dbout a causal connection. Sometimes the conclusion is about the higher order constructs to use in describing an obtained connection at the samplelevel. In this sense, these five principles have analoguesor parallels both in the construct validity literature (e.g.,with construct content, with loru.rg.nt and discriminant validity, and with the need for theoretical rationales for consrructs) and in the cognitive scienceand philosophy literatures that study how people decidewhether instancesfall into a category(e.g.,concerning the roles that protorypical characteristicsand surface versus deep similarity play in determining category membership). But at other times, the conclusion about generalization refers to whether a connection holds broadly or narrowly over variations in persons, settings,treatments, or outcomes. Here, too, the principles have analogues or parallels that we can recognizefrom scientific theory and practice, as in the study of dose-response relationships (a form of interpolation-extrapolation) or the appeal to explanatory mechanismsin generalizing from animals to humans (a form of causal explanation). Scientistsuse rhese five principles almost constantly during all phases of re- search.For example, when they read a published study and wonder if some variation on the study's particulars would work in their lab, they '$7hen think about similarities of the published study to what they propose to do. they conceptualize the new study, they anticipate how the instancesthey plan to study will match the prototypical featuresof the constructs about which they are curious. They may de- iign their study on the assumptionthat certain variations will be irrelevant to it but that others will point to key discriminations over which the causal relationship does not hold or the very character of the constructs changes.They may include measuresof key theoretical mechanisms to clarify how the intervention works. During data analysis, they test all these hypotheses and adjust their construct descriptions to match better what the data suggest happened in the study. The introduction section of their articles tries to convince the reader that the study bears on specific constructs, and the discussion sometimes speculatesabout how results -igttt extrapolate to different units, treatments, outcomes, and settings. Further, practicing scientistsdo all this not just with single studies that they read or conduct but also with multiple studies. They nearly always think about</region>
          <region class="unknown" id="143" page="25" column="1">A GroundedTheoryof Causal Generalization Practicingscientists routinely make causal generalizations in their research, and they almostneveruseformal probability samplingwhen they do. In this book, we presenta theory of causalgeneralization that is groundedin the actualpracticeof science (Matt, Cook, 6c Shadish, 2000). Although this theory was originally de- velopedfrom ideasthat were groundedin the constructand externalvalidiry literatures (Cook, 1990,1991.),we have since found that these ideas arecommonin a diverse literatureabout scientificgeneralizations (e.g.,Abelson,1995;Campbell &amp; Fiske, 1.959; Cronbach&amp; Meehl, 1955; Davis, 1994; Locke, 1'986; Medin, 1989;Messick, 1ggg,1'995; Rubins,1.994;'Willner, 1,991';'$7ilson, Hayward,Tu- nis, Bass, &amp; Guyatt, 1,995];t. \7e providemore details aboutthis grounded theory in Chapters1L through L3, but in brief it suggests that scientists make causal generalizations in their work by usingfive "L. SurfaceSimilarity.They assess the operations and the prototypicalcharacteristics of the target of generalization.</region>
          <region class="DoCO:FigureBox" id="Fx144">
            <image class="DoCO:Figure" src="62p6.page_025.image_25.png" thmb="62p6.page_025.image_25-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="145" page="26" column="1">I EXPERIMENTS AND THEGENERALIZATION OF CAUSAL CONNECTIONS I ZS I</outsider>
          <region class="DoCO:FigureBox" id="Fx147">
            <image class="DoCO:Figure" src="62p6.page_026.image_26.png" thmb="62p6.page_026.image_26-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="148" confidence="possible" page="27" column="1">26 1. EXPERTMENTS AND GENERALIZED CAUSAL INFERENCE |</region>
          <region class="DoCO:TextChunk" id="149" confidence="possible" page="27" column="1">how their own studiesfit into a larger literature about both the constructsbeing measured and the variables that may or may not bound or explain a causalconnection, often documenting this fit in the introduction to their study.And they apply all five principleswhen they conduct reviewsof the literature,in which they make inferences about the kinds of generalizations that a body of research can suppoft. Throughoutthis book, and especially in Chapters11 to L3, we providemore detailsabout this groundedtheory of causal generalization and about the scientific practices that it suggests. Adopting this grounded theory of generalization does not imply a rejectionof formal probabilitysampling.Indeed, we recommend such sampling unambiguously when it is feasible, along with purposive sampling schemes to aid generalization when formal randomselection methods cannot be implemented. But we alsoshow that samplingis just one methodthat practicingscientists use to make causalgeneralizations, along with practical logic, applicationof diverse statistical methods,and useof features of designother than sampling.</region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="150" page="27" column="1">EXPERIMENTS AND METASCIENCE</h1>
        <region class="DoCO:TextChunk" id="151" confidence="possible" page="27" column="1">Extensivephilosophicaldebatesometimes surroundsexperimentation. Here we briefly summarize some key featuresof these debates, and then we discuss some implications of thesedebatesfor experimentation. However,there is a sense in which all this philosophical debate is incidentalto the practiceof experimentation. Experimentationis as old as humanity itself, so it preceded humanity'sphilo- sophicaleffortsto understand causation and genenlization by thousands of years. Even over just the past 400 yearsof scientificexperimentation, we can seesome constancyof experimentalconcept and method, whereasdiversephilosophical conceptions of the experiment have comeand gone. As Hacking(1983)said, "Experimentation has a life of its own" (p. 150). It has beenone of science's most powerful methodsfor discovering descriptive causal relationships, and it hasdone so well in so many ways that its placein science is probably assured forever.To justify its practicetodag a scientist need not resortto sophisticated philosophical reasoningabout experimentation. Nonetheless, it doeshelp scientists to understand these philosophical debates. For example,previousdistinctionsin this chapterbetweenmolar and molecular causation,descriptiveand explanatorycause,or probabilisticand deterministic causalinferences all help both philosophersand scientists to understandbetter both the purposeand the resultsof experiments (e.g., Bunge,1959; Eells, 1991'; Hart &amp; Honore, 1985;Humphreys,"t989; Mackie, 1'974; Salmon,7984,1989; Sobel,1993;P.A. \X/hite,1990).Here we focus on a differentand broadersetof critiquesof science itself, not only from philosophybut alsofrom the history,so- ciologS and psychology of science (see usefulgeneral reviewsby Bechtel, 1988; H. I. Brown, 1977; Oldroyd, 19861. Some of theseworks have beenexplicitly about the nature of experimentation, seeking to createa justified role for it (e.g.,</region>
        <region class="DoCO:FigureBox" id="Fx152">
          <image class="DoCO:Figure" src="62p6.page_027.image_27.png" thmb="62p6.page_027.image_27-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="153" page="28" column="1">I EXPERIMENTS AND METASCIENCE I 27</outsider>
        <region class="DoCO:TextChunk" id="154" confidence="possible" page="28" column="1">'1.990; Bhaskar, L975; Campbell,1982,,1988; Danziger, S. Drake, l98l; Gergen, 1,973; Gholson, Shadish, Neimeyer,6d Houts, L989;Gooding,Pinch,6cSchaffer, 'Woolgar, 1,989b;Greenwood, L989; Hacking, L983; Latour, 1'987;Latour 6c 1.979;Morawski, 1988; Orne,1.962;R. RosenthaL,1.966;Shadish &amp; Fuller, L994; Shapin,1,9941. These critiqueshelp scientists to seesomelimits of experimentation in both science and society.</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="155" confidence="possible" page="28" column="1">TheKuhnian Critique</h2>
          <region class="DoCO:TextChunk" id="156" confidence="possible" page="28" column="1">Kuhn (1962\ described scientificrevolutionsas differentand partly incommensu- rableparadigms that abruptly succeedgd each other in time and in which the grad- ual accumulation of scientific knowledge was a chimera. Hanson(1958),Polanyi (1958),Popper('J.959), Toulmin (1'961), Feyerabend (L975),and Quine (1'95t' 1,969) contributedto the critical momentum, in part by exposingthe grossmis- takesin logicalpositivism's attemptto build a philosophyof science based on re- constructinga successful science such as physics.All thesecritiquesdeniedany firm foundationsfor scientificknowledge(so, by extension,experiments do not provide firm causalknowledge). The logicalpositivistshopedto achieve founda- tions on which to build knowledgeby tying all theory tightly to theory-freeob- servationthrough predicatelogic. But this left out important scientificconcepts that could not be tied tightly to observation; and it failed to recognize that all observations are impregnated with substantive and methodological theory,making it impossible to conducttheory-free tests.lt The impossibility of theory-neutral observation (often referred to as the Quine-Duhem thesis) impliesthat the resultsof any singletest (and so any single experiment) are inevitably ambiguous. They could be disputed,for example,on groundsthat the theoreticalassumptions built into the outcome measurewere wrong or that the study made a fatity assumptionabout how high a treatment dosewas requiredto be effective. Some of theseassumptions are small,easilyde- tected,and correctable, such aswhen a voltmeter givesthe wrong readingbecause the impedance of the voltagesource was much higherthan that of the meter ('$fil- son, L952).But other assumptions are more paradigmlike, impregnating a theory so completely that other parts of the theory makeno sense without them (e.g.,the assumption that the earthis the centerof the universe in pre-Galilean astronomy). Because the number of assumptions involved in any scientifictest is very large, researchers can easily find some assumptions to fault or can even posit new</region>
          <region class="unknown" id="157" page="28" column="1">11. However, Holton (1986) reminds us nor to overstatethe relianceof positivistson empirical data: "Even the father of positivism,AugusteComte, had written . . . that without a theory of somesort by which to link phenomenato some principles 'it would not only be impossibleto combine the isolatedobservationsand draw any usefulconclusions,we would not evenbe able to rememberthem, and, for the most part, the fact would not be noticed by our eyes"' (p. 32). Similarly, Uebel (1992) providesa more detailedhistorical analysisof the protocol sentence debate in logical positivism, showing somesurprisinglynonstereorypical positions held by key playerssuch as Carnap.</region>
          <region class="DoCO:FigureBox" id="Fx158">
            <image class="DoCO:Figure" src="62p6.page_028.image_28.png" thmb="62p6.page_028.image_28-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="sidenote" id="159" page="29" column="1">28</outsider>
          <region class="unknown" id="160" page="29" column="1">r. rxeenlMENTs AND GENERALIZED CAUSAL INFERENCE |</region>
          <outsider class="DoCO:TextBox" type="sidenote" id="161" page="29" column="1">I</outsider>
          <region class="unknown" id="162" page="29" column="1">assumptions (Mitroff &amp; Fitzgerald,1.977).In this way, substantive theoriesare lesstestablethan their authors originally conceived. How cana theory be tested if it is madeof clayrather than granite? For reasons we clarify later,this critique is more true of singlestudiesand less true of programsof research. But evenin the latter case, undetected constantbiases ."tt t.r,tlt in flawed inferences about cause and its genenlization.As a result,no ex- perimentis everfully certain,and extrascientific beliefs and preferences always have ioo- to influencethe many in all scientificbelief.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="163" confidence="possible" page="29" column="1">ModernSocial Psychological Critiques</h2>
          <region class="unknown" id="164" page="29" column="1">Sociologists working within epistemological relativism, and the strongprogram (e.g.,Barnes,1974; Bloor, 1976; Collins, l98l;Knorr-Cetina,L981-; Latour 6c'Woolgar,1.979;Mulkay, 1'979)have shown thoseextrascientific processes at work in science. Their empiricalstudies show that scientists often fail to adhereto norms part of good science (e.g., objectivity neutrality,sharingof information).They havealso rho*n how that which comesto be reportedas partly de- terminedby socialand psychological forces and partly by issues of economicand political power both within science and in the largersociety-issues that arerarely mention;d in publishedresearch reports.The most gistsattributesall scientific knowledgeto suchextrascientific processes, claiming ihat "the natural world has a small or nonexistent role in the construction of sci- "l'98I, p. 3). Collins doesnot denyontologicalrea.lism, that real the world. Rather,he that whateverexternal reality may existcanconstrainour scientifictheories. For example, if atomsreally exist, do they affectour scientifictheoriesat all? If our theory postulates an atom, is it describin g a realentitythat existsroughly aswe describe it? Epistetnologi,cal rel- atiuistssuch as Collins respondnegativelyto both questions, believingthat the most important influences in science are social,psychological, economic, and political, "ttd th"t thesemight evenbe the only influences on scientific theories- This view is not widely endorsed outsidea small group of sociologists, but it is a useful counterweight to naiveassumptions that scientificstudies somehow directlyre- veal natur. to assumptiorwe resultsof all studies, including experiments, are profoundly subjectto theseextrascientific influences, from their conception to reportsof their results.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="165" confidence="possible" page="29" column="1">Science and Trust</h2>
          <region class="DoCO:TextChunk" id="166" confidence="possible" page="29" column="1">A standard image of the scientist is as a skeptic, a person who only trusts results that have been personally verified. Indeed, the scientific revolution of the'l'7th century</region>
          <region class="DoCO:FigureBox" id="Fx167">
            <image class="DoCO:Figure" src="62p6.page_029.image_29.png" thmb="62p6.page_029.image_29-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="168" page="30" column="1">I EXPERIMENTS AND METASCIENCE I 29 I</outsider>
          <region class="DoCO:TextChunk" id="169" confidence="possible" page="30" column="1">claimed that trust, particularly trust in authority and dogma, was antithetical to good science.Every authoritative assertion,every dogma, was to be open to question, and the job of science was to do that questioning. That image is partly wrong. Any single scientific study is an exercisein trust (Pinch, 1986; Shapin, 1,994).Studies trust the vast majority of already developed methods, findings, and concepts that they use when they test a new hypothesis. For example, statistical theories and methods are usually taken on faith rather than personally verified, as are measurement instruments. The ratio of trust to skepticism in any given study is more llke 99% trust to 1% skepticism than the opposite. Even in lifelong programs of research, the single scientist trusts much -or. than he or she ever doubts. Indeed, thoroughgoing skepticism is probably impossible for the individual scientist, po iudge from what we know of the psychology of science(Gholson et al., L989; Shadish 6c Fuller, 1'9941. Finall5 skepticism is not even an accuratecharacterrzation of past scientific revolutions; Shapin (1,994) shows that the role of "gentlemanly trust" in L7th-century England was central to the establishment of experimental science.Trust pervades science,de- spite its rhetoric of skepticism.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="170" confidence="possible" page="30" column="1">lmplications for Experiments</h2>
          <region class="unknown" id="171" page="30" column="1">The net result of thesecriticismsis a greaterappreciationfor the equivocalityof all scientificknowledge. The experiment is not a clearwindow that reveals nature directly to us.To the contrary,experiments yield hypotheticaland fallible knowl- edgethat is often dependent on context and imbuedwith many unstatedtheoret- ical assumprions. Consequentlyexperimental results are partly relativeto those assumptions and contextsand might well changewith new assumptions or con- texts.In this sense, all scientists are epistemological constructivists and relativists. The differenceis whether they are strong or weak relativists.Strong relativists share Collins'sposition that only influenceour theories. 'Weak relativistsbelieve that both the ontologicalworld and the worlds of ideol- og5 interests, values, hopes,and wishesplay a role in the constructionof scien- tiiic knowledge.Most practicingscientists, including ourselves, would probably describe themselves ", Lrrtological realistsbut weak epistemological relativists.l2 To the extent that experiments reveal nature to us, it is through a very clouded windowpane (Campbell, 1988). Suchcounterweights to naiveviewsof experiments were badly needed. As re- centlyas 30 yearsago,the centralrole of the experimentin science was probably</region>
          <region class="unknown" id="172" page="30" column="1">1.2. If spacepermitred,we could exrendthis discussion to a host of other philosophicalissues that have beenraised about the experiment, such as its role in discovery versusconfirmation, incorrect assertionsthat the experiment is tied to as logical positivismor pragmatism,and the various mistakesthat are frequentlymadei., suchdiscussions (e.g., Campbell, 1982,1988; Cook, 1991; Cook 6&lt; Campbell, 1985; Shadish, 1.995a\.</region>
          <region class="DoCO:FigureBox" id="Fx173">
            <image class="DoCO:Figure" src="62p6.page_030.image_30.png" thmb="62p6.page_030.image_30-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="174" page="31" column="1">I</outsider>
          <outsider class="DoCO:TextBox" type="sidenote" id="175" page="31" column="1">30</outsider>
          <region class="unknown" id="176" page="31" column="1">| 1. EXPERTMENTS AND GENERALTZED CAUSAL INFERENCE</region>
          <region class="unknown" id="177" page="31" column="1">taken more for granted than is the case today. For example, Campbell and Stanley (1.9631 described themselves as:</region>
          <region class="unknown" id="178" page="31" column="1">committed to the experiment: as the only means for settling disputes regarding educational practice, as the only way of verifying educational improvements, and as the only way of establishing a cumulative tradition in which improvements can be introduced without the danger of a faddish discard of old wisdom in favor of inferior novelties. (p. 2)</region>
          <region class="DoCO:TextChunk" id="179" page="31" column="1">Indeed,Hacking (1983) points out that "'experimental method' usedto be iust another name for scientific method" (p.149); and experimentation was then a more fertile ground for examples illustrating basic philosophical issuesthan it was a source of contention itself. , 'We Not so today. now understand better that the experiment is a profoundly human endeavor,affected by all the same human foibles as any other human en- deavor, though with well-developed procedures for partial control of some of the limitations that have been identified to date. Some of these limitations are common to all science,of course. For example, scientiststend to notice evidencethat confirms their preferred hypothesesand to overlook contradictory evidence.They make routine cognitive errors of judgment and have limited capacity to process large amounts of information. They react to peer pressures to agreewith accepted dogma and to social role pressuresin their relationships to students,participants, and other scientists.They are partly motivated by sociological and economic re- wards for their work (sadl5 sometimesto the point of fraud), and they display all- too-human psychological needs and irrationalities about their work. Other limitations have unique relevance to experimentation. For example, if causal results are ambiguous, as in many weaker quasi-experiments,experimentersmay attribute causation or causal generalization based on study features that have little to do with orthodox logic or method. They may fail to pursue all the alternative causal explanations becauseof a lack of energS a need to achieveclosure, or a bias toward accepting evidence that confirms their preferred hypothesis.Each experiment is also a social situation, full of social roles (e.g., participant, experimenter, assistant) and social expectations (e.g., that people should provide true information) but with a uniqueness (e.g., that the experimenter does not always tell the truth) that can lead to problems when social cues are misread or deliberately thwarted by either party. Fortunately these limits are not insurmountable, as formal training can help overcome some of them (Lehman, Lempert, &amp; Nisbett, 1988). Still, the relationship between scientific results and the world that science studies is neither simple nor fully trustworthy. These social and psychological analyseshave taken some of the luster from the experiment as a centerpieceof science.The experiment may have a life of its own, but it is no longer life on a pedestal. Among scientists,belief in the experiment as the only meansto settle disputes about causation is gone, though it is still the preferred method in many circumstances. Gone, too, is the belief that the power experimental methods often displayed in the laboratory would transfer easily to applications in field settings. As a result of highly publicized science-related</region>
          <region class="DoCO:FigureBox" id="Fx180">
            <image class="DoCO:Figure" src="62p6.page_031.image_31.png" thmb="62p6.page_031.image_31-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="181" page="32" column="1">I A WORLD WITHOUT EXPERIMENTS OR CAUSES? I gT I</outsider>
          <region class="DoCO:TextChunk" id="182" confidence="possible" page="32" column="1">events such asthe tragicresults of the Chernobylnucleardisaster, the disputes over certaintylevelsof DNA testingin the O.J. Simpsontrials, and the failure to find a cure for most cancers after decades of highly publicizedand funded effort, the general public now betterunderstands the limits of science. Yet we should not take these critiques too far. Those who argue against theory-free tests often seem to suggest that everyexperiment will comeout just as the experimenter wishes. This expectation is totally contrary to the experience of researchers, who find instead that experimentation is often frustratingand disap- pointing for the theoriesthey loved so much. Laboratory resultsmay not speak for themselves, but they certainlydo not speakonly for one'shopesand wishes. We find much to valuein the laboratoryscientist's belief in "stubborn facts" with a life spanthat is greaterthan the fluctqatingtheorieswith which one tries to explain them.Thus many basicresultsabout gravityare the same, whetherthey are containedwithin a framework developed by Newton or by Einstein;and no suc- cessor theory to Einstein's would be plausibleunlessit could accountfor most of the stubbornfactlike findingsabout falling bodies.There may not be pure facts, but someobservations are clearlyworth treating as if they were facts. Some theorists of science-Hanson, Polanyi, Kuhn, and Feyerabend included-have so exaggerated the role of theory in science as to make experimental evidence seem almost irrelevant.But exploratory experiments that were unguidedby formal theory and unexpected experimental discoveries tangentialto the initial research motivationshaverepeatedly been the sourceof greatscientific advances. Experiments have providedmany stubborn,dependable, replicablere- sultsthat then become the subject of theory. Experimental physicists feel that their laboratorydata help keeptheir more speculative theoreticalcounterparts honest, giving experiments an indispensable role in science. Of course,thesestubborn facts often involve both commonsense presumptionsand trust in many well- established theoriesthat make up the sharedcore of belief of the science in question. And of course, these stubbornfactssometimes proveto beundependable, are reinterpreted as experimental artifacts,or are so ladenwith a dominantfocal theory that they disappear once that theory is replaced. But this is not the casewith the greatbulk of the factual base, which remainsreasonably dependable over rel- ativelylong periodsof time.</region>
          <region class="unknown" id="183" page="32" column="1">A WORLD WITHOUT EXPERIMENTS OR CAUSES?</region>
          <region class="DoCO:TextChunk" id="184" confidence="possible" page="32" column="1">To borrow a thought experiment from Maclntyre (1981),imaginethat the slates of science and philosophywerewiped cleanand that we had to constructour understanding of the world anew.As part of that reconstruction, would we reinvent the notion of a manipulablecause? \7e think so, largely because of the practical utility that dependable manipulandahave for our ability to surviveand prosper. IUTould we reinvent the experimentas a method for investigatingsuch causes?</region>
          <region class="DoCO:FigureBox" id="Fx185">
            <image class="DoCO:Figure" src="62p6.page_032.image_32.png" thmb="62p6.page_032.image_32-thumb.png"/>
          </region>
          <region class="unknown" id="186" page="33" column="1">I 32 1. EXPERTMENTS AND GENERALTZED CAUSAL TNFERENCE |</region>
          <region class="DoCO:TextChunk" id="187" confidence="possible" page="33" column="1">Again yes,because humanswill always be trying to betterknow how well these manipulablecauses work. Over time, they will refinehow they conductthoseex- perimentsand so will againbe drawn to problemsof counterfactual inference, of cause precedingeffect,of alternative explanations, and of all of the other features of causation that we havediscussed in this chapter.In the end, we would probably end up with the experimentor something very much like it. This book is one more stepin that ongoingprocess of refining experiments. It is about improving the yield from experiments that take placein complexfield settings, both the quality of causalinferences they yield and our ability to generalize these inferences to constructs and over variationsin persons, settings, treatments, and outcomes.</region>
          <region class="DoCO:FigureBox" id="Fx188">
            <image class="DoCO:Figure" src="62p6.page_033.image_33.png" thmb="62p6.page_033.image_33-thumb.png"/>
          </region>
          <region class="unknown" id="189" page="34" column="1">A Critical Assessment of Our Assumptions</region>
          <region class="unknown" id="190" page="34" column="1">As.sump.tion (e-simp'shen): [Middle Englishassumpcion, from Latin as- sumpti, assumptin-adoption, from assumptus,past participle of ass- mere,te adopt; seeassume.] n. 1. The act of taking to or upon oneself: assumption of an obligation. 2.The act of taking overiassumption of command. 3. The act of taking for granted:assumptionof a false theory. 4. Somethingtaken for granted or accepted as true without proof; a supposition: a ualid assumption. 5. Presumption; arrogance. 5. Logic.A minor premise.</region>
          <region class="DoCO:TextChunk" id="191" page="34" column="1">fltHIS BooK covers five central topics across its 13 chapters. The first topic | (Chapter 1) deals with our general understanding of descriptive causation and I experimentation. The second (Chapters 2 and 3) deals with the types of validity and the specific validity threats associatedwith this understanding. The third (Chapters 4 through 7) deals with quasi-experimentsand illustrates how combin- ing design features can facilitate better causal inference. The fourth (Chapters 8 through L0) concerns randomized experiments and stresses the factors that im- pede and promote their implementation. The fifth (Chapters 11 through L3) deals with causal generalization, both theoretically and as concerns the conduct of individual studies and programs of research.The purpose of this last chapter is to critically assess some of the assumptions that have gone into these five topics, especially the assumptions that critics have found obiectionable or that we antici- 'We pate they will find objectionable. organize the discussionaround each of the five topics and then briefly justify why we did not deal more extensivelywith nonexperimental methods for assessing causation. I7e do not delude ourselvesthat we can be the best explicators of our own assumptions. Our critics can do that task better. But we want to be as comprehen- srve siveand an(l as explclt explicit as we can. can. This I nrs ls is rn in part part because because we we are are convrnced convinced ot of the the acl- advantages of falsification as a major component of any epistemology for the social sciences,and forcing out one's assumptions and confronting them is one part of falsification. But it is also becausewe would like to stimulate critical debateabout theseassumptionsso that we can learn from those who would challengeour think-</region>
          <outsider class="DoCO:TextBox" type="footer" id="192" page="34" column="1">456</outsider>
          <region class="DoCO:FigureBox" id="Fx193">
            <image class="DoCO:Figure" src="62p6.page_034.image_34.png" thmb="62p6.page_034.image_34-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="194" page="35" column="1">CAUSATION AND EXPERIMENTATION | rct</outsider>
          <region class="unknown" id="195" page="35" column="1">ing. If be a future book that forward the tradition emanating from Campbelland Stanleyvia Cook and Campbellto this book, then that futuie book *o,rld probably be all the better for building upon all the justifiedcriticisms comingfrom thosewho do not agreewith us, eitheron partic- ,rlu6 o, on the whole havetaken to the analysis of descriptive cau- sationand its generayzition.'We would like this chapternot only to model the at- i.-p, to be cr"iti.alabout the assumprions all scholarsmust inevitablymake but alsoto encourage others to think about theseassumptions and how they might be addressed in fuiure</region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="196" page="35" column="1">CAUSATION AND EXPERIM ENTATION</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="197" confidence="possible" page="35" column="1">Causal Arrows and Pretzels</h2>
          <region class="DoCO:TextChunk" id="198" confidence="possible" page="35" column="1">Experiments test the influence of one or at most a small subset of descriptive causes.If statistical interactions are involved, they tend to be among very few treatments or between a single treatment and a limited set of moderator variables' Many researchers believe that the causal knowledge that results from this typical .*p.ii-..rtal structure fails to map the many causal forces that simultaneously af- fe.t "ny given outcome in compiex and nonlinear ways (e.g., Cronbach et al', 19g0; Magnusson,2000). These critics assertthat experiments prioritize on ar- ,o*, .onrr-.cting A to B when they should instead seekto describe an explanatory pretzel or set of intersectingpretzels,as it were. They also believethat most causal ielationships vary across ,rttitt, settings, and times, and so they doubt whether there ".. "ny constant bivariate causal relationships (e.g., Cronbach 6c Snow, 1977).Those that do appearto be dependablein the data may simply reflect sta- tistically underpow.r.i irr,, of modeiators or mediators that failed to reveal the true underlying complex causal relationships. True-variation in effect sizesmight also be obrc.rr"d b.c"rrs. the relevant substantive theory is underspecified, or the outcome measuresare partially invalid, or the treatment contrast is attenuated, or causally implicated variables afe truncated in how they are sampled (McClelland</region>
          <region class="unknown" id="199" page="35" column="1">6c Judd, 1993). As valid as they do not invalidatethe casefor experi- ments.The purposeof experiments is not to some.phenome- non; it is to ldentify whethera small setof variables makes a and above all the other forces affect- ing that such as the preceding have not stJppedbelievers in more complex iausal acting as though many .r,rol relationships can be usefullycharacterized as dependable main effects or as very simpl. nonlin."rities that are also dependable enoughto be_u_seful. In this connection, from the United States, where</region>
          <region class="DoCO:FigureBox" id="Fx200">
            <image class="DoCO:Figure" src="62p6.page_035.image_35.png" thmb="62p6.page_035.image_35-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="201" page="36" column="1">I</outsider>
          <region class="DoCO:TextChunk" id="202" confidence="possible" page="36" column="1">14.A CRTTTCAL ASSESSMENT OFOUR ASSUMPTTONS</region>
          <region class="unknown" id="203" page="36" column="1">4s8 |</region>
          <region class="DoCO:TextChunk" id="204" page="36" column="1">objections to experimentation are probably the most prevalent and virulent. Few educational researchers seem to object to the following substantiveconclusions of the form that A dependably causesB: small schools are better than large ones; time-on-task raises achievement; summer school raises test scores;school deseg- regation hardly affects achievement but does increaseWhite flight; and assigning and grading homework raises achievement.The critics also do not seemto object to other conclusions involving very simple causal contingencies: reducing class "sizable" size increasesachievement,but only if the amount of change is and to a level under 20; or Catholic schools are superior to public ones, but only in the inner city and not in the suburbs and then most noticeably in graduation rates rather than in achievementtest scores. , The primary iustification for such oversimplifications-and for the use of the experiments that test them-is that some moderators of effects are of minor relevance to policy and theory even if they marginally improve explanation. The most important contingencies are usually those that modify the sign of a causal relationship rather than its magnitude. Sign changesimply that a treatment is benefi- cial in some circumstancesbut might be harmful in others. This is quite different from identifying circumstancesthat influence just how positive an effect might be. Policy-makers are often willing to advocate an overall change,even if they suspect it has different-sizedpositive effects for different groups, as long as the effects are rarely negative. But if some groups will be positively affected and others nega- tively political actors are loath to prescribe different treatments for different groups becauserivalries and jealousies often ensue. Theoreticians also probably pay more attention to causal relationships that differ in causal sign becausethis result implies that one can identify the boundary conditions that impel such a dis- parate data pattern. Of course, we do not advocate ignoring all causal contingencies.For example, physicians routinely prescribe one of severalpossibleinterventions for a given diagnosis.The exact choice may depend on the diagnosis,test results,patient preferences, insurance resources, and the availability of treatments in the patient's area. However, the costs of such a contingent system are high. In part to limit the number of relevant contingencies,physicians specialize,andwithin their own spe- cialty they undergo extensivetraining to enable them to make thesecontingent decisions. Even then, substantial judgment is still required to cover the many situations in which causal contingencies are ambiguous or in dispute. In many other policy domains it would also be costly to implement the financial, management, and cultural changesthat a truly contingent system would require even if the req- uisite knowledge were available. Taking such a contingent approach to its logical extremes would entail in education, for example, that individual tutoring become the order haveto be carefullymatched matched</region>
          <region class="unknown" id="205" page="36" column="1">of the day. dav. Students and instructorswould for overlap in teachingand learning skills and in the curriculum supportsthey would need. some moderators can be studied experimentallSeither by measuringthe moderator so it can be testedduring analysisor by deliberately</region>
          <region class="DoCO:FigureBox" id="Fx206">
            <image class="DoCO:Figure" src="62p6.page_036.image_36.png" thmb="62p6.page_036.image_36-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="207" page="37" column="1">CAU5ATION AND EXPERIMENTATION I Ot'</outsider>
          <region class="DoCO:TextChunk" id="208" confidence="possible" page="37" column="1">varying it in the next study in a program of research' In conductingsuch experi- ments,onemovesaway from thethik-bo" experiments of yesteryear towardtak- ing causalcontingencies more seriouslyand toward routinely study!1gthem by, foi .""-ple, disaggregating the treatmentto examineits causallyeffective com- ponents,iir"ggt.glting the effect,toexamineits causallyimpactedcomponents, .ondrr.ting ,n"ty*r ofi.-ographic and psychological moderatorvariables, and exploringlhe causalpathwa-ysihtooghwhjch (parts.of) the treatment affects lparts of) the outcomJ.To do all of this well in a singleexperiment is not possi- tl.. brrtto do someof it well is possible and desirable.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="209" confidence="possible" page="37" column="1">Epistemological Criticisms of E4periments</h2>
          <region class="DoCO:TextChunk" id="210" confidence="possible" page="37" column="1">In highlightingstatistical conclusionvalidity and in-selecting examples, we have often linked causaldescriptionto quantitativemethodsand hypothesis testing' Many criticswill (wrongly)r.. this asimplying a discredited theory of positivism' As a philosophyof scieniefirst outlined in the early L9th century'positivismre- 1.ct.d' lrro*t.ag. metaphysical *lih descriptions speculations, of e*perienced especiallyabout phenomena- unobservables, A narrower and school equated of logical pisitivism .*.rg.d in the eatly 20th century that also rejectedrealism *til. "lro .-phasizing Ih. ,rr. of data-theory connections in predicate logic form lated ""J " epistemologies fr.f.r.r.. for *.r. p"redicting lonf ago phenomena discredited, over especially explainingthem' as explanations Both thesere- of how science op.r"trr.*so few criticsseriously critici'e experiments on this basis'How- ever,many critics use the term positiuismwith less historical fidelity to attack quantitativesocialscience methodsin genera-l (e'g', Lincoln &amp; Guba, 1985)' liuilding on the rejectionof logicalpositivism,they reiectthe useof quantification and forLal logic in observatiron, measurement, and hypothesistesting.Because theselast features are part of experiments, to reiectthis looseconception of positivism entailsrejecting experiments. However,the errorsin suchcriticismsare nu- merous.For example,to ,eject a specificfeatureof positivism (like the idea that f,r"rrtifi.rtion tiheory;doesnot and nJcessarily p redicatelogic imlly are reiectingall the only permissible relatedand links more between generalproposi- data and tions jsuch asthe notion that somekinds of quantificationand hypothesis testing may be usefulfor knowledge growth).Ife and othershave outlinedmore sucher- rors elsewhere (Phillips, 1990; Shadish, I995al' other epistemological criticisms of experimentation cite the work of historians of science such asKuh"n (1,g62),of sociologists of science such asLatour and'wool- gar ltiZll "rrd of fhiloroph.ir of scienceiuch as Harr6'(1931).Thesecritics tend to focuson threethings.orre.i, the incommensurability of theories, the notion that theoriesare neverper"fectly specified and so can always be reinterpreted. As a result, when disconfirming data seemto imply that a theory should be reiected'its poriolut., can insteadbI reworkedin order to make the theory and observations consistent with eachother.This is usuallydoneby addingnew contingencies to the</region>
          <region class="DoCO:FigureBox" id="Fx211">
            <image class="DoCO:Figure" src="62p6.page_037.image_37.png" thmb="62p6.page_037.image_37-thumb.png"/>
          </region>
          <region class="unknown" id="212" page="38" column="1">460 | 14.A CRIT|CAL ASSESSMENT OF OURASSUMPTTONS I</region>
          <outsider class="DoCO:TextBox" type="sidenote" id="213" page="38" column="1">I</outsider>
          <region class="DoCO:TextChunk" id="214" page="38" column="1">theory that limit the conditions under which it is thought to hold. A second critique 'We is of the assumption that experimental observations can be used as truth tests. would like observations to be objective assessments that can adjudicate between different theoretical explanations of a phenomenon. But in practice, ob- servationsare not theory neutral; they are open to multiple interpretations that include such irrelevanciesas the researcher's hopes, dreams, and predilections. The consequence is that observations rarely result in definitive hypothesistests.The final criticism follows from the many behavioral and cognitive inconsistenciesbe- tween what scientists do in practice and what scientific norms prescribe they should do. Descriptions of scientists' behavior in laboratories reveal them as choosing to do particular experiments becausethey have an intuition about a relationship, or they are simply curious to seewhat happens, or they want to play with a new piece of equipment they happen to find lying around. Their impetus, therefore, is not a hypothesis carefully deduced from a theory that they then test by means of careful observation. Although these critiques have some credibilitg they are overgeneralized.Few experimentersbelievethat their work yields definitive results even after it has been subjected to professional review. Further, though these philosophical, historical, and social critiques complicate what a "fact" means for any scientific method, nonethelessmany relationships have stubbornly recurred despite changesassoci- ated with the substantive theories, methods, and researcherbiasesthat first generated them. Observations may never achieve the status of "facts," but many of them are so stubbornly replicable that they may be consideredas though they were facts. For experimenters, the trick is to make sure that observations are not impregnated with just one theory, and this is done by building multiple theories into observationsand by valuing independent replications, especiallythose of substantive critics-what we have elsewherecalled critical multiplism (Cook, 1985; Shadish,'1.989, 1994). Although causal claims can never be definitively tested and proven, individual experiments still manage to probe such claims. For example, if a study produces negative results, it is often the casethat program developersand other advocates then bring up methodological and substantive contingenciesthat might have changedthe result. For instance, they might contend that a different outcome measure or population would have led to a different conclusion. Subsequent studies then probe these alternatives and, if they again prove negative, lead to yet another round of probes of whatever new explanatory possibilities have emerged. After a time, this process runs out of steam, so particularistic are the contingencies that remain to be examined. It is as though a consensusemerges:"The causal relationship was not obtained under many conditions. The conditions that remain to be examined are so circumscribed that the intervention will not be worth much even if it is effectiveunder these conditions. " 'W'e agreethat this processis as much or more social than logical. But the reality of elastic theory does not mean that decisions about causal hypotheses are only social and devoid of all empirical and logical content.</region>
          <region class="DoCO:FigureBox" id="Fx215">
            <image class="DoCO:Figure" src="62p6.page_038.image_38.png" thmb="62p6.page_038.image_38-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="footer" id="216" page="38" column="1">I t I J</outsider>
          <outsider class="DoCO:TextBox" type="header" id="217" page="39" column="1">CAUSATION AND EXPERIMENTATION | +er</outsider>
          <region class="DoCO:TextChunk" id="218" confidence="possible" page="39" column="1">The criticismsnoted are especially useful in highlightingthe limited value of individual studies relativeto reviewsof research programs.Suchreviewsare better because the greaterdiversityof study features makes it lesslikely that the same theoretical biases that inevitablyimpregnate any one studywill reappear across all the studies under review.Still, a dialecticprocess of point, response, and counter- point is needed even with reviews,againimplying that no singlereview is defini- iirr.. Fo, example, in response to Smith and Glass's (1'977) meta-analytic claim that psychotheiapy *", .ff..tive, Eysen ck (L977)and Presby (1'977) pointedout methojological and substantivecontingencies that challengedthe original re- viewers'reJults. They suggested that a differentanswerwould havebeenachieved if Smith and Glassitrd ""t combinedrandomizedand nonrandomizedexperi- mentsor if they had usednarrower calegories in which to classifytypesof therapy. Subsequent studies probed thesechallenges to Smith and Glassor brought foith nouef or,., 1e.g., \il'eisz et al., 1,992).This processof challengingcausal claims with specificalternatives has now slowedin reviewsof psychotherapy as many major contingencies that might limit effectiveness have beenexplored.The currenrconsensus fiom reviewsof many experiments in many kinds of settings is that psychotherapy is effective; it is not iust the product of a regression process lrporrt"nrors remission) wherebythosewho are temporarily in needseekprofes- ,ii""t help and get better,as they would haveevenwithout the therapy'</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="219" confidence="possible" page="39" column="1">Neglected AncillarY Questions</h2>
          <region class="DoCO:TextChunk" id="220" confidence="possible" page="39" column="1">Our focus on causalquestions within an experimental framework neglects many other questions that arerelevantto causation. These includequestions about how to decideon the importanceor leverage of any singlecausalquestion.This could entail exploringwhethera causalquestionis evenwarranted,as it often is not at the early sa"g.-ofdevelopment of an issue.Or it could entail exploringwhat type of c".rsalquestionis moie important-one that fills an identifiedhole in somelit- erature,o, orr. that setsout to identify specificboundary conditionslimiting a causalconnection, or one that probesthe validity of a centralassumption held by all the theoristsand researchers within a field, or one that reducesuncertainty about an important decision when formerly uncertaintywas high. Our approach alsoneglects the realitythat how oneformulatesa descriptive causal questionusu- aily enLils meetingsomestakeholders' interests in the socialresearch more than those of others.TLus to ask about the effectsof a national program meetsthe needs of Congressional staffs, the media,and policy wonks to learnaboutwhether the program"*orks. But it can fail to meet the needsof local practitionerswho ,rro"lly"*"nt to know about the effectiveness of microelements within the program ,o thut they can usethis knowledgeto improve their daily practice.-In more Ih.or.ti."l work, to ask how some interventionaffectspersonalself-efficacy is likely to promote individuals'autonomyneeds, whereasto ask about the effects of a'persoasive communicationdesigned to changeattitudescould well cater to</region>
          <region class="DoCO:FigureBox" id="Fx221">
            <image class="DoCO:Figure" src="62p6.page_039.image_39.png" thmb="62p6.page_039.image_39-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="222" page="40" column="1">t</outsider>
          <region class="DoCO:TextChunk" id="223" confidence="possible" page="40" column="1">14.A CR|T|CAL ASSESSMENT OFOUR ASSUMPT|ONS</region>
          <region class="unknown" id="224" page="40" column="1">462 |</region>
          <region class="DoCO:TextChunk" id="225" page="40" column="1">the needs of those who would limit or manipulate such autonomy. Our narrow technical approach to causation also neglectedissuesrelated to how such causal knowledge might be used and misused. It gave short shrift to a systematic analysis of the kinds of causal questions that can and cannot be answered through experiments. \7hat about the effects of abortion, divorce, stable cohabitation, birth out of wedlock, and other possibly harmful events that we cannot ethically manipulate? What about the effects of class,race, and gender that are not amenable 'What to experimentation? about the effects of historical occurrencesthat can be studied only by using time-seriesmethods on whatever variables might or might not be in the archives?Of what use, one might ask, is a method that cannot get at some of the most important phenomena that shape our social world, often over generations, as in the caseof race, class,and gender? Many statisticians now consider questions about things that cannot be manipulated as being beyond causal analysis,so closely do they link manipulation to causation. To them, the cause must be at least potentially manipulable, even if it is not actually manipulated in a given observational study. Thus they would not consider race ^ cause, though they would speak of the causal analysis of race in studies in which Black and White couples are, say, randomly assignedto visiting rental units in order to seeif the refusal rates vary, or that entail chemically changing skin color to seehow individuals are responded to differently as a function of pigmentation, or that systematicallyvaried the racial mix of studentsin schools or classrooms in order to study teacher responsesand student performance. Many critics do not like so tight a coupling of manipulation and causation. For example, those who do status attainment researchconsider it obvious that race causally influences how teachers treat individual minority students and thus affects how well these children do in school and therefore what jobs they get and what prospects their own children will subsequentlyhave. So this coupling of causeto manipulation is a real limit of an experimental approach to causation. Although we like the coupling of causation and manipulation for purposes of defining experiments, we do not seeit as necessaryto all useful forms of cause.</region>
          <region class="unknown" id="226" page="40" column="1">VALIDITY</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="227" confidence="possible" page="40" column="1">Objections to InternalValidity</h2>
          <region class="unknown" id="228" page="40" column="1">There are several criticismsof Campbell's(1957) validity typology and its exten- sions(Gadenne, 1976;Kruglanski &amp; Kroy, 1.976;Hultsch 6c Hickey,1978;Cron- 'We bach, 1982; Cronbachet al., 1980). start first with two criticismsof internal validity raisedby to a lbsser extent by Kruglanskiand Kroy (1'976):(1) an atheoretically definedinternal validity (A causes B) is trivial without reference to constructs;and (2) causationin singleinstances is impossible, in- cludingin singleexperiments.</region>
          <region class="DoCO:FigureBox" id="Fx229">
            <image class="DoCO:Figure" src="62p6.page_040.image_40.png" thmb="62p6.page_040.image_40-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="230" page="41" column="1">vALtDtrY nol I</outsider>
          <region class="DoCO:TextChunk" id="231" confidence="possible" page="41" column="1">lnternal Validity ls Trivial Cronbach (L982) writes:</region>
          <region class="unknown" id="232" page="41" column="1">I consider it pointless to speak of causeswhen all that can be validly meant by refer- enceto a causein a particular instanceis that, on one trial of a partially specified manipulation under.orrditior6 A, B, and c, along with other conditions not named, phenomenon p was introduce the word cause seemspointless. Campbell's writings make internal validity a property of trivial, past-tense'and local statements'</region>
          <region class="DoCO:TextChunk" id="233" confidence="possible" page="41" column="1">(p .t3 7 ) Hence,.,causal language is superfluous" (p. 140).Cronbach does not retaina specific role fo, .",rr"Iinferenceln his validity typology at all. Kruglanski and Kroy (1976)criticizeinternalvalidity similanlSsaying:</region>
          <region class="unknown" id="234" page="41" column="1">The concrete events which constitute the treatment within a specific research are meaningful only as members of a general conceptual category' ' ' ' Thus, it is simply impossibleto draw strictly an experiment: our concepts are g.rr.r"l and each pr.r,rppor"s an implicit general theory about resemblanceberween different 1'57)</region>
          <region class="DoCO:TextChunk" id="235" confidence="possible" page="41" column="1">All theseauthors suggest collapsinginternal with constructvalidity in different ways. Of course, we agree that researchers conceptualize and discuss treatments and outcomes in concepfual terms. As we saidin Chapter3, constructs are so basicto l"rrgo"g. and thought that it is impossible to conceptualize scientificwork with- out"thJm. Indeed,ir, *"ny important respects, the constructswe use constrain what we experience, a point agreedto by theoristsranging from Quine (L951' L96g)to th; postmodernists (Conner,1989;Testeq1993). So when we say that internalvalidity concerns an atheoretical local molar causalinference, we do not mean that the researcher should conceptualize experiments or report a causal claim as "somethingmadea differencer" to useCronbach's (1982, p' 130) exag- geratedcharacterization. Still, it is both sensible and usefulto differentiateinternal from constructvalidity. The task of sortingout constructs is demanding enoughto warrant separate attention from the task of sorting out causes. After all, operationsare concept laden,and it is very rare for researchers to know fully what thoseconcepts are. In fu.t, th, ,erearchrialmostcertainlycannotknow them fully because paradigmatic .orr..p,, areso implicitly and universally imbuedthat thoseconcepts and their assumptions "r. ,oi,'.,imes entirely unrecognized_ by researchcommunitiesfor y."ri. Indeed,the history of science is repletewith examplesof famousseries of ."p.rim.nts in which a causalrelationshipwas demonstrated earlS but it took y."r, for the cause(or effect)to be consensually and stablynamed.For instance, in psychology and linguistics many causalrelationships originally emanated from a behavioriit paradigl but were later relabeledin cognitive terms; in the early Hawthorne st;dy, illumination effectswere later relabeled as effectsof obtrusive observers;and some cognitive dissonanceeffects have been reinterpretedas</region>
          <region class="DoCO:FigureBox" id="Fx236">
            <image class="DoCO:Figure" src="62p6.page_041.image_41.png" thmb="62p6.page_041.image_41-thumb.png"/>
          </region>
          <region class="unknown" id="237" page="42" column="1">464 I 14.A CRITICAL ASSESSMENT OF OURASSUMPTIONS</region>
          <region class="unknown" id="238" page="42" column="1">attribution effects.In the history of a discipline,relationships that are correctly identified as causalcan be important evenwhen the causeand effectconstructs are incorrectlylabeled.Suchexamples exist because the reasoning used to draw causalinferences (e.g., requiring evidence that treatmentpreceded outcome)dif- fers from the reasoning used to generalize (e.g., matchingoperations to prototypical characteristics of constructs). \Tithout understanding what is meant by descriptive causation, we have no means of telling whether a claim to have established such causationis justified. Cronbach's(1982) prosemakesclear that he understands the importanceof causallogic; but in the end, his sporadically expressed craft knowledgedoesnot add up to a coherent theory of validity of descriptive causal inferences. His equation of internal validity as part of reproducibility (under replication) misses the point that one can replicateincorrectcausalconclusions. His solution to suchquestions is simplythat "the forceof eachquestion can bereduced by suit- able controls" (1982,p. 233).This is inadequate, for a complete analysis of the problem of descriptive causal inference requiresconcepts we can useto recognize suitablecontrols.If a suitablecontrol is one that reduces the plausibilityof, say historyor maturation, as Cronbach (1982,p.233)suggests, this is little morethan internalvalidity aswe haveformulatedit. If one needs the concepts enoughto use them, then they should be part of a validity typology for cause-probing methods. For completeness, we might add that a similar boundaryquestionarisesbe- tween constructvalidity and externalvalidity and between constructvalidity and statisticalconclusionvalidity. In the former case,no scientistever framesan external validity questionwithout couchingthe questionin the languageof con- structs.In the latter case,researchers never conceptualize or discuss their results solelyin terms of statistics. Constructsare ubiquitousin the process of doing re- searchbecause they are essential for conceptualizing and reporting operations. But again,the answerto this objectionis the same.The strategies for making inferences about a constructare not the sameas strategies for making inferences about whether a causal relationship holds over variation in persons,settings, treatments, and outcomes in externalvalidity or for drawing valid statistical con- clusionsin the caseof statisticalconclusionvalidity.Constructvalidity requiresa theoreticalargumentand an assessment of the correspondence betweensamples and constructs. Externalvalidity requiresanalyzing whethercausalrelationships hold over variations in persons,settings,treatments,and outcomes.Statistical conclusion validity requires close examinationof the statistical procedures and as- sumptionsused.And again,one can be wrong about constructlabelswhile being right about externalor statisticalconclusionvalidity.</region>
          <region class="unknown" id="239" page="42" column="1">Objections to Causation in SingleExperiments</region>
          <region class="DoCO:TextChunk" id="240" confidence="possible" page="42" column="1">A second criticism of internal validity deniesthe possibility of inferring causation in a single experiment. Cronbach (1982) says that the important feature of causation is the "progressivelocalizationof a cause" (Mackie, 1974, p.73) over mul-</region>
          <region class="DoCO:FigureBox" id="Fx241">
            <image class="DoCO:Figure" src="62p6.page_042.image_42.png" thmb="62p6.page_042.image_42-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="footer" id="242" page="42" column="1">J</outsider>
          <outsider class="DoCO:TextBox" type="header" id="243" page="43" column="1">vALrDrry otu |</outsider>
          <region class="DoCO:TextChunk" id="244" page="43" column="1">tiple experimentsin a program of researchin which the uncertainties about the essential i."t.rr.r of the cause are reduced to the point at which one can character- ize exacflywhat the causeis and is not. Indeed, much philosophy of causation as- serts that we only recognize causes through observing multiple instances of a putative causal relationship, although philosophers differ as to whether the mechanism for recognition involves logical laws or empirical regularities (Beauchamp, 1974;P. White, 1990). However, some philosophers do defend the position that causescan be in- ferred in singleinstances(e.g.,Davidson, 1,967;Ducasse'1,95L1' Madden &amp; Hum- ber, L97'1,). A good example is causation in the law (e.g.,Hart &amp; Honore, 1985)' by which we judge whether or not one person, say, caused the death of another despitethe fact that the defendant may 4ever before have been on trial for a crime. The verdict requires a plausible casethat (among other things) the defendantb actions precededlhe death of the victim, that those actions were related to the death, that other potential causesof the death are implausible, and that the death would not have occurred had the defendant not taken those actions-the very logic of causal relationships and counterfactualsthat we outlined in Chapter 1. In fact, the defendant'scriminal history will often be specifically excluded from consideration in iudging guilt during the trial. The lessonis clear. Although we may learn more "bo,rt ."nsation from multiple than from single experiments, we can rnf.ercause in single experiments.Indeed, experimenterswill do so whether we tell them to or not. Providing them with conceptual help in doing so is a virtue, not a vice; failing to do so is a major flaw in a theory of cause-probing methods. Of course, individual experiments virtually always use prior concepts from other experiments.However, such prior conceptualizations are entirely consistent with the claim that internal validity is about causal claims in single experiments. If it were not (at least partly) about single experiments, there would be no point to doing the experiment, for the prior conceptualization would successfullypre- dict what will be observed.The possibility that the data will not support the prior conceptualization makes internal validity essential.Further, prior conceptualizations are not logically necessary;we can experiment to discover effects that we have no prior conceptual structure to expect: "The physicist George Darwin used to say tliat once in a while one should do a completely crazy experiment, like blowing the trumper to the tulips every morning for a month. Probably nothing wiil hafpen, but if something did happen, that would be a stupendousdiscovery" (Hacking, L983, p. 15a). But we would still need internal validity to guide us in judging if the trumpets had an effect.</region>
          <region class="unknown" id="245" page="43" column="1">Objections to Descriptive Causation A few authorsobjectto the very notion of descriptive causation. Typicall5 how- ever,suchobjections are madeabout a caricature of descriptive causation that has not teen usedin philosophyor in science for many years-for example,a billiard ball modelthat requiresa commitmentto deterministic causation or that excludes</region>
          <region class="DoCO:FigureBox" id="Fx246">
            <image class="DoCO:Figure" src="62p6.page_043.image_43.png" thmb="62p6.page_043.image_43-thumb.png"/>
          </region>
          <region class="unknown" id="247" page="44" column="1">466 ra.n cRrrcAL AssEssMENT oFouR AssuMproNs |</region>
          <region class="unknown" id="248" page="44" column="1">reciprocalcausation. In contrast,mostwho write aboutexperimentation today es- pousetheoriesof probabilisticcausation in which the many difficultiesassociated with identifyingdependable causal relationships are humbly acknowledged. Even more important, thesecriticsinevitablyusecausal-sounding language themselves, for example,replacing "cause" with "mutual simultaneous shaping" (Lincoln 6c Guba, 1985, p. 151).Thesereplacements seem to us to avoidthe word but keep the concept,and for good reason.As we saidat the end of ChapterL, if we wiped the slatecleanand constructed our knowledgeof the world aneq we believe we would end up reinventingthe notion of descriptive causationall over again, so greatlydoesknowledgeof causes help us to survivein the world.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="249" confidence="possible" page="44" column="1">Objections Concerning the Discrimination Between Construct Validityand ExternalValidity</h2>
          <region class="DoCO:TextChunk" id="254" page="44" column="1">Although we traced the history of the present validity system briefly in Chapter 2, readers may want additional historical perspectiveon why we made the changes we made in the present book regarding construct and external validity. Both Campbell (1957) and Campbell and Stanley(1963) only usedthe phraseexternal validitS which they defined as inferring to what populations, settings,treatment variables, and measurement variables an effect can be generalized.They did not rcfer at all to construct validity. However, from his subsequentwritings (Campbell, 1986), it is clear Campbell thought of construct validity as being part of external validity. In Campbell and Stanley therefore, external validity subsumed generalizing from researchoperations about persons, settings,causes,and effects for the purposes of labeling theseparticulars in more abstract terms, and also generalizing by identifying sourcesof variation in causal relationships that are attrib- utable to person, setting, cause, and effect factors. All subsequentconceptualiza- tions also share the same generic strategy based on sampling instancesof persons, settings, causes,and effects and then evaluating them for their presumed corre- spondenceto targets of inference. In Campbell and Stanley'sformulation, person, setting, cause,and effect categories share two basic similarities despite their surface differences-to wit, all of them have both ostensive qualities and construct representations.Populations of persons or settings are composed of units that are obviously individually ostensive. This capacity to point to individual persons and settings, especially when they are known to belong in a referent category permits them to be readily enu- merated and selectedfor study in the formal ways that sampling statisticiansprefer. By contrast, although individual measures (e.g., the Beck Depression Inven- tory) and treatments (e.g., a syringe full of a vaccine) are also ostensive,efforts to enumerate all existing ways of measuring or manipulating such measuresand treatments are much more rare (e.g.,Bloom, L956; Ciarlo et al., 1986; Steiner&amp; Gingrich, 2000). The reason is that researchers prefer to use substantivetheory to determine which attributes a treatment or outcome measureshould contain in any <marker type="page" number="45"/><marker type="block"/> given studS recognizing that scholars often disagreeabout the relevant attributes of th. higher order entity and of the supposed best operations to representthem. None of ihis negatesthe reality that populations of persons or settingsare also defined in part by the theoretical constructs used to refer to them, just like treatments and outiomes; they also have multiple attributes that can be legitimately con- tested. '!(hat, for instance, is the American population? \7hile a legal definition surely exists,it is not inviolate. The German conception of nationality allows that the gieat grandchildren of a German are Germans even if their parents and grand- p"r*t, have not claimed German nationality. This is not possible for Americans. And why privilege alegaldefinition? A cultural conception might admit as American all thor. illegal immigrants who have been in the United Statesfor decades and it might e*cl.rde those American adults with passports who have never lived in the United States. Given that person's,settings, treatments, and outcomes all have both construct and ostensive qualities, it is no surprise that Campbell and Stanley did not distinguish between construct and external validity. Cook and Camptell, however, did distinguish between the two. Their un- stated rationale for the distinction was mostly pragmatic-to facilitate memory for the very long list of threats that, with the additions they made' would have had to fit under bampbell and Stanley's umbrella conception of external validity. In their theoretical diicussion, Cook and Campbell associated construct validity with generalizingto causesand effects, and external validity with generalizing to and across persons, settings, and times. Their choice of terms explicitly refer- encedCronbach and Meehl (1955) who used construct and construct validity in measurementtheory to justify inferences "about higher-order constructs from research operations'; lcook &amp; Campbel| 1,979, p. 3S). Likewise, Cook and Campbeli associatedthe terms population and external ualidity with sampling theory and the formal and purposive ways in which researchersselect instances of persons and settings. But to complicate matters, Cook and Campbell also brlefly acknowledged that "all aspectsof the researchrequire naming samples in gener-alizable termi, including samplesof peoples and settings as well as samples of -r"r,rres or manipulations" (p. 59). And in listing their external validity threats as statistical inieractions between a treatment and population, they linked external validity more to generalizing across populations than to generalizing to them. Also, their construct validity threats were listed in ways that emphasized generalizing to cause and effect constructs. Generalizing across different causes ind effect, *", listed as external validity becausethis task does not involve at- tributing meaning to a particular measure or manipulation. To read the threats in Cook and Campbell, external validity is about generalizing acrosspopulations of persons and settings and across different cause and effect constructs, while construct validity is about generalizing to causesand effects.Where, then, is gen- era\zing from samples of persons or settings to their referent populations? The text disiussesthis as a matter of external validitg but this classification is not apparent in the list of validity threats. A system is neededthat can improve on Cook and Campbell's partial confounding between objects of generalization (causes</region>
          <outsider class="DoCO:TextBox" type="footer" id="251" page="44" column="1">.J</outsider>
          <region class="DoCO:FigureBox" id="Fx252">
            <image class="DoCO:Figure" src="62p6.page_044.image_44.png" thmb="62p6.page_044.image_44-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="253" page="45" column="1">vALrDtrY I oe,</outsider>
          <region class="DoCO:FigureBox" id="Fx255">
            <image class="DoCO:Figure" src="62p6.page_045.image_45.png" thmb="62p6.page_045.image_45-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="256" page="46" column="1">468 14.A CRITICAL ASSESSMENT OF OURASSUMPTIONS</outsider>
          <region class="DoCO:TextChunk" id="257" confidence="possible" page="46" column="1">and effects versus persons and settings) and functions of generalization (generalizing to higher-order constructs from researchoperations versus inferring the de-</region>
          <outsider class="DoCO:TextBox" type="sidenote" id="258" page="46" column="1">,i-{11 f i..</outsider>
          <region class="unknown" id="259" page="46" column="1">greeof replicationacrossdifferent constructsand populations). This book usessucha functional approachto differentiate constructvalidity from externalvalidity. It equates constructvalidity with labelingresearch operations, and externalvalidity with sources of variation in causalrelationships. This new formulation subsumes all of the old. Thus, Cook and Campbellt understandingof constructvalidity asgeneralizing from manipulations and measures to causeand effectconstructsis retained.So is externalvalidity understood as gen- eralizingacrosssamples of persons, settings, and times.And generalizing across different causeor effectconstructsis now,evenmore clearlyclassified as part of exrernalvalidity.Also highlightedis the needto label samples of persons and settings in abstractterms, iust as measures and manipulationsneedto be labeled. Suchlabelingwould seem to be a matterof constructvalidity giventhat construct validity is functionallydefinedin termsof labeling.However,labelinghumansam- ples might have been read as being a matter of external validity in Cook and Campbell,given that their referents were human populationsand their validity typeswereorganized more around referents than functions.So,althoughthe new formulation in this book is definitelymore systematic than its predecessors, we are unsurewhetherthat systematizationwillultimately result in greaterterminologi- cal clarity or confusion.To keepthe latter to a minimum, the following discussion reflectsissues pertinentto the demarcation of constructand externalvalidity that have emerged either in deliberations betweenthe first two authorsor in classes that we havetaughtusingpre-publication versions of this book.</region>
          <region class="unknown" id="260" page="46" column="1">Is Construct Vatidity a Prerequisite for External Vatidity? In this book, we equateexternalvalidity with variation in causal relationships and constructvalidity with labeling.research operations.Somereaders might seethis assuggesting that successful generalization of a causal relationship requires the accurate labelingof eachpopulation of personsand eachtype of settingto which generalization is sought,eventhough we can neverbe certainthat anythingis la- beledwith perfectaccuracy. The relevanttask is to achieve the most accurate assessment availableunder the circumstances. Technically, we can.test genenlization across entitiesthat are akeadyknown to be confounded and thus not labeled well-e.g., when causaldata arebrokenout by genderbut the females in the sample are, on average, more intelligentthan the malesand thereforescorehigheron everythingelsecorrelatedwith intelligence. This exampleillustrates how danger- ous it is to rely on measured surfacesimilarity alone (i.e.,genderdifferences) for determining how a sampleshouldbe labeledin populationterms.\7e might more accuratelylabel genderdifferences if we had a random sampleof each gender taken from the same population.But this is not often found in experimental work, and eventhis is not perfectbecause gender is known to be confounded with other attributes (e.g.,income,work status)evenin the population,and thoseother at-</region>
          <outsider class="DoCO:TextBox" type="footer" id="261" page="46" column="1">t .,.J</outsider>
          <region class="DoCO:FigureBox" id="Fx262">
            <image class="DoCO:Figure" src="62p6.page_046.image_46.png" thmb="62p6.page_046.image_46-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="263" page="47" column="1">vALrDrrY I oo,</outsider>
          <region class="DoCO:TextChunk" id="264" page="47" column="1">tributes may be pertinent labels for some of the inferencesbeing made. Hence, we usually have to rely on the assumption that, becausegender samplescome from the same physical setting, they are comparable on all background characteristics that might be correlated with the outcome. Becausethis assumption cannot be fully testedand is ^nyw^y often false-as in the hypothetical example above-this means rhat we could and should measure all the potential confounds within the limits of our theoretical knowledge to suggestthem, and that we should also use these measuresin the analysis to reduce confounding. Even with acknowledged confounding, sample-specific differences in effect sizesmay still allow us to conclude that a causal relationship varies by something associatedwith gender.This is a useful conclusion for preventing premature over- generalization.Iilith more breakdownq, confounded or not, one can even get a senseof the percentage of contrastsacrosswhich a causal relationship does and does not hold. But without further work, the populations across which the relationship varies are incompletely identified. The value of identifying them better is particularly salient when some effect sizescannot be distinguished from zero. Although this clearly identifies a nonuniversal causal relationship, it does not advance theory or practice by specifying the labeled boundary conditions over which a causal relationship fails to hold. Knowledge gains are also modest from generalization strategiesthat do not explicitly contrast effect sizes.Thus, when different populations are lumped together in a single hypothesis test, researcherscan learn how large a causal relationship is despite the many unexamined sources of variation built into the analysis. But they cannot accurately identify which constructs do and do not co-determine the relationship's size. Construct validity adds useful specificity to external validity concerns, but it is not a necessarycondition for external validity.'We can generalize across entities known to be confounded' albeit lessusefully than acrossaccurately labeled entities. This last point is similar to the one raised earlier to counter the assertion of Gadenne (L9761and Kruglanski and Kroy (1976) that internal validity requires the high consrruct validity of both causeand effect. They assertthat all scienceis about constructs, and so it has no value to conclude that "something causedsomething sfss"-1hs result that would follow if we did a technically exemplary randomized experiment with correspondingly high internal validity but the causeand effect were not labeled. Nonetheless, a causal relationship is demonstrably en- tailed, and the finding that "something reliably causedsomething else" might lead to further researchto refine whatever clues are available about the cause and effect constructs. A similar argument holds for the relationship of construct to external validity. Labels with high construct validity are not necessaryfor internal or for external validity, but they are useful for both. Researchers necessarilyuse the language of constructs (including human and setting population ones) to frame their research questions and selecttheir repre- sentationsof constructsin the samplesand measureschosen.If they have designed their work well and have had some luck, the constructs they begin and end with will be the same,though critics can challengeany claims they make. However, the</region>
          <region class="DoCO:FigureBox" id="Fx265">
            <image class="DoCO:Figure" src="62p6.page_047.image_47.png" thmb="62p6.page_047.image_47-thumb.png"/>
          </region>
          <region class="unknown" id="266" page="48" column="1">470 14.A CRITICAL ASSESSMENT OF OURASSUMPTIONS</region>
          <region class="DoCO:TextChunk" id="267" confidence="possible" page="48" column="1">samplesand constructs might not match we[], and then the task is to examine the samples and ascertain what they might alternatively stand for. As critics like</region>
          <region class="unknown" id="268" page="48" column="1">Gadenne, Kruglanski,and Kroy havepointedout, suchreliance on the operational levelseems to legitimize operations as havinga life independent of constructs. This is not the case, though,for operations are intimatelydependent on interpretations</region>
          <region class="DoCO:TextChunk" id="269" confidence="possible" page="48" column="1">at all stages of research. Still, every operation fits some interpretations, however tentative that referent may be due to poor researchplanning or to nature turning out to be more complex than the researcher'sinitial theory.</region>
          <region class="unknown" id="270" page="48" column="1">How Does Variation AcrossDifferent Operational Representations of the SameIntendedCause or EffectRelate to Construct and ExternalValidity?</region>
          <region class="DoCO:TextChunk" id="271" confidence="possible" page="48" column="1">In Chapter 3 we emphasizedhow the valid labeling of a cause or effect benefits from multiple operational instances,and also that thesevarious instancescan be fruitfully analyzedto examine how a causal relationship varies with the definition used. If each operational instance is indeed of the sameunderlying construct, then</region>
          <region class="unknown" id="272" page="48" column="1">the samecausalrelationshipshouldresult regardless of how the cause or effectis</region>
          <region class="DoCO:TextChunk" id="273" confidence="possible" page="48" column="1">operationally defined. Yet data analysis sometimes revealsthat a causal relationship varies by operational instance.This means that the operations are not in fact</region>
          <region class="unknown" id="274" page="48" column="1">equivalent, so that theypresumably tap both into differentconstructs and into different causalrelationships. Either the differentlyrelated to what now must be seenas two distinct outcomes, or the sameeffectconstruct</region>
          <region class="DoCO:TextChunk" id="275" page="48" column="1">is differently related to two or more unique causal agents.So the intention to promote the construct validity of causesand effects by using multiple operations has now facilitated conclusions about the external validiry of causesor effects;that is, when the external validity of the causeand effect are in play, the data analysishas revealed that more than one causal relationship needsto be invoked. FortunatelS when we find that a causal relationship varies over different causes or different effects, the research and its context often provide clues as to how the causalelementsin eachrelationshipmight be (re)labeled. For example,the researcher will generally examine closely how the operations differ in their particulars, and will also study which unique meaningshave been attached to variants like thesein the ex-</region>
          <region class="unknown" id="276" page="48" column="1">isting literature.While the meaningsthat are achieved might be lesssuccessful be-</region>
          <region class="DoCO:TextChunk" id="277" confidence="possible" page="48" column="1">cause they have been devised post hoc to fit novel findings, they may in some cr- cumstances still attain an acceptable level of accuracy and will certainly prompt continued discussion to account for the findings. Thus, we come full circle. I7e began with multiple operational representations of the same causeor effect when testing a single causal relationship; then the data forced us to invoke more than one relationship; and finally the pattern of the outcomes and their relationship to the existing literature can help improve the labeling of the new relationships achieved.A construct validity exercise begets an externat validity conclusion that prompts the need for relabeling constructs. Demonstrating effect size variation acrossoperations presumed to represent the same cause or effect can enhance external validity by</region>
          <region class="DoCO:FigureBox" id="Fx278">
            <image class="DoCO:Figure" src="62p6.page_048.image_48.png" thmb="62p6.page_048.image_48-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="279" page="49" column="1">vALlDlrY I ort</outsider>
          <region class="DoCO:TextChunk" id="280" confidence="possible" page="49" column="1">showingthat nally ventingany ures elements smoothlybetween.onr,r.r.i and envisaged; in by each..f"io"ritp mislabeling providffilues more and constructs in that of the case, from and shouldbe cause and it externalvalidity detailsof causalrelationships can or effectinherent eventually labeled.'We the causalrelationshipsabout increase concerns'involving see in are here the constructvalidity involved original analytictasksthat than choiceof each' was how by meas- origi- flow pre- the</region>
          <region class="DoCO:TextChunk" id="281" confidence="possible" page="49" column="1">should Generalizingfrom a single sample of Personsor settings Be Classifiedas External or Construct Validity? If population.How a study hasa singlesampleof ,"nlrrr-pre should pers.ons be labeled or settings, is an this issue'Given samplemust that represent construct a validity ter immediately be matter come usage eralizations lidity, lidity. treatsgeneralizing validity with involved.So all, this operations, in evenwhen Second, externalvalidity of is matter, the about and constructvalidity from;;;i; generalsocialscience obvious*n", only trrir-J".r if thoughtheir rJ.iirrg, ;rh.;;;", g.".t"iit-g i*o rr.t" mentionsinteracti,ons probl.-, hardly irrdiuidrr"lsamples not i, comparisonof of Itbeling list analogousto fit'with ,"y peopleto fio* seems of "r-ir.. ih", community'someparts .*t.*"1 a the relevantsincewith sample the labefing Firstl its lample betweenthe discussion variation pofulation generalizing validity of this of personsand peopleis an personsor highlightsa issueof threats in in are causal treatmentand Cook from a a a matter constructvalidity?Af- doesnot of singlesampleit settings settings matter relationships potential which treatment and of of Campbellthat is as explicitly constructva- saythat attributesof externalva- treatedas an conflict and external would is gen- out- deal not in a the ulation. pling settingand The for considerwhy issue represe";i"; is Person. most acutewhen " samplingstatisticians *.il-dJrignated the sample universe. was are so randomly Suchsamplingensures keen to promoterandom selected from. the that sam- pop- the variables sample tion appliesto boundedpop.rl"tiJ., label(whether and within the populatiJndistributions ,";;[. the more limits from K.y or of which tg less"ccorit.;, samplingerror. tle to or.i sample, are rl*r, identical which a-requirement of Notice random randomsampling on all that measured samplingis this in includes samplingtheory and guarantees having unmeasured the popula- a well also and something are bel alsowell can equallyvalidly often tabeied, obviousin r""a.- be applied,o practice.Given sampling itt. then saripl.. that guarantees many For instance'the well that boundedpopulations a valid populationla- population of telephone labeled. Chicagopr.fi*., ownersin boundedpopulationand bel, which Hence,i prefixesor.d Detroii is why "nJ *""fa samplingstatisticians o, itt." orty i' be random tlie mislabelthe in difficuli. city the Edgewater saripling, of Chicagolsknown resulting ,rrJt"ndom believe the sJction-of that samplelabel sampleas no digit methodis and Chicago- dialing representing is is obviouslycorrectly the frol superiorto Given populationla- that telephone a clearly list ran- of dom selectio'f- iun.ii"g"tumples when the populationlabelis known'</region>
          <region class="DoCO:FigureBox" id="Fx282">
            <image class="DoCO:Figure" src="62p6.page_049.image_49.png" thmb="62p6.page_049.image_49-thumb.png"/>
          </region>
          <region class="unknown" id="283" page="50" column="1">472 I T+.N CRITICAL ASSESSMENT OF OURASSUMPTIONS</region>
          <region class="DoCO:TextChunk" id="284" confidence="possible" page="50" column="1">With purposive sample selection,this elegant rationale cannot be used, whetheror not the population label is known. Thus, if respondents were selected haphazardly from shoppingmalls all over Chicago,many of the peoplestudied would belongin the likely populationof interest-residents of Chicago.But many would not because some Chicagoresidents do not go to malls at the hours interviewing takes place, and becausemany personsin these malls are not from Chicago.Lacking random sampling,we could not evenconfidentlycall this sample "peoplewalking in Chicagomalls," for other constructs such as volunteering to be interviewed may be systematicallyconfounded with sample membership. So, meremembership in the sampleis not sufficientfor accurately representing a population, and by the rationalein the previousparagraph, it is alsonot sufficientfor accurately labelingthe sample.All this leadsto two conclusions worth elaborat- ing: (1) that random sampling can sometimes promote constructvalidity, and (2) thatexternalvalidity is in play when inferring that a singlecausalrelationship from a sample would hold in a population,whetherfrom a randomsample or not. On the first point, the conditions under which random samplingcan some- timespromote the constructvalidity of singlesamples are straightforward. Given a well boundeduniverse, samplingstatisticians have justifiedrandom samplingas away of clearlyrepresenting in the sampleall populationattributes. This must in- cludethe populationlabel,and so random samplingresultsin labelingthe sample in the sameterms that apply to the population. Random samplingdoesnot, of course,tell us whetherthe population label is itself reasonably accurate; random samplingwill also replicatein the sampleany mistakes that are madein labeling the population. However,given that many populationsare alreadyreasonably well-labeled based on past research and theory and that suchsituationsare often intuitively obviousfor researchers experienced in an area,random samplingcan, underthese circumstances, be countedon to promoteconstructvalidity.However, when random selection has not occurredor when the populationlabel is itself in doubt, this book hasexplicatedother principlesand methodsthat can be usedfor labelingstudy operations,including labelingthe samples of personsand settings in a study. On the second point, when the questionconcerns the validity of generalizing from a causalrelationship in a singlesample to its population,the readermay also wonder how externalvalidity can be in play at all. After all, we haveframedex- ternal validity as beingabout whetherthe causalrelationship holds overuariation in persons, settings, treatment variables, and measurement variables. If thereis only one random samplefrom a population,where is the variation over which to ex- aminethat causal relationship? The answeris simple:the variationis between sampled and unsampled personsin that population.As we saidin Chapter2 (and as was true in our predecessor books), external validity questionscan be about whether a causalrelationshipholds (a) over variationsin persons, settings, treat- ments,and outcomes that were in the experiment,and (b) for persons, settings, treatments, and outcomes that werenot in the experiment. Thosepersons in a pop-</region>
          <region class="DoCO:FigureBox" id="Fx285">
            <image class="DoCO:Figure" src="62p6.page_050.image_50.png" thmb="62p6.page_050.image_50-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="286" page="51" column="1">vALlDlw | 473</outsider>
          <region class="DoCO:TextChunk" id="287" confidence="possible" page="51" column="1">ulation who were not randomly sampledfall into the latter category. Nothing about externalvalidity,eitherin the presentbook or in its predecessors, requires that all possible uariuiion, of externalvalidity interestactuallybe observed in the study-indeed, it would beimpossible to do so,and we providedseveral arguments in Cirapter2 aboutwhy it would not be wise to limit external validity questions only to variationsactuallyobserved in a study.Of course,in most cases external ualidiry generalizations to things that were not studied are difficult, having to rely on the .L.r..pt, and methodswe outlined in our grounded theory of generalized causalinference in Chapters11 through 13. But it is the great beautyof random samplingthat it guaran;es that this generalization will hold over both sampledand ,rnr"-pl".d p.rr6nr. So it is indeedan externalvalidity questionwhah-e1acausal relationship that hasbeenobserved in a singlerandomsample would hold for those units that were in the populationbut not'in the random sample. Inthe end,this book treatsthe labelingof a singlesample of persons or settings asa matterof constructvalidiry whetheror not random samplingis used.It alsi treatsthe generalization of causalrelationships from a singlesampleto unobserved instances as a matterof externalvalidity-againrwhether or not random samplingwas used.The fact that random sampling(which is associated with ex- ,.rrr"l uiiairy in this book) sometimes happens to facilitatethe constructlabeling of a sampleis incidentalto the fact that the population label is alreadyknown. Though many populationlabelsare indeedwell-known, many more are still mat- ,.r, of debate,as reflected in the examples we gavein Chapter3 of whetherper- sonsshouldbe labeledschizophrenic or settingslabeledas hostilework environ- ments.In theselatter cases, random samplingmakesno contribution to resolving debates about the applicabilityof thoselabels.Instead,the principlesand methods we outlinedin Ci"pt.rs 11 through 13 will haveto be brought to bear.And when random samplinghasnot beenused,thoseprinciplesand methodswill also haveto be broughito b.". on the externalvalidity problemof generalizing causal relationships from singlesamples to unobserved instances.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="288" confidence="possible" page="51" column="1">Objections About the Completeness of the Typology</h2>
          <region class="DoCO:TextChunk" id="289" confidence="possible" page="51" column="1">The first objectionof this kind is that our lists of particularthreatsto validity are incomplete. Bracht and Glass(1,968), for example,ad-ded new externalvalidity threatsthat they thought were overlookedby Campbell and Stanley(1,96311' and more recentlyAiken ind West (1991) pointed to new reactivity threats._ These challenges "r. i*portant because the key to the most confidentcausalconclusions in our ,f,.ory of validity is the ability to construct a persuasive argumentthat every plausibleand identifiedthreat to validity has beenidentifiedand ruled out. How- iver, thereis no guarantee that all relevantthreatsto validity havebeenidentified. Our lists are not divinely ordained,as can be observed from the changes in the threats from Campbel IUST) to Campbell and Stanley (1'963)to Cook and</region>
          <region class="DoCO:FigureBox" id="Fx290">
            <image class="DoCO:Figure" src="62p6.page_051.image_51.png" thmb="62p6.page_051.image_51-thumb.png"/>
          </region>
          <region class="unknown" id="291" page="52" column="1">14.A CRITICAL ASSESSMENT OF OURASSUMPTIONS</region>
          <region class="DoCO:TextChunk" id="292" confidence="possible" page="52" column="1">Campbell(1979) to this book. Threatsare better identifiedfrom insiderknowl- edgethan from abstractand nonlocal lists of threats. A second objectionis that we may haveleft out particularvalidity fypesor organized them suboptimally. Perhaps the bestillustration that this is true is Sack- ett's(1979) treatmentof bias in case-control studies. Case-control studies do not commonly fall under the rubric of experimentalor quasi-experimental designs; but they are cause-probing designs, and in that sense a general interestin generalized causalinferenceis at leastpartly shared.Yet Sackettcreateda different ty- pology.He organized his list around seven stages of research at which biascan occur: (1) in readingaboutthe field, (2) in sample specification and selection, (3) in defining the experimentalexposure,(4) 'in measuringexposureand outcome, (5) in dataanalysis, (5) in interpretation of analyses, and (71inpublishing results. Each of thesecould generate a validiry type, someof which would overlapcon- siderably with our validity types.For example,his conceptof biases "in executing the experimentalmanoeuvre" (p. 62) is quite similar to our internal validiry whereas his withdrawal biasmirrors our attrition. However,his list alsosuggests new validity types,such as biasesin readingthe literature,and biases he lists at each stageare partly orthogonal to our lists. For example,biases in readingin- clude biases of rhetoric in which "any of several techniques are usedto convince the readerwithout appealing to reason"(p. 60). In the end,then, our claim is only that the present typologyis reasonably well informed by knowledgeof the nature of generalized causal inference and of some of the problemsthat are frequentlysalientabout thoseinferences in field experi- mentation.It can and hopefullywill continueto be improvedboth by addition of threatsto existing validity types and by thoughtful exploration of new validity typesthat might pertainto the problem of generalized causal inference that is our main concern.t</region>
          <region class="unknown" id="293" page="52" column="1">1. We are acutelyaware of, and modestlydismayedat, the many differentusages of thesevalidity labelsthat have developedover the years and of the risk that posesfor terminological confusion---even though we are responsible for rnany of thesevariations ourselves.After all, the understandingsof validiry in this book differ from those in Campbelland distinctionwas betweeninternal and externalvalidity. They alsodiffer from Cook and Campbell (7979), in which externalvalidity was concerned with generalizing to and across populations of personsand settings,whereasall issuesof generalizingfrom the causeand effect operations constitutedthe domain of constructvalidity. himselfrelabeled internalvalidiry and external validiry as local molar causalvalidity and the principle of proximal similarity, respectively. Steppingoutside Campbell'stradition, usedtheselabelswith yet other meanings. He said internalvalidity is the problem of generalizing from samples to the domain about which the questionis asked,which soundsmuch like our construct validity except that he specifically denied any distinction betweenconstruct validiry and external validiry, using the latter term to refer to generalizingresults to unstudied populations, an issueof extrapolation beyond the data at hand. Our understandingof external validity includessuch extrapolations as one case,but it is not limited to that because it also has to do with empirically identifying sourcesof variation in an effect sizewhen existing data allow doing so. Finally, many other authors have casually used all theselabels in completelydifferent ways (Goetz &amp; LeCompte,1984; Kleinbaum,Kupper, &amp; Morgenstern,1982;Menard, 1991).So in view of all thesevariations, we urge that theselabels be used only with descriptionsthat make their intended understandings clear.</region>
          <outsider class="DoCO:TextBox" type="footer" id="294" page="52" column="1">::j !t t</outsider>
          <region class="DoCO:FigureBox" id="Fx295">
            <image class="DoCO:Figure" src="62p6.page_052.image_52.png" thmb="62p6.page_052.image_52-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="296" page="53" column="1">VALIDTTY 47s |</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="297" confidence="possible" page="53" column="1">Objections Concerning the Natureof Validity</h2>
          <region class="DoCO:TextChunk" id="298" confidence="possible" page="53" column="1">'We defined validity as the approximate truth of an inference. Others define it differently. Here are some alternatives and our reasonsfor not using them'</region>
          <region class="unknown" id="299" page="53" column="1">Validity in the New TestTheory Tradition Testtheorists discussed validity (e.g., cronbach, 1946; Guilford,1,946) well before inventedhis only begin to touch on the many iss.re, pertinentto validity that aboundin that tradition. Here we outline a f.* i.y poinis that help that of test theory.The early emphasis in test theory was mostly on inferences about what a test meas- or.j, ltltll with creditsCook a and -a-pbell for the giving notion "proper of constructvalidity. breadth to the Cronbach notion of 152) in through their claim that constructvalidity is not j"tt li-it.d to inferences about outcomesbut also about causes and about orherfeatures of experiments. In addition, early test theory tied validity to the truth of suchinferences: "The literatureon on the truthfulness of testinterpretation" (Cronbach, 1988, p' 5)' However,the yearshave bro.tght change to this early understanding' In one validity in test theory ;V"lidiry ii an integrated evaluative judgmentof the degree to which empiricalev- idenceand theoreti"cal rationales supportthe adequacy and appropriateness of inferences and actions based on testscores or other modesof assessment" (p. L3); and later he saysthat "Validiry is broadly definedas nothing lessthan an evaluative summary'of both the ruid.tr.. for and the actual-as well as potential- consequen.., of scoreinterpretation and use" (199 5, p.74L)._Whereas. our un- validity is that inferences are the subjectof validation, this definition suggeJt, th"t actionsare also subjectto validation and that validation is actually evaluation. These extentionsare far from our view. A little historywill help here.Tests are designed for practicaluse.Commer- cial test developers hope to profit from sales to thosewho usetests;employers hope to ,rr. t.rt, to and test takershope that testswill tell them something useful about themsqlves. These practicalapplications gen- tf,e to identify the characteristics of better and worse tests.APA appointeda Cronbachto address the problem.The first in a contin- uing series of this wolk alsoled to Cronbach and Melhl', (1955)classic article on The test standards have been freq.rerrtiy revised, most recentlycosponsored by other professional associations Psychological Association, and National Council on Measurement in 1999)' Re- qoirl-.nts to adhereto rhe standards became part of professional ethical codes. Th" ,tandardswere also influential in legaland and have</region>
          <region class="DoCO:FigureBox" id="Fx300">
            <image class="DoCO:Figure" src="62p6.page_053.image_53.png" thmb="62p6.page_053.image_53-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="301" page="54" column="1">14.A</outsider>
          <region class="unknown" id="302" page="54" column="1">CRITICAL ASSESSMENT OF OURASSUMPTIONS</region>
          <region class="DoCO:TextChunk" id="303" confidence="possible" page="54" column="1">ing practices (e.g., Albermarle Paper Co. v. MoodS 1975; Washington v. Davis,</region>
          <region class="unknown" id="304" page="54" column="1">beencited,for example,in U.S.Supreme Court cases about alleged misuses of test- L976) and have influencedthe "Uniform Guidelines"for personnel selectionby the Equal EmploymentOpportunity al. (1978).Various validity standards were particularly salientin theseuses. Because of this legal,professional, and regulatoryconcernwith the useof test-</region>
          <region class="DoCO:TextChunk" id="305" confidence="possible" page="54" column="1">ing, the researchcommunity concerned with measurementvalidity began to use the</region>
          <outsider class="DoCO:TextBox" type="sidenote" id="306" page="54" column="1">,;;:: i'</outsider>
          <region class="unknown" id="307" page="54" column="1">word ualidity moreexpansivelyforexample, atest" (Cronbach, 1989, p. M9).It is only a short distance from validating use to validating action, because most of the relevantuseswere actionssuchas hiring or firing someone or labelingsomeone retarded.Actions,in turn, haveconsequences-some positive,suchas efficiencyin hiring and accurate diagnosis that allows bettertailor- ing of treatment, and somenegative, such as lossof incomeand stigmatization. So Messick (1989 , 199 5l proposed that validationalsoevaluate those consequences, es- peciallythe consequences. Thus evaluating the consequences of test usebecame a key featureof validity in test theory.The net resultwas a blurring of the line betweenvalidity-as-truth and validity-as-evaluation, to the point where Cronbach (1988) of a testor testuseis evaluation" (p.4).</region>
          <region class="DoCO:TextChunk" id="308" confidence="possible" page="54" column="1">'We strongly endorse the legitimacy of questions about the use of both tests and experiments. Although scientistshave frequently avoided value questions in the mis- taken belief that they cannot be studied scientifically or that scienceis value free, we cannot avoid values even if we try. The conduct of experiments involves values at every step, from question selection through the interpretation and reporting of results. Concerns about the usesto which experiments and their results are put and the value of the consequences of those usesare all important (e.g.,Shadishet al., 1991), as we illustrated in Chapter 9 in discussingethical concerns with experiments. However, if validity is to retain its primary association with the truth of knowledge claims, then it is fundamentally impossible to validate an action be- causeactions are not knowledge claims. Actions are more properly evaluated, not validated. Supposean employer administers a test, intending to use it in hiring decisions. Suppose the action is that a person is hired. The action is not itself a knowledge claim and therefore cannot be either true or false. Supposethat person then physically assaultsa subordinate. That consequence is also not a knowledge claim and so also cannot be true or false. The action and the consequences merely exist; they are ontological entities, not epistemological ones. Perhaps Messick (1989) really meant to ask whether inferencesabout actions and consequences are true or false. If so, the inclusion of action in his (1,989)definition of validity is entirely superfluous, for validity-as-truth is already about evidencein support of in- ferences,including those about action or consequ.rr..s.'</region>
          <region class="unknown" id="309" page="54" column="1">2. Perhaps partly in recognitionof this, the most recentversionof the test Research Association, American Psychological Association, and National Council on Measurement in Education, 1999) helpsresolvesomeof the problemsoudined hereinby removingreference to validatingaction from the definition of validity: "Validity refersto the degree to which evidence and theory support the interpretations of test scores entailedby proposedusesof tests" (p. 9).</region>
          <outsider class="DoCO:TextBox" type="footer" id="310" page="54" column="1">i ,l , I t</outsider>
          <region class="DoCO:FigureBox" id="Fx311">
            <image class="DoCO:Figure" src="62p6.page_054.image_54.png" thmb="62p6.page_054.image_54-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="312" page="55" column="1">VALIDITY I 477</outsider>
          <region class="DoCO:TextChunk" id="313" page="55" column="1">Alternatively perhaps Messick ('1.989,L995) meant his definition to instruct "Valid- test validators to eualuatethe action or its consequences, as intimated in: ity is broadly defined as nothing less than an evaluative summary of both the evidence for and the actual-as well as potential--consequences of score interpretation and use" (1,995, p. 742). Validity-as-truth certainly plays a role in evaluating testsand experiments.But we must be clear about what that role is and is not. Philosophers(e.g., Scriven, 1980; Rescher,1969) tell us that a judgment about the value of something requires that we (1) selectcriteria of merit on which the thing being evaluated would have to perform well, (2) set standards of per- formanci for how well the thing must do on each criterion to be judged positivel5 (3) gather pertinent data about the thing's performance on the criteria, and then i+j i"6gr4te the results into one or more evaluative conclusions. Validity-as-truth is one (but only one) criterion of merit in dvaluation; that is, it is good if inferences about a test are true, just as it is good for the causal inference made from an experiment to be true. However, validation is not isomorphic with evaluation. First, criteria of merit for tests (or experiments) are not limited to validity-as-truth- For example, a good test meetsother criteria, such as having a test manual that reports ,ror*^r, being affordable for the contexts of application, and protecting confiden- tialiry ", "ppropriate. Second,the theory of validity Jvlessickproposed gives no help in accomplishing some of the other steps in the four-step evaluation process outlined previously. To evaluate a test, we need to know something about how much ualidity the inference should have to be judged good; and we need to know how to integrate results from all the other criteria of merit along with validity into an overall waluation. It is not a flaw in validity theory that these other steps are not addressed,for they are the domain of evaluation theory. The latter tells us something about how to executethesesteps (e.g.,Scriven, 1980, 1'991)and also about other matters to be taken into account in the evaluation. Validation is not evaluation; truth is not value. Of course, the definition of terms is partly arbitrary. So one might respond that one should be able to conflate validity-as-truth and validity-as-evaluation if one so chooses.However: The very fact that termsmusrbesupplied with arbitrarymeanings requires that words be usedwith a greatsense of responsibility. This responsibility is twofold: first, to established ,6"9"; second, to the limitationsthat the definitionsselected impose on the "l'982, user.(Goldschmidt, P. 642) 'We need the distinction between truth and value becausetrue inferencescan be about bad things (the fact that smoking causescancer does not make smoking or cancer good); "nd f"lr. inferencescan lead to good things (the astrologer'sadvice to Piscei to 'lavoid alienating your coworkers today" may have nothing to do with heavenly bodies, but may still be good advice). Conflating truth and value can be actively harmful. Messick (1995) makes clear that the social consequences of testing are to be judged in terms of "bias, fairness, and distributive justice" (P. 745). 'Wi agreewith this statement,but this is test evaluation, not test validity. Messick</region>
          <region class="DoCO:FigureBox" id="Fx314">
            <image class="DoCO:Figure" src="62p6.page_055.image_55.png" thmb="62p6.page_055.image_55-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="315" page="56" column="1">I</outsider>
          <region class="DoCO:TextChunk" id="316" confidence="possible" page="56" column="1">478 ra. n cRrTrcAL ASSESSMENT OFOUR ASSUMPTTONS |</region>
          <region class="DoCO:TextChunk" id="318" page="56" column="1">notes that his intention is not to open the door to the social policing of truth (i.e., a test is valid if its social consequences are good), but ambiguity on this issuehas nonethelessopened this very door. For example, Kirkhart (1,995)cites Messick as justification for judging the validity of evaluations by their social consequences: "Consequential validity refers here to the soundnessof changeexerted on systems by evaluationand the extent to which thosechanges are just" (p.a).This notion is risky because the most powerful arbiter of the soundnessand iustice of social consequences is the sociopolitical systemin which we live. Depending on the forces in power in that system at any given time, we may find that what counts as valid is effectively determined by the political preferencesof those with power. <marker type="block"/> Validity in the Qualitative Traditions One of the most important developmentsin recent social researchis the expanded use of qualitative methods such as ethnography ethnology, participant observation, unstructured interviewing, and case study methodology (e.g., Denzin 6c Lincoln, 2000). These methods have unrivaled strengths for the elucidation of meanings, the in-depth description of cases,the discovery of new hypotheses,and the description of how treatment interventions are implemented or of possible causal explanations. Even for those purposes for which other methods are usually preferable,such as for making the kinds of descriptivecausalinferences that are the topic of this book, qualitative methods can often contribute helpful knowledge and 'S7henever on rare occasionscan be sufficient (Campbell, 1975; Scriven, 1976ll. resources allow, field experiments will benefit from including qualitative methods both for the primary benefits they are capable of generatingand also for the assis- tance they provide to the descriptive causal task itself. For example, they can uncover important site-specificthreats to validiry and also contribute to explaining experimental results in general and perplexing outcome patterns in particular. However, the flowering of qualitative methods has often been accompanied by theoretical and philosophical controversy, often referred to as the qualitative- quantitative debates. These debates concern not just methods but roles and re- wards within science,ethics and morality and epistemologiesand ontologies. As part of the latter, the concept of validity has receivedconsiderableattention (e.g., Eisenhart &amp; Howe, 1992; Goetz &amp; LeCompte,1984; Kirk &amp; Miller, 1'986;Kvale, 1.989;J. 'Wolcott, Maxwell, 1.992;J. Maxwell 6c Lincoln, 1.990;Mishler, 1,990;Phillips, 1,987; 1990). Notions of validity that are different from ours have occa- sionally resulted from qualitative work, and sometimesvalidity is rejectedentirely. However, before we review those differences we prefer to emphasize the com- monalities that we think dominate on all sides of the debates.</region>
          <region class="DoCO:TextChunk" id="319" confidence="possible" page="56" column="1">Comtnonalities. As we read it, the predominant view among qualitative theorists is that validity is a concept that is and should be applicable to their work..We start with examples of discussionsof validity by qualitative theorists that illustrate these similarities because they are surprisingly more common than someportrayals in the</region>
          <outsider class="DoCO:TextBox" type="footer" id="320" page="56" column="1">:</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="321" page="56" column="1">I</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="322" page="56" column="1">:l{</outsider>
          <region class="DoCO:FigureBox" id="Fx323">
            <image class="DoCO:Figure" src="62p6.page_056.image_56.png" thmb="62p6.page_056.image_56-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="324" page="57" column="1">VALIDITY I O''</outsider>
          <region class="DoCO:TextChunk" id="325" confidence="possible" page="57" column="1">qualitative-quantitative debates suggest and because they demonstrate an underlying by"*ori unity of social interestin scientiits.For producingvalid example,Maxwell knowledge that (1990) we believe says, is "qualitative widely shared researchers are just as concerned as quantitativeonesabout'getting it wrong,' and validity broadlydefinedsimplyrefersto the possible ways one'saccountmight be *rorrg, quafi[tive and theoristswho how these 'validity saythey threats' rejectthe can word be addressed" ualidity will (p. admit 505). that Even they those "go to considerable pains not to getit all wrong" (Wolcott,1990,p. L27).Kvale(1989) tiesvalidity directlyto truth, saying "conceptsof validity are rootedin more com- prehensive epistemological assumptions of the nature of true knowledge"(p. 1-1); Kirk and later and that Miller validity (1986) l'refersto say "the the technicaluseof truth and correctness the term of 'valid' a statement"(p.731. is as a properly hedgedweak synonymfor 'true' " (p. L9). Maxwell (L9921says"Validiry in a broad sense, pertainsto this relationshipbetweenan accountand something out- sidethat account" (p. 283). All theseseemquite compatiblewith our understanding of validity. Maxvreli's(7992\ accountpoints to other similarities. He claimsthat validity is always relative to "the kinds of understandings that accountscan embody" (p. 28il and that different communitiesof inquirers are interested in different kindsof understandings. He notesthat qualitativeresearchers are interested in five kinds of understandings about: (1) the descriptions of what was seen and heard, (2) the meaningof what was seenand heard, (3) theoreticalconstructions that characteriz. *h"t was seenand heardat higher levelsof abstraction, (4) generalizationof accounts to other persons, times, or settings than originally studied,and (5) evaluations of the objectsof study (Maxwell, 1'992;he saysthat the last two understandings are of interestrelativelyrarely in qualitativework). He then proposes ?ine orrd..standings. a five-p-art validity 'We agree typology that validity for qualitativeresearchers, is relativeto understanding, one for eachof though the we usuallyrefer to in-ference iather than understanding. And we agree that different communities of inquirerstend to be interested in different kinds of understand- ings,though common interests are illustratedby the apparentlysharedconcerns thlt both ixperimentersand qualitativeresearchers have in how bestto charac- terizewhatwas seen and heardin a study (Maxwell'stheoreticalvalidity and our constructvalidity). Our extended discussion of internal validity reflectsthe interest of the community of experimenters in understanding descriptive causes' pro- portionatelymore so than is relevantto qualitativeresearchers, even when their reportsare necessarily replete with the language of causation. This observation is ,rot " criticismof qualitativeresearchers, nor is it a criticism of experimenters as being lessinterested than qualitativeresearchers in thick descriptionof an indi- vidualcase. On the other hand, we should not let differences in prototypical tendencies acrossresearch communitiesblind us to the fact that when a particular under- standingls of interest, the pertinentvalidity concerns are the sameno matterwhat the metlodology usedto developthe knowledge claim. It would be wrong for a</region>
          <region class="DoCO:FigureBox" id="Fx326">
            <image class="DoCO:Figure" src="62p6.page_057.image_57.png" thmb="62p6.page_057.image_57-thumb.png"/>
          </region>
          <region class="unknown" id="327" page="58" column="1">14.A CRITICAL ASSESSMENT OF OURASSUMPTIONS</region>
          <region class="DoCO:TextChunk" id="328" confidence="possible" page="58" column="1">qualitative researcher to claim that internal validity is irrelevantto qualitative methods.Validity is not a properry of methodsbut of inferences and knowledge claims. On those infrequent occasions in which a qualitative researcher has a stronginterestin a local molar causal inference, the concerns we haveoutlinedun- der internal validity pertain.This argumentcuts both ways,of course. An exper- imenterwho wonderswhat the experiment means to participants could learna lot from the concerns that Maxwell outlinesunder interpretivevalidity. Maxwell (1992) also points out that his validity typology suggests threats to validity about which qualitativeresearchers seek "evidence that would allow them to be ruled-out. . . usinga logic similar to that of quasi-experimental researchers such as Cook and Campbell" (p. 296). He does not outline such threatshimself,but his descriptionallows one to guess what somemight look like. To judge from Maxwell's prose,threats to descriptivevalidity include errors of commission(describing something that did not occur),errorsof omission (failingto describe something that did occur),errorsof frequency (misstat- ing how often something occurred), and interrater disagreement about description. Threatsto the validity of knowledge claims havealsobeeninvoked by qualitative theorists other than Maxwell-for example,by Becker(1979), Denzin(1989'), and Goetzand LeCompte(1984).Our only significant disagreement with Maxwell's discussionof threats is his claim that qualitative researchers are lessable to use "designfeatures"(p. 296) to deal with threatsto validity. For instance,his preferreduseof multiple observers is a qualitativede- signfeaturethat helpsto reduce errors of omission, commission, and frequency. The repertoireof designfeatures that qualitativeresearchers use will usuallybe quite different from those used by researchers in other traditions, but they are designfeatures (methods) all the same.</region>
          <region class="DoCO:TextChunk" id="329" confidence="possible" page="58" column="1">Dffirences. Theseagreements notwithstanding,many qualitativetheoristsap- proach validity in ways that differ from our treatment.A few of thesedifferences are based on arguments that are simplyerroneous (Heap, 7995;Shadish, 1995a). But many are thoughtful and deserve more attention than our space constraints allow. Following is a sample. Somequalitativetheoristseither mix togetherevaluativeand socialtheories of truth (Eisner, \979,1983) or propose to substitute the socialfor theevaluative. SoJensen (1989) saysthat validiry refersto whethera knowledgeclaim is "meaningful and relevant" (p. 107) to a particular language community; andGuba and Lincoln (1,982) say that truth can be reduced to whetheran accountis credibleto thosewho read it. Although we agreethat socialand evaluative theories comple- ment eachother and are both helpful, replacingthe evaluative with the socialis misguided. These social alternatives allow for devastatingcounterexamples (Phillips, 1987): the swindler'sstory is coherentbut fraudulent;cults convince members of beliefsthat havelittle or no apparentbasisotherwise; and an account of an interactionbetweenteacherand studentmight be true evenif neitherfound it to be credible.Bunge(1992) showshow one cannotdefinethe basicideaof er-</region>
          <outsider class="DoCO:TextBox" type="footer" id="330" page="58" column="1">I :J I :iil</outsider>
          <region class="DoCO:FigureBox" id="Fx331">
            <image class="DoCO:Figure" src="62p6.page_058.image_58.png" thmb="62p6.page_058.image_58-thumb.png"/>
          </region>
          <region class="unknown" id="332" page="59" column="1">14.A CRITICAL ASSESSMENT OF OURASSUMPTIONS</region>
          <region class="DoCO:TextChunk" id="334" page="59" column="1">qualitative researcher to claim that internal validity is irrelevant to qualitative methods. Validity is not a properfy of methods but of inferencesand knowledge claims. On those infrequent occasions in which a qualitative researcher has a strong interest in a local molar causal inference,the concernswe have outlined under internal validity pertain. This argument cuts both ways, of course. An experimenter who wonders what the experiment meansto participants could learn a lot from the concerns that Maxwell outlines under interpretive validity. Maxwell (1,992) also points out that his validity typology suggeststhreats to validity about which qualitative researchers seek "evidencethat would allow them to be ruled-out . . . using a logic similar to that of quasi-experimentalre- searcherssuch as Cook and Campbell" (p. 296). He does not outline such threats himself, but his description allows one to guess what some might look like. To judge from Maxwell's prose, threats to descriptive validity include errors of commission (describing something that did not occur), errors of omission (failing to describesomething that did occur), errors of frequency (misstat- itg how often something occurred), and interrater disagreement about description. Threats to the validity of knowledge claims have also been invoked by qualitative theorists other than Maxwell-for example, by Becker (1,979), Denzin (1989), and Goetz and LeCompte (1984). Our only significant disagreement with Maxwell's discussion of threats is his claim that qualitative re- searchersare less able to use "design features" (p. 2961to deal with threats to validity. For instance, his preferred use of multiple observers ls a qualitative design feature that helps to reduce errors of omission, commission, and frequency. The repertoire of design featuresthat qualitative researchers use will usually be quite different from those used by researchersin other traditions, but they are design features (methods) all the same. <marker type="block"/> Differences. These agreementsnotwithstanding, many qualitative theorists approach validity in ways that differ from our treatment. A few of thesedifferences are basedon argumentsthat are simply erroneous(Heap, 1.995;Shadish,1995a). But many are thoughtful and deservemore attention than our spaceconstraints allow. Following is a sample. Some qualitative theorists either mix together evaluative and social theories "1.979,1983) of truth (Eisner, or propose to substitutethe socialfor the evaluative. So Jensen(1989) saysthat validiry refers to whether a knowledge claim is "meaningful and relevant" (p. L07l to a particular language community; and Guba and Lincoln (t9821say that truth can be reduced to whether an account is credible to those who read it. Although we agree that social and evaluative theories comple- ment each other and are both helpful, replacing the evaluative with the social is misguided. These social alternatives allow for devastating counterexamples (Phillips, L987): the swindler's story is coherent but fraudulent; cults convince members of beliefs that have little or no apparent basis otherwise; and an account of an interaction between teacher and student might be true even if neither found it to be credible. Bunge (1992) shows how one cannot define the basic idea of er-</region>
          <outsider class="DoCO:TextBox" type="sidenote" id="335" page="59" column="1">.il</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="336" page="59" column="1">j t I I 'iil</outsider>
          <region class="DoCO:FigureBox" id="Fx337">
            <image class="DoCO:Figure" src="62p6.page_059.image_59.png" thmb="62p6.page_059.image_59-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="338" page="60" column="1">VALIDITY I +ET</outsider>
          <region class="DoCO:TextChunk" id="339" confidence="possible" page="60" column="1">ror usingsocialtheoriesof truth. Kirk and Miller (1986) capturethe needfor an evaluative theory of truth in qualitativemethods:</region>
          <region class="unknown" id="340" page="60" column="1">In response to the propensity of so many nonqualitative researchtraditions to use such hidden positivist assumptions, some social scientists have tended to overreact by stressinj the possibility ;f alternative interpretations of everything to th€ exclusion of urry .ffor, to chooseamong them. This extreme relativism ignores the other side of ob- leciivity-that there is an external world at all. It ignores the distinction between lrro*l"dg. and opinion, and results in a separateinsight that cannot be reconciledwith anyone 15)</region>
          <region class="DoCO:TextChunk" id="341" confidence="possible" page="60" column="1">A second difference refers to equating the validity of knowledgeclaimswith their evaluation, as we discussed earlier with tqsttheory (e.g., Eisenhart 6C Howe, L992)' This is mostexplicitin Salner (L989),whosuggested that much of validityin qualitative methodoiogyconcerns the criteria "that are useful for evaluatingcompeting claims',(p. 51);"id rh. urges researchers to expose the moral andvalueimplications the of ,.r."rch, sameas *.rch for test asMessick theory. 'We (1.989) endorsethe said in reference need to to evaluateknowledge testtheory.Our response claims is broadly including their moial implications; but this is not the same as saying that the claim is-t.ue. Truih is just onecriterionof merit for a good knowledge claim. A third differencemakes validity a result of the processby which truth emerges. For instance, emphasizing the dialecticprocessthat givesrise to truth' Salnei(l9g9l says: ,,ValidLnowledge claims emerge . . . from the conflict and differences between the contextsthemselves as thesedifferences are communicated and negotiated among people who share decisions and actions"(p. 61).Miles and Huberman(1984)rpr"t of th. problemof validity in qualitative methodsbeing an insufficiency of 'Lnalysis procedures for qualitative data" (p. 230). Guba and Lincoln (1989) argue that tiustworthinessemerges from communicationwith other colleagues arid stakeholders. The problemwith all thesepositionsis the error of thinklng that validity is a property of methods.Any procedure for generating knowledg! can g.n.r"i. invalid-knowledge, so in the end it is the knowledge claim itself that muJt be judged.As Maxwell (1992) says,"The validity of an account is inherent,not in the procedures used to produceand validateit, but in its relationshipto thosethings it is intendedto be an accountof" (p' 281)' reformulatedfor A fourth differencesuggests qualitativemethodsbecause that traditional validiry approaches "historically to validity arosein must the be context of experimental research"(Eisenhart 6C Howe, 1992,p' 64\' Othersre- ject validity for similar reasons except that they saythat validity arosein test the- o.y 1..g.,*lol.orr, 19gO). Both are incorrect,for validiry concerns probably first "ror. Jrt.*"ti.ally in philosophypreceding test theory and experimental science by hundredsor thour"ndr of years.Validity is pertinentto any discussion of the warrant for believing knowledgeand is not specificto particular. methods. A fifth differenie .on..rrri the claim that there is no ontological reality at all, so thereis no truth to correspond to it. The problemswith this perspective "r. .rror1nous (Schmitt,1,995). First, evenif it were true' it would apply only to</region>
          <region class="DoCO:FigureBox" id="Fx342">
            <image class="DoCO:Figure" src="62p6.page_060.image_60.png" thmb="62p6.page_060.image_60-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="343" page="61" column="1">-T</outsider>
          <region class="DoCO:TextChunk" id="344" confidence="possible" page="61" column="1">8z I r+.n cRtlcAL AssEssMENT oF ouR AssuMploNs</region>
          <region class="DoCO:TextChunk" id="345" confidence="possible" page="61" column="1">correspondence theories of truth; coherence and pragmatist theories would be unaffected. Second, the claim contradicts our experience. As Kirk and Miller ( 1 9 8 6 1 p u ti t : Thereis a world of empiricalreality out there.The way we perceive and understand that world is largelyup to us, but the world doesnot tolerateall understandings of it equally(sothat the individualwho believes he or shecanhalt a speeding train with his or her bare handsmay be punishedby the world for actingon that understanding). ( p .1 1 )</region>
          <region class="DoCO:TextChunk" id="346" page="61" column="1">Third, the claim ignores evidenceabout the problems with people'sconstructions. Maxwell notes that "one of the fundamental insights of the social sciences is that people's constructions are often systematic distortions of their actual situation" (p. 506). FinallS the claim is self-contradictory becauseit implies that the claim itself cannot be rrue. A sixth difference is the claim that it makes no senseto speak of truth because there are many different realities, with multiple truths to match each (Filstead, 1.979;Guba 6c Lincoln, L982; Lincoln 6c Guba, 1985). Lincoln (L990), for example, says that "a realist philosophical stance requires, indeed demands, a singular reality and thereforea singulartruth" (p. 502), which shejuxtaposesagainst her own assumption of multiple realities with multiple truths. Whatever the merits of the underlying ontological arguments, this is not an argument against validity. Ontological realism (a commitment that "something" does exist) does not require a singular reality but merely a commitment that there be at least one reality. To take just one example, physicists have speculated that there may be circumstancesunder which multiple physical realities could exist in parallel, as in the case of Schrodinger's cat (Davies,1984; Davies &amp; Brown, 1986). Such circumstances would in no way constitute an objection to pursuing valid characterizationsof those multiple realities. Nor for that matter would the existenceof multiple realities require multiple truths; physicists use the same principles to account for the multiple realities that might be experiencedby Schrodinger'scat. Epistemological realism (a commitment that our knowledge reflects ontological reality) does not require only one true account of that world(s), but only that there not be two contradictory accounts that are both true of the same ontological referent.3 How many realities there might be, and how many truths it takes to account for them, should not be decided by fiat. A seventh difference objects to the belief in a monolithic or absolute Truth (with capital T). rUfolcott (1990) says, "'What I seek is something else, a quality that points more to identifying critical elements and wringing plausible interpretations from them, something one can pursue without becoming obsessed with</region>
          <region class="unknown" id="347" page="61" column="1">3. The fact that different people might have different beliefs about the same referent is sometimes cited as violating this maxim, but it need not do so. For example, if the knowledge claim being validated is "John views the program as effective but Mary views it as ineffective," the claim can be true even though the views of John and Mary are contradictory.</region>
          <outsider class="DoCO:TextBox" type="footer" id="348" page="61" column="1">j ii j</outsider>
          <region class="DoCO:FigureBox" id="Fx349">
            <image class="DoCO:Figure" src="62p6.page_061.image_61.png" thmb="62p6.page_061.image_61-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="350" page="62" column="1">VALIDITY I 483</outsider>
          <region class="DoCO:TextChunk" id="351" confidence="possible" page="62" column="1">finding the right or ultimate answer'the correctversion,the Truth" (p' 146)' He describes "the critical point of departurebetweenquantities-oriented and quali- ties-oriented research [as beingthat] we cannot 'know'with the former'ssatisfy- ing levelsof certainty" (p. 1,47).Mishler (t990) objectsthat traditional ap- prl".h., to validationare portrayed "as universal,abstractguarantorsof truth" ip. +ZOl.Lincoln (1990) thinksthat "the realist positiondemands absolute truth" tp. SOZI. However, it is misguided to attributebeliefs in certaintyor absolute truth tf appioaches to validity srrchas that in this book.'We hope we havemadeclear by now that thereare no guarantorsof valid inferences. Indeed,the more experi- encethat mostexperimenters gain, the morethey appreciate the ambiguityof their results. Albert Einstein once said, "An experiment is something everybody believes exceptthe personwho madeit" (Holton, 1986, p. 13).Like \(olcott, most ex- periri-renter, ,..k only to wring plausibleinterpretations from their work, believ- irrg thut "prudence sat poisedbetween skepticism and credulity" (Shapin,1994, p."xxix). rilfle tteed nor, shouldnot, and frequentlycannot decidethat one account i, ,broirrt.ly true ani the other completelyfalse.To the contrary' tolerancefor multiple knowledgeconstructions is a virtual necessity (Lakatos, 1'978)because evidence is frequeirtlyinadequate to distinguishbetweentwo well-supported ac- counrs(islight " p"tti.l. or wave?), and sometimes accounts that appear to be un- supported "An by euiJence for manyyears turn out to betrue (do germs cause ulcers?)- moral shoitcomings. eighih difference The arguments claims that here traditional aremany,for understandings example, that of validity it "forces have issues (Lincoln, of politics, 1,990, p. ial,res 503) and (social implicitly and scientific), empowers and "social ethics science to be 'experts' submerged" . whoseclass preoccupations (primarily'$7hite, male, and middle-class) ensure status minoritygroupmembers" for somevoiceswhile marginalizittg. (Lincoln, "1.990,p. . . thoseof 502). Althoughthese women, persons arguments of color, may or b. ou"..tlted, they contain important cautions.Recallthe examplein Chapter3 that '..r.ur.h. partly ,,Eventhe due to the rats dominance werewhite of males" 'White malesin in healthresearch. the designand No doubt executionof this biaswas health None of the methodsdiscussed in this book are intendedto redress this problem or are capableof it. The purposeof experimental designis to elucidate ca.rsal inferences -or. than morallnferences.'What is lessclearis that this problem requiresabandoning notions of validity or truth. The claim that traditional ,pprou.h.s to truth forcibly submerge political and ethicalissues is simplywrong. Tb-the extent that morality is reflectedin the questionsasked,the assumptions made,and the outcomes examined, experimentefs can go a long way by ensuring a broad representation of stakeholder voicesin study design. Further,moral social sciencereiuires commitment to truth. Moral righteousness without truthful analysis is ihe stuff of totalitarianism.Moral diversity helpspreventtotalitarian- ism, but without the discipline provided by truth-seeking,diversity offers no -."16 to identify thoseoptionsthat are good for the human condition,which is, after all,the essence of morality.In order to havea moral socialscience, we must haveboih the capacityto elucidate personalconstructions and the capacity to see</region>
          <region class="DoCO:FigureBox" id="Fx352">
            <image class="DoCO:Figure" src="62p6.page_062.image_62.png" thmb="62p6.page_062.image_62-thumb.png"/>
          </region>
          <region class="unknown" id="353" page="63" column="1">484 14.A CR|T|CAL ASSESSMENT OF OURASSUMPTTONS |</region>
          <region class="unknown" id="354" page="63" column="1">how thoseconstructions reflect and distort reality (Maxwell, 19921. 'We embrace the moral aspirations of scholarssuchas Lincoln, but giving voiceto thoseaspi- rations simply doesnot requireus to abandonsuchnotions as validity and truth.</region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="355" page="63" column="1">Q UASI.EXPE RI M ENTATION</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="356" confidence="possible" page="63" column="1">Criteria for RulingOut Threats: The Centralityof FuzzyPlausibility</h2>
          <region class="DoCO:TextChunk" id="361" page="63" column="1">In a randomized experiment in which all groups are treated in the sameway excepr for treatment assignment,very few assumptionsneed to be made about ro,rr.", of bias. And those that are made are clear and can be easily tested,particularly as concerns the fidelity of the original assignment process and its subsequentmainte- nance. Not surprisinglS statisticiansprefer methods in which the assumptionsare few, transparent, and testable. Quasi-experiments, however, rely heavily on re- searcheriudgments about assumptions, especiallyon the fuzzy but indispensable concept of plausibility. Judgments about plausibility are neededfor deciding which of the many threats to validity are relevant in a given study for deciding whether a particular designelement is capable of ruling out a given threat, for estimating by how much the bias might have been reduced, and for assessing whether multiple threats that might have been only partially adjusted for might add up to a total bias greater than the effect size the researcher is inclined to claim. Vith quasi- experiments, the relevant assumptions are numerous, their plausibility is less evident, and their single and joint effectsare lesseasily modeled. We acknowledgethe fuzzy way in which particular internal validity threats are often ruled out, and it is becauseof this that we too prefer randomized experiments (and regressiondiscon- tinuity designs)over most of their quasi-experimentalalternatives. But quasi-experiments vary among themselveswith respect to the number, transparencg and testability of assumptions. Indeed, we deliberately ordered the chapters on quasi-experiments to reflect the increase in inferential power that comes from moving from designs without a pretest or without a comparison group to those with both, to those based on an interrupted time series,and from there to regression discontinuity and random assignment.Within most of these chapters we also illustrated how inferencescan be improved by adding design el- ements-more pretest observation points, better stable matching, replication and systematic removal of the treatment, multiple control groups, and nonequivalent dependentvariables. In a sense,the plan of the four chapters on quasi-experiments reflects two purposes. One is to show how the number, transparency and testability of assumptions varies by type of quasi-experimental design so that, in the best of quasi-experiments,internal validity is not much worse than with the randomized experiment. The other is to get students of quasi-experimentsto be more sparing with the use of this overly general label, for it threatens to tar all quasi- <marker type="page" number="64"/><marker type="block"/> experimentswith the samenegativebrush. As scholarswho have contributed to the institution alization of the t i^ quoti-experiment, we feel a lot of ambivalence about our role. Scholarsneed to itrint critically about alternatives to the randomized experiment, and from this need arisesthe need for the quasi-experimental label. But all instancesof quasi-experimentaldesignshould not be brought under the sameunduly broad quasi-experimentalumbrella if attributes of the best studiesdo not closely match the weaker attributes of the field writ large. Statisticians seek to make their assumptions transparent through the use of formal models laid out as formulae. For the most part, we have resistedthis strategy becauseit backfires with so many readers,alienating them from the very con- .!pt.r"t issuesthe formul ae aredesignedto make evident.'We have used words in- stead.There is a cost to this, and not jupt in the distaste of statistical cognoscenti' particularly those whose own research has emphasized statistical models- The main cost is that our narrative approach makes it more difficult to formally demonstrate how much fewer and more evident and more testable the alternative interpretations became as we moved from the weaker to the stronger quasi- .*p.ri-.rrts, both within the relevant quasi-experimental chapters and acrossthe set of them. 'We regret this, but do not apologize for the accessibility we tried to create by minimirirrg the use of Greek symbols and Roman subscripts. Fortunately, this deficit is not absolute, as both we and others have worked to develop meth;ds that can be used to measurethe size of particular threats' both in particular studies(e.g.,Gastwirth et al., L994;Shadishet al., 1998; Shadish,2000) and in sets of studiis (e.g.,Kazdin 6c Bass, 1989; Miller, Turner, Tindale, Posavac,6c</region>
          <region class="DoCO:FigureBox" id="Fx358">
            <image class="DoCO:Figure" src="62p6.page_063.image_63.png" thmb="62p6.page_063.image_63-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="footer" id="359" page="63" column="1">t</outsider>
          <outsider class="DoCO:TextBox" type="header" id="360" page="64" column="1">QUASI-EXPERIMENTATION | +SS</outsider>
          <region class="unknown" id="362" page="64" column="1">Dugoni, 1,991;Ror."nitt.t &amp; Rubin,1,978;Willson &amp; Putnam,t982\. Further, our</region>
          <region class="DoCO:TextChunk" id="363" confidence="possible" page="64" column="1">narrative approach has a significant advantage over a more narrowly statistical emphasisii allows us to addressa broad er array of qualitatively different threats to validitS threats for which no statistical measure is yet available and that therefore mighi otherwise be overlooked with too strict an emphasison quantification. Better to h"u. imprecise attention to plausibility than to have no attention at all paid to many imptrtant threats just becausethey cannot be well measured'</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="364" confidence="possible" page="64" column="1">PatternMatchingas a Problematic Criterion</h2>
          <region class="unknown" id="365" page="64" column="1">This book is more explicitthan its predecessors about the imbuing a causalhypothesis with multiple tistable the data, providedthat they serve tt reduce the viability of alternative causal explanations. In a sense' we havesoughtto substitute a pattern-matching me{rod-ology 'We for the u-sual assessment of wheth-era few means,oft.n only fwo, reliably differ. do this not because .o-pl.*ity itself is a desideratum in science. To the contrary,simpliciry in the num- be, of questions asked and highly prizedin science. The simplicity of ,arrjomized experiments for descriptive causal well. However,the samesimple circumstance does not hold with quasi-experiments. With them. we haveassirtedthat causalinference is improvedthe more specific,</region>
          <region class="DoCO:FigureBox" id="Fx366">
            <image class="DoCO:Figure" src="62p6.page_064.image_64.png" thmb="62p6.page_064.image_64-thumb.png"/>
          </region>
          <region class="unknown" id="367" page="65" column="1">488 | ro.o cRtlcAL AssEssMENT oF ouR AssuMploNs</region>
          <region class="unknown" id="368" page="65" column="1">generating these lists.The main concernwas to havea consensus of educationre- searchers he guessed that the number of thesebest practicesthat depended on randomizedexperiments would be zero. Several nationally known educationalresearchers were present,agreedthat such assignment probably playedno role in generating the list, and felt no distress at this. So long as the belief is widespreadthat quasi-experiments constitutethe summit of what is neededto support causalconclusions, the support for experimentation that is currently found in health, agriculture,or health in schoolsis unlikely to occur.Yet randomizationis possible in.manyeducational contextswithin schools if the will existsto carry it out (Cook et al., 1999;Cook et al., in press). An un- fortunate and inadvertentside effect of seriousdiscussion of quasi-experiments may sometimes be the practicalneglect of randomized experiments. That is a pity.</region>
          <region class="unknown" id="369" page="65" column="1">RANDOMIZED EXPERIMENTS</region>
          <region class="unknown" id="370" page="65" column="1">This section lists objections that havebeenraised to doingrandomized experiments, and our analysis of the more and lesslegitimate issues that these obiections raise.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="371" confidence="possible" page="65" column="1">Experiments Cannot Be Successfully lmplemented</h2>
          <region class="DoCO:TextChunk" id="372" page="65" column="1">Even a little exposure to large-scalesocial experimentation shows that treatments are often improperly or incompletely implemented and that differential attrition often occurs. Organizational obstaclesto experiments are many. They include the reality that different actors vary in the priority they attribute to random assignment, that some interventions seem disruptive at all levels of the organization, and that those at the point of service delivery often find the treatment require- ments a nuisance addition to their aheady overburdened daily routine. Then there are sometimes treatment crossovers,as units in the control condition adopt or adapt components from the treatment or as those in a treatment group are ex- posed to some but not all of these same components. These criticisms suggestthat the correct comparison is not between the randomized experiment and better quasi-experiments when each is implemented perfectly but rather between the randomized experiment as it is often imperfectly implemented and better quasi- experiments. Indeed, implementation can sometimes be better in the quasi- experiment if the decision not to randomize is based on fears of treatment degradation. This argument cannot be addressedwell becauseit dependson specifying the nature and degree of degradation and the kind of quasi-experimental alternative. But taken to its extreme it suggeststhat randomized experiments have no special warrant in field settings becausethere is no evidencethat they are stronger than other designs in practice (only in theory). But the situation is probably not so bleak. Methods for preventing and coping with treatment degradation are improving rapidly (seeChapter 10, this vol-</region>
          <region class="DoCO:FigureBox" id="Fx373">
            <image class="DoCO:Figure" src="62p6.page_065.image_65.png" thmb="62p6.page_065.image_65-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="374" page="66" column="1">RANDOMIZED EXPERIMENTS I AAS</outsider>
          <region class="DoCO:TextChunk" id="375" confidence="possible" page="66" column="1">umel Boru ch,1997;Gueron,1,999;Orr, L999).More important, random assign- -.n, may still create a superiorcounterfactual to its alternatives evenwith the flaws mentionedherein.FLr e*ample,Shadishand Ragsdale (1'9961foundthat, .o-p"..d with randomized."p..i-.tts without attrition, randomizedexperi- mentswith attrition still yieldedbetter effect size estimates than did nonrandomized experiments. Sometimes, of course,an alternative to severely degraded ran- domizaiion will be best,such as a strong interruptedtime series with a control' But routine rejectionof degraded randomizedexperiments is a poor rule to fol- l,o*; it takescarefulstudy and judgmentto decide.Further,many alternatives to experimentation are themselu.i ,ob;..t to treatmentimplementation flaws that thieatenthe also occur in validity them. 'we of inferences also suspect from that them. implementationflaws Attrition and treatment are salientin crossovers ex- f.ri-errt"tion "r. .o critical of because eachothlr's experiments work. hav6 By contrast,criteria beenaround so for long assessing and experimenters the quality of implementation and results from othermethodsarefar more recent (e'g',Datta, D97j,and they may thereforebe lesswell developed conceptuallS less subjected to peercriticism,and lessimprovedby the lessons of experience.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="376" confidence="possible" page="66" column="1">Experimentation Needs StrongTheoryand Standardized Treatment lm Plementation</h2>
          <region class="DoCO:TextChunk" id="377" confidence="possible" page="66" column="1">Many critics claim that experimentationis more fruitful when an intervention rs basedon strongsubstantive theory when implementationof treatment detailsis faithful to that theor5 when the rlsearchsettingis well managed, and when implementation does ,roi uury much between units' In many field experiments' these conditions are not met. For example,schools arclarge, complex, social organiza' iio"r *ith multiple programs,disputatiouspolitics, and conflicting stakeholder goals.Many progr"*, a"re implemented variablyacross school districts,aswell as f.ror, ..hoth, .Lrrroo-r, arri ,t.rdents. There can be no presumPli9n of standard implementation or fidelity to programtheory (Berman&amp; Mclaughlin, 1'977)' But thesecriticismsur., i' fa-ct, misplaced. Experimentsdo not requirewell- specifiedprogram theories,good program management, standardimplementa- tion, or treatments that are tJtally ?aithful to theory' Experiments make. a contribution when they simplyprobewhetheran intervention-as-implemented makesa marginal improvem.tttt.yord other backgroundvariability. Still, the preceding fa.tJ* can ieducestatisticalpower and so cloud causalinference. This suggests that in settingsin which *or. of these conditions hold, experimentsshould: (L) uselargesamples to detecteffects; (2) take painsto reducethe influence of ex- traneousvariation either by designor through measurement and statistical manipulation; and (3) studyimplementation quality both as a variableworth study- i"g * its own right in oid.r to ascertain which settingsand providers implement thl interventionbetterand asa mediatorto seehow implementation carriestreat- ment effects to outcome.</region>
          <region class="DoCO:FigureBox" id="Fx378">
            <image class="DoCO:Figure" src="62p6.page_066.image_66.png" thmb="62p6.page_066.image_66-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="page_nr" id="379" page="67" column="1">490</outsider>
          <region class="unknown" id="380" page="67" column="1">|</region>
          <region class="DoCO:TextChunk" id="381" confidence="possible" page="67" column="1">r+.a cRtTtcAL ASSESSMENT OFOUR A5SUMPTIONS</region>
          <region class="DoCO:TextChunk" id="382" confidence="possible" page="67" column="1">Indeed,for many purposes the lack of standardizationmay aid in understanding how effective an interventionwill be undernormal conditionsof implementation. In the social world, few treatmentsare introduced in a standardand theory-faithful way. Local adaptationsand partial implementationare the norm. If this is the case, then someexperiments should reflect this variation and ask whetherthe treatment cancontinueto be effective despite all the variation within groupsthat we would ex- pectto find if the treatmentwerepolicy.Programdeveloperiand socialtheorists may want standardization at high levelsof implementation, but policy analysrs should not welcomethis if it makesthe research conditionsdifferenifro- the practicecondi- tions to which they would like to generalize. Of course, it is most desiiable to be able to answerboth setsof questions-about policy-relevant effects of treatments that are variably implemented and alsoabout the more theory-relevant effects of optimal ex- posureto the intervention.In this regard,one might recall recenteffortsio analyze the effects of the original intent to treat through traditional meansbut alsoof the ef- fectsof the actual treatmentthrough using random assignment as an instrumental variable(Angristet al., 1996a\.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="383" confidence="possible" page="67" column="1">Experiments Entail Tradeoffs Not Worth Making</h2>
          <region class="DoCO:TextChunk" id="384" confidence="possible" page="67" column="1">The choiceto experimentinvolvesa number of tradeoffsthat someresearchers be- lieveare not worth making (Cronbach,7982). Experimenration prioritizeson un- biased answers to descriptive causal questions. But, givenfinite r.rour..r, somere- searchers prefer to investwhat they havenot into marginalimprovements in internal validity but into promoting higher constructand externalvalidity. They might be content with a greaterdegreeof uncertainryabout the quality of a causalconnec- tion in orderto purposively samplea greater range of populations of peopleor set- tings or, when a particular population is central to the research, in ordeito gener- ate a formally representative sample.They might evenusethe resources to improve treatment fidelity or to includemultiplemeasures of averyimportantoutcome con- struct. If a consequence of this preference for constructand ixternal validity is to conducta quasi-experiment or evena nonexperiment rather than a randomizedex- periment, then so be it. Similar preferences make other critics look askance when advocates of experimentationcounselrestrictinga study to volunteersin order to increase the chances of beingable to implementand maintainrandomassignment or when thesesameadvocates advise closemonitoring of the treatmentto ensure its fideliry therebycreatinga situation of greaterobtruiiveness rhan would pertain if the same treatment were part of someongoingsocialpolicy (e.g., Heckman,1992). In the language of Campbelland Stanley (1,963;., theclaim was that ."p.ri*.rrt"- tion traded off externalvalidity in favor of internal validiry. In the parlanceof this book and of Cook and Campbell(1979),it is that experimentatiortrades off both externaland constructvalidity for internal validiry to its detriment. Critics also claim that experiments overemphasize conservative standards of scientificrigor. Theseinclude (1) usinga conservative criterion to protect against</region>
          <region class="DoCO:FigureBox" id="Fx385">
            <image class="DoCO:Figure" src="62p6.page_067.image_67.png" thmb="62p6.page_067.image_67-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="386" page="68" column="1">RANDOMIZED EXPERIMENTS | *tt</outsider>
          <region class="DoCO:TextChunk" id="387" confidence="possible" page="68" column="1">wrongly concludinga treatmentis effectiv e (p &lt;.05) at the risk of failing to de- tect true treatment;ffects;(2) recommending intent-to-treatanalyses that include as part of the treatmentthoseunits that have neverreceived treatment;(3) deni- gr"ting inferences that result from exploring unplanned treatment interactions with characteristics of units, observations, settings, or times;and (4) rigidly pursuing a priori experimentalquestionswhen other interestingquestions emerge duriig " ,t,rdy. Mort laypersons use a more liberal risk calculusto decide about .u,rrul inferences in their own lives,as when they considertaking up some poten- ii"ity lifesaving therapy.Should not science do the same' be lessconservative? Snoula it notlt least-sometimes make different tradeoffs betweenprotection againstincorrectinferences and the failure to detecttrue effects? critics further obiectthat experimepts prioritize descriptive over explanatory causation. The criticsin qrrestion would toleratemore uncertaintyabout whether the interventionworks in order to learn more about any explanatory processes that havethe potentialto generalize across units, settings' observations, and times' Further,,o-. critics pr.f!, to pursuethis explanatory knowledgeusing qualitative meihodssimilar io thor. of th. historian,journalist, and ethnographer than by meansof, sa5 structuralequation modeling that seems much more opaque than the narrativereportsof theseother fields' critics alsodislikethe priority that experiments give to providing policymakers with ofren belated"rrri.r, about what works insteadof providing real-time help to service providersin local settings. These providers are rarely interested in " torrg-a.tayed r,rrnmaryofwhat, ptogt"- has.achieved. They often preferre- ceiving.o.riin,ro.rs feedback about their work and especially about thoseelements oiprJ.ri.. that they can changewithout undue complication' A recent letter to theNew York Timescapturedthis preference:</region>
          <region class="unknown" id="388" page="68" column="1">Alan Krueger . . claims to eschew value iudgments and wants to approach issues (about empirically. Yet his insistenceon postponing changesin education policy until studiesby iesearchers approach certainry is itself a value judgment in favor of the status quo. In view of the tragic state of affairs in parts of public education, his judgment is a most (Petersen,1999)</region>
          <region class="DoCO:TextChunk" id="389" confidence="possible" page="68" column="1">we agreewith many of thesecriticisms. Among all possible _research ques- tions,cau-sal questions constitute only a subset. And of all possible causal meth- ods,experimentation is not relevant io all types of questions and all typesof circumstance. One needonly read the list of options and contingencies outlined in Ch"p,.r, 9 and L0 to appreciate how foolhardy it is to advocate experimentation on a routine basisas a causal "gold standard"that will invariably resultin clearly interpretableeffect sizes.However,many of the criticisms about tradeoffs are basedon artificial dichotomies,correctableproblems,-and even over- simplifications. Experiments can and should examinereasons for variableim- pl.-.nt"tion, and they should searchto uncover mediating processes' They neednot use stringentalpha rates;only statisticaltradition argues for the '05 level.Nor needonJ restric t dataanalyses only to the intent-to-treat'though that</region>
          <region class="DoCO:FigureBox" id="Fx390">
            <image class="DoCO:Figure" src="62p6.page_068.image_68.png" thmb="62p6.page_068.image_68-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="391" page="69" column="1">'aloJ lueururo.rdeJoru qf,ntu e sdeld drrprlerrlpuJetur ql1qlv\ ur surerSord ;o rpuar lurluerelm puorq dpursrrd -Jns eql ol pue qf,ntu oor drlPllu^ Ieurelur aztseqduraap reqr qf,Jeesar;o sururSord ur pa8raureaABr{ stsaSSns drolsrq lpql sassau&gt;lea^\ IertuaraJuragr ol uouuane Bur -llEf, orp arrrtaqrey .(rq8rlrodsaql uI erurl slr a^eq lsntu ad&amp; drlPler d-rela) /lpll -EA lEuJalxa Jo pnJlsuof, JaAodrrprlerrIeuJalur 1o drerurrd eurlnoJ due -ro; 8ur1er lou eJEeM'T lardu{J uI Jealr oPELu a.&amp;\ se 'esJnoJIO 'parseSSns arreq srrlr.rr tsed req.a'\ sPeaJxa dlrear8 sluaut-radxaeldrllnur Ja o senssr drrpllel leuJalxa pug lrnrls -uol qloq sserppeol dlneder aql.slsdleue-Eleruur dlrrap lsoru aeso^4, se 1ng ,sans -sI 'larrr asJl{t dlfsapou qroq Sulssarpp" sanssr,{rrprlerr ur r.lf,Ear Ipuralxo perFrll puB e^Er{ lJnrlsuoJ slueurradxa r{foq sserppE lpnprlrpur ot .paluerg r{f,rBrsar letuaurr.ladxe Io srueJSord;o dlpeder agr qrr^\ pessarduneJEaA\ ,1ser1uoc dg '8ur1uru r{uo^\ tanau aJu lpql stJoapqJfarrnbar sluaurrradxeter{l tsa88nsol luet -.rodur ool sr s{Jo.&amp;\ rpqra 1no Surpurg .sanqod IErJospaseq-sseua^rpa}Ja alouord ol lue1Y\ ol{1v\ sJJels rlaql Pue srolelsr8al asoql ro; d1-rrlnrrued 'cnerualqord eJoru uala P sI uollEluaurtradxa dlqeqo-rdaru sJel\supJeell arntreruard tnoqtr^ q8noqlly saurl 'sploq aturl uorlenlrs Buol-opelep atues qrns aql .re8uep lsorule pue IEar 'uerSor4 ruaurdolartaq IooqJS aqr uuSaq rauoJ sauef arurs sread 0t sl lI .sra/\,s -uB ou PUP sluolutradxa ou aleq all\ Pue 'sloogrs peleJelalf,eue8aq urle-J drua11 eruts sread SI sl fI 'sfteJJeJIeI{l rnoqe sJa^.r{sup Jeolf, ou e^Erl llrls o.&amp;\ puu .pasod -ord SFII arain 'elgBpuedapunsI sJarlf,no^ Iooqtrs lEql uollf,auuoJ erurs srcad lusnpr 0t A ou E tnogp sr lI .splp suorsnlf,uoo .uoryo oor leraua8 IIE ,.raddeq puorg e^Pq Sutmerp ol slBarql ol 'spuno;8 &gt;lsIJ ssaFn ol sI 'saf,uareJur IEIIuePI^ero uolluelJetul da4;o uE lerrSoyuo Jo dtr.rSalur srlaJJear{l elqrsneldurrdl-realf, eql uo Sursrulordtuot sarpnts leluauuadxa arg lnoqlrd\ drrprlerr passoJl Suorls leuralur o4 eq louupf, spunoq aruos 'lurod srql ot rrlaqledtuds d11erauafi are am qfinoqrly .ftget 'qrequo.r3 :og5t ''1" Ir r{luquorJ ''3'a) spoqlau lutuaurradxo ra8uorls aqr Bur -zrsuqdruasrue.rSo-rd tuory ueql serpnls leluaunradxeuou pue Iuluaur-radxa_isenb ;o dlerrlua uela ro dlrsoru lslsuof, rer{l qf,reasarJo sure-r8ord tuory peuJuel aq IIri\,\</region>
          <region class="unknown" id="393" page="69" column="1">uo upluroJul InJasn aroru r'rrr pourr Et'*:u"';rrx;;<email id="392">H:;</email>r:::ilil? ilHt", ",</region>
          <region class="DoCO:TextChunk" id="394" confidence="possible" page="69" column="1">sluerue^ordur leur8rulu JeuIJ-JaAa to 1uo3aqr pur 'lsa88ns stxel eruos su plSrr se eq tou paau sluaur.radxg '(salqerrul Surlelpau Jo sarnseau Burppe,.8.a)tuaqr;o ^{et salulleluoslnq 'saJJnosal artnber sarnparo-rd asaql ilV'{ooq srql ur peurllno spoqrau eqt Sursn pelpreua8aq plnoqs uortuzrlereua8 lesneolnoqp alqrssodse uolletuJotul qlntu sB puv 'sasseoordSurlelpau pue sauof,lno pepuelurun Burre -^oJsrp tE parurp uorllellor etvp a^nelrlenb aq plnor{s pue upJ aleql .saruof,lno pue 'stuerulearl 's8urpas (suos.rad ;o sluerussasse ;o dfrpryel lf,nJlsuof, ar{r puu salduresJo sseualrleluasardar er.lrJo sasdyeue lulueurr.radxeuou eg osle plnoqs Pue uBr erarll 'paqsqqnd aq uE3 sluaurrradxs tuoJJ sllnsoJ urrelul .dlsnorl -nBf, suolsnlf,uol rraqf 3urqf,nof, PuE seleJ JoJJa ale8rgo.ld lsure8e Surpren8 hlo11u remod lptrrtsrlels pue droagl elrtuetsqns leql luatxe eqr ol suorlrrJal -uI IEf,Itsllels aroldxa osle uet sJatuJurrradxg 'srsdleur auo oq dlorruryap pFor{s</region>
          <region class="unknown" id="395" page="69" column="1">sNoll_dwnssv uno lo l_Nty\sslssv tv)tl|u) v .tt I zov</region>
          <outsider class="DoCO:TextBox" type="footer" id="396" page="69" column="1">I</outsider>
          <region class="DoCO:FigureBox" id="Fx397">
            <image class="DoCO:Figure" src="62p6.page_069.image_69.png" thmb="62p6.page_069.image_69-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="398" page="70" column="1">RANDOMIZED EXPERIMENTS | 493 I</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="399" confidence="possible" page="70" column="1">Experiments Assume an InvalidModel of Research Utilization</h2>
          <region class="DoCO:TextChunk" id="400" confidence="possible" page="70" column="1">To somecritics, experiments recreatea naive rational choicemodel of decision making. That is, one first lays out the alternatives to choose among (the treat- *.rr,rt] then one decides on criteria of merit (the outcomes); then one collectsin- formation on eachcriterion for eachtreatment(the data collection), and finally one makes a decisionabout the superior alternative.UnfortunatelS empirical work on the useof socialscience daia showsthat useis not so simpleas the rational choice model suggests (c. \ufeiss 6c Bucuvalas, 1980; c''weiss, 1988)' First, evenwhen."-rir. and effectquestions are askedin decision contexts'ex- p.ri-.nt"l resultsare still usedalong with other forms of information-from existing theories,personaltestimony,extrapolationsfrom surveys' consensus of a fieldlchims from experts with interests to defend,and ideas that haverecentlybe- .o*. trendy.Decisions are shapedpartly by ideology,interests, politics' person- ality, windows of-opportunity, and ualues;and they are as much made by a policy-shapirrg.o-*nrrity (cronbachet al., 1980) as by an individualor com- *i,,... Fuither,manydecisions are not so much madeasaccreted overtime asear- lier decision,.orrrir"in later ones,leavingthe final decision maker with few options ('Weiss, 1980). Indeed,by the time ixperimental results are available,new decisionmakersand issues may havereplaced old ones. Second,.*p.rirn.nts often yield contestedrather than unanimous verdicts that therefore have uncertain implications for decisions. Disputes arise about whether the causalquestionswere correctly framed, whether resultsare valid' whetherrelevantoutcomes were assessed, and whetherthe results entail a specific decision.For example,reexaminations of the Milwaukee educational voucher ;"rdy offereddifferentconclusions about whetherand whereeffects occurred (H' Fuller, 2000; Greene, Peterson, 6c Du, 1.999;'Sritte, 1'998,"1'999,2000)' SimilarlS differenteffect,ir., *.r. generated from the Tennessee class size experiment (Finn EcAchilles ,1.990;Hanusi'ek,1999;Mosteller, Light, 6c Sachs, 1996)' Sometimes, scholarlydisagreements are at issue,but at other timesthe disputes reflectdeeply conflictedstakeholder interests. Third, short-terminstrumentaluseof experimental data is more likely when the interventionis a minor variant on existingpractice.For example, it is easier to change textbooksin a classroom or pills givenlo patientsor eligibility criteriafor ;;;;"* day-care entry centersfor than it welfare is to relocate recipientsthroughout hospitalsto.underserved an entire locationsor state' Because to open the more feasible .tt""g.t are so ,ood.r, in scope, they are lesslikely to dramatically affecttheproble- ih.y address. So critics note that prioritizing on shor-t-term instrumental change tendsto preserve most of the statusquo and is unlikely to solve tr.rr.hunt social"probl.-s. bf course'thereare someexperiments that truly twist the lion,stail andinvolvebold initiatives.Thus moving families from densely poor inner-citylocationsto the suburbsinvolveda changeof three standard deviations</region>
          <region class="DoCO:FigureBox" id="Fx401">
            <image class="DoCO:Figure" src="62p6.page_070.image_70.png" thmb="62p6.page_070.image_70-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="402" confidence="possible" page="71" column="1">494 14.A CRIT|CAL ASSESSMENT OFOUR ASSUMPTTONS |</region>
          <region class="DoCO:TextChunk" id="403" page="71" column="1">in the poverty level of the sending and receiving communities, much greater than what happens when poor families spontaneously move.'S7hethersuch a dramatic change could ever be used as a model for cleaning out the inner cities of those who want to move is a moot issue. Many would judge such a policy to be unlikely. Truly bold experiments have many important rationales; but creating new policies that look like the treatment soon after the experiment is not one of them. Fourth, the most frequent use of research may be conceptual rather than instrumental, changing how users think about basic assumptions,how they understand contexts, and how they organize'or label ideas. Some conceptual uses are intentional, as when a person deliberately reads a book on a current problem; for example, Murray's (1984) book on social policy had such a conceptual impact in the 1980s, creating a new social policy agenda. But other conceptual usesoccur in passing, as when a person reads a newspaper story referring to social research. Such usescan have great long-run impact as new ways of thinking move through the system, but they rarely change particular short-term decisions. These arguments against a naive rational decision-making model of experimental usefulnessare compelling. That model is rightly rejected. However, mosr of the objections are true not just of experiments but of all social sciencemethods. Consider controversies over the accuracy of the U.S. Census,the entirely descriptive results of which enter into a decision-making process about the apportion- ment of resourcesthat is complex and highly politically charged. No method offers a direct road to short-term instrumental use. Moreover, the obiections are exaggerated.In settings such as the U.S. Congress,decision making is sometimes influenced instrumentally by social scienceinformation (Chelimsky, 1998), and experiments frequently contribute to that use as part of a researchreview on ef- fectivenessquestions. Similarlg policy initiatives get recycled, as happened with school vouchers, so that social science data that were not used in past years are used later when they become instrumentally relevant to a current issue (Polsby, 1'984; Quirk, 1986).In addition, data about effectiveness influence many stakeholders' thinking even when they do not use the information quickly or instrumentally. Indeed, researchsuggests that high-quality experiments can confer exrra 'Weiss credibility among policymakers and decision makers (C. &amp; Bucuvalas, 1980)' as happened with the Tennessee class size study. We should also not forget that the conceptual use of experiments occurs when the texts used to train pro- fessionalsin a given field contain results of past studies about successfulpractice (Leviton 6c Cook, 1983). And using social sciencedata to produce incremental change is not always trivial. Small changescan yield benefits of hundreds of mil- lions of dollars (Fienberg,Singer,&amp; Tanur, 1985). SociologistCarol'Weiss, an advocate of doing research for enlightenment's sake, says that 3 decadesof experience and her studies of the use of social sciencedata leave her "impressed with the utility of evaluation findings in stimulating incremental increasesin knowledge and in program effectiveness. Over time, cumulative incrementsare not such small potatoes after all" ('Weiss, 1998, p. 31,9). Finallg the usefulness of experimentscan be increased by the actions outlined earlier in this chapter that involve comple-</region>
          <region class="DoCO:FigureBox" id="Fx404">
            <image class="DoCO:Figure" src="62p6.page_071.image_71.png" thmb="62p6.page_071.image_71-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="405" page="72" column="1">RANDOMIZED EXPERIMENTS I ott</outsider>
          <region class="DoCO:TextChunk" id="406" confidence="possible" page="72" column="1">mentingbasicexperimental design with adjunctssuchas measures of implementation a-nd mediationo, qualitativemethods-anything that will help clarify program process and implementation problems.In summarSinvalid modelsof the ir.foln.rs of experimintalresultsseem to us to be no more nor lesscommon 'we than invalid modelslf th. use of any other social science methods. have learned much in the last severaldecades about use, and experimenters who want their work to be usefulcan take advantages of thoselessons (Shadish et al., 1'99I).</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="407" confidence="possible" page="72" column="1">TheConditions of Experimentation Differ from the Conditions of Policy lmplementation</h2>
          <region class="DoCO:TextChunk" id="408" confidence="possible" page="72" column="1">Experiments are often doneon a smalleiscalethan would pertain if services were i-il.-r.rted state-or nationwide,and so they cannot mimic all the details rele- u"rr, ,o full policy implementation. Hence policy implementationof an interven- ,i"" -ry yi.ta aiff.rint o,rt.omesthan the experiment(Elmore, 1996)' For example, t"r.d partly on researchabout the benefits of reducing class size, Tennessee and Caliiornia implementedstatewidepoliciesto have more classes with fewer studentsin each.This required many new teachersand new class- rooms.However,because of a nationalteacher shortage, some of thosenew teachers may havebeenlessqualifiedthan those in the experiment;and a shortageof classrooms led to more .rs. of trailers and dilapidatedbuildings that may have harmedeffectiveness further. Sometimes an experimental treatmentis an innovation that generates enthu- siasticeffortsto implementit well. This is particularly frequentwhen the experiment is done by a charismatic innovator whosetacit knowledgemay exceed that of thosewho would be expected to implementthe program in ordinary pr^ctrce and whosecharismamay inducehigh-qualityimplementation. Thesefactorsmay generate more srr...srfoi outcomes than will be seenwhen the intervention is implemented as routine PolicY. Policy implementationmay also yield different-resultswhen experimental treatments are implemented in a fashionthat differs from or conflictswith prac- ticesin real-*orld application.For example, experiments studying psychotherapy outcomeoften standardize treatmentwiih a manual and sometimes observe and correct the therapistfor deviatingfrom the manual (shadishet al., 2000); but thesepractices are rare in clinicallractice. If manualized treatmentis more effective (bhambless&amp; Hollon, 1998; Kendall, 1998), experimentalresults might transferpoorly to practicesettings. Raniom assigrrm.nt may also changethe program from the intended policy implementation (i{eckman,l992l. For ixample, thosewilling to be randomized -"y diff.r from those for whom the treatment is intended; randomizatLon may changepeople'spsychologicalor social responseto treatment compared with those"wlroself-select treatment;and randomizationmay disrupt administration and implemenration by forcingthe programto copewith a differentmix of clients'</region>
          <region class="DoCO:FigureBox" id="Fx409">
            <image class="DoCO:Figure" src="62p6.page_072.image_72.png" thmb="62p6.page_072.image_72-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="410" page="73" column="1">I</outsider>
          <region class="unknown" id="411" page="73" column="1">496 14.A CR|T|CAL ASSESSMENT OF OURASSUMPTIONS |</region>
          <region class="DoCO:TextChunk" id="412" confidence="possible" page="73" column="1">Heckman 0TPA) evaluation claims this "calls kind into of questionthe problem with validity the Job of the taining experimental PartnershipAct estimates as a statement about theJTPAsystem as a whole" (Heckman,1.992, p. ZZ1,). In many respects, we agree with these criticisms, though it is worth noting sev- eral responses to them. First, theyassumealack of generalizabllity from experi- ment to policy but that is an empirical question. Some data suggesr thar general- ization may be high despite differencesbetweenlab and field (C. Anderson, LindsaS &amp; Bushman, 1999) or betweenresearchand practice (Shadish et al., 2000). Second, it can help to implement.treatment under conditionsthat aremore characteristic of practiceif it doesnot unduly compromise other research priori- ties. A little forethoughtcan improve the surfacesimilarity of units, trearments, observations, settings, or timesto their intendedtargets. Third, someof these crit- icismsare true of any research methodologyconducted in a limited context,such as locally conductedcasestudiesor quasi-experiments, because local implemen- tation issues always differ from large-scale issues. Fourth, the potentiallydisrup- tive natureof experimentally manipulatedinterventions is sharedby many 'rrr"or"h locally invented novel programs, euen uhen they are not studied by any methodology at all.Innovation inherentlydisrupts,and substantive literatures are rife with examplesof innovationsthat encountered policy implementationim- pediments(Shadish, 1984). However,the essential problem remainsthat large-scale policy implementation is a singularevent,the effectsof which cannot be fully known exceptby do- ing the full implementation. A singleexperiment, or evena smallseries of ri-ilrt ones,cannotprovidecompleteanswers about what will happenif the intervention is adoptedas policy. However,Heckman'scriticism needsreframing.He fails to distinguishamongvalidity types(statistical conclusion, internal,.onrtro.., exter- nal). Doing so makesit clearthat his claim that suchcriticism"calls into question the validity of the experimental estimates as a sratement about the JTPA,yrt.rr, ", a whole" (Heckman,1.992, p.221,) is reallyabout external validityand construcr validity,not statistical conclusion or internalvalidity.Exceptin thenarrow econo- metrics traditionthat he understandably cites (Haavelmo, 7944;Marschak ,7953; Tinbergen,1956),few socialexperimenters ever claimedthat experiments could describe the "systemas a whole"-even Fisher(1935)acknowledged this trade- off. Further,the econometric solutionsthat Heckman suggests cannot avoid the sametradeoffsbetweeninternal and externalvalidity. For example,surveys and certain quasi-experiments can avoid someproblemsby observingexistinginter- ventionsthat have aheadybeenwidely implemented, but the validity of tleir es- timatesof program effectsare suspect and may themselves change if the program were imposedevenmore widely as policy. Addressing these criticismsrequiresmultiple lines of evidence-randomized experimentsof efficacyand effectiveness, nonrandomizedexperiments that ob- serveexistinginterventions, nonexperimental surveys to yield estimates of repre- sentativeness, statisticalanalyses that bracketeffectsunder diverseassumpd;ns,</region>
          <region class="DoCO:FigureBox" id="Fx413">
            <image class="DoCO:Figure" src="62p6.page_073.image_73.png" thmb="62p6.page_073.image_73-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="footer" id="414" page="73" column="1">J</outsider>
          <outsider class="DoCO:TextBox" type="header" id="415" page="74" column="1">RANDOMIZED EXPERIMENTS I Ot I</outsider>
          <region class="DoCO:TextChunk" id="416" confidence="possible" page="74" column="1">qualitative observation to discover potential incompatibilities between the inter- ventiol and its context of likely implementation, historical study of the fates of similar interventions when they were implemented as policg policy analysesby those with expertisein the type of intervention at issue,and the methods for causal generalizationin this book. The conditions of policy implementation will be dif- i.r.rr, from the conditions characteristic of any rese^rchstudy of it, so predicting generalizationto policy will always be one of the toughest problems.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="417" confidence="possible" page="74" column="1">lmposing Treatments ls Fundamentally Flawed Compared with Encouraging the Growthof Local Solutions to Problems</h2>
          <region class="unknown" id="418" page="74" column="1">Experiments impose treatments on recipients. Yet som,e late 20th-centurythought ,.rjg.rt, that be inferior to solutionsthat are locally gen- .rJr".a by thoseiho h"n. the problem. Partly,this view is premisedon research findings of few effectsfor the Great the 1960sin the UniteJ States (Murrag 1.984; Rossi, L987),with the presumptionthat a portion of the failurewas due to the the programs.Partly,the view reflectsthe success of late 2Oth-century free market economics and conservative political ideologiescompared with centrally controlled economiesand more fi|eral political beliefs. Experimentally imposedtreatments are seen in some quartersas beinginconsistent with suchthinking' IronicallS the first objectionis basedon resultsof experiments-if it is true that impos.i progr"*s do not work, experiments provided the evidence. More- over,these tro-.ff..t findingsmay havebeenpartly due to methodological failures of experiments as they were implemented at that time. Much progress in solving practicalexperimental partly in response to, those experiments. If so,it is premature to assume these experiments definitivelydemon- stiated no effect,especlaly given our increased ability to detectsmall effectsto- day ' (D. Greenberg 6c shroder, 1,997; !993). We must alsoiistinguish between political-economic currencyand the effects of interventions. 'We know of no comparisons of, say,the effectsof locally gener- atedversus imposed solutions.Indeed, the methodological problemsin doing such comparisons are daunting, interventionsinto the two categories and unlonfounding the categories with ferences. Bariing an unexpected solutionto the seemingly causalinference in nonrandomized designs, answering questions about the effects of locally generated solutionsmay kind of high-qualityexper- imentatioi being it is likely that locally generated solutions may it also is likely that someof thosesolu- tions will haveto be experimentally evaluated.</region>
          <region class="DoCO:FigureBox" id="Fx419">
            <image class="DoCO:Figure" src="62p6.page_074.image_74.png" thmb="62p6.page_074.image_74-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="420" page="75" column="1">I</outsider>
          <region class="unknown" id="421" page="75" column="1">498 | 14.A CRIT|CAL ASSESSMENT OF OURASSUMPTTONS</region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="422" page="75" column="1">CAUSALGENERALIZATION: AN OVERLY COMPLICATED THEORY?</h1>
        <region class="DoCO:TextChunk" id="431" page="75" column="1">Internal validity is best promoted via random assignment,an omnibus mechanism that ensuresthat we do not have many assumptions to worry about when causal in- ferenceis our goal. By contrast, quasi-experiments require us to make explicit many assumptions-the threats to internal validity-that we then have to rule out by fiat, by design,or by measurement. The latter is a more complex and assumption-riddled processthat is clearly inferior to random assignment.Something similar holds for causal generalization,in which random selectionis the most parsimonious and theoretically justified method, requiring the fewest assumptionswhen causalgeneralization is our goal. But becauserandom selectionis so rarely feasible,one instead has to construct an acceptabletheory of generaliz tion out of purposive sampling, 'We a much more difficult process. have tried to do this with our five principles of generalizedcausal inference.These, we contend, are the keys to generalizedinfer- ence that lie behind random sampling and that have to be identified, explicated, and ano assessed assessed rt if we are to make make better general inferences, rnterences, even rt if they are not perfect ones. But these principles are much more complex to implement than is random sampling. Let us briefly illustrate this with the category called American adult women. We could represent this category by random selection from a critically appraised register of all women who live in the United Statesand who arc at least 21 years of age.I7ithin the limits of sampling error, we could formally generalizeany characteristics we measured on this sample to the population on that register. Of course, we cannot selectthis way becauseno such register exists.Instead,one does onet experiment with an opportunistic sample of women. On inspection they all '1,9 turn out to be between and 30 years of age, to be higher than average in achievementand abilit5 and to be attending school-that is, we have useda group of college women. Surface similarity suggests that each is an instance of the category woman. But it is obvious that the modal American woman is clearly not a college student. Such students constitute an overly homogeneoussample with respect to educational abilities and achievement,socioeconomicstatus, occupation, and all observable and unobservable correlates thereof, including health status, current employment, and educational and occupational aspirations and expectations. To remedy this bias, we could use a more complex purposive sampling design that selectswomen heterogeneouslyon all these characteristics.But purposive sampling for heterogeneousinstances can never do this as well as random selection can, and it is certainly more complex to conceive and execute.I7e could go on and illustrate how the other principles faclhtate generalization. The point is that any theory of generalization from purposive samples is bound to be more complicated than the simplicity of random selection. But becauserandom selection is rarely possible when testing causal relationships within an experimental framework, we need these purposive alternatives. <marker type="page" number="76"/><marker type="block"/> Yet most experimental work probably still relies on the weakest of these alternatives, surfaci similarity.'We seek to improve on such uncritical practice. Unfortu- nately though, there is often restricted freedom for the more careful selection of instancesof units, treatments, outcomes, and settings, even when the selection is done purposively.It requires resourcesto sample irrelevanciesso that they are het- erogeneouson many attributes, to measure several related constructs that can be discriminated from each other conceptually and to measure a variety of possible explanatory processes. This is partly why we expect more progress on causal generalization from a review context rather than from single studies. Thus, if one researcher can work with college women, another can work with female school- teachers, and another with female retirees, this creates an opportunity to see if thesesourcesof irrelevant homogeneity make a difference to a causal relationship or whether it holds over all these differ6nt types of women. UltimatelS causal generalizationwill always be more complicated than assessing the likelihood that a relationship is causal.The theory is more diffuse, more recent, and lesswell testedin the crucible of researchexperience.And in some quar- ters there is disdain for the issue,given the belief and practice that relationshipsthat replicate once should be consideredas generaluntil proven otherwise' not to speak oithe belief that little progressand prestigecan be achieved by designingthe next experiment to be some minor variant on past studies. There is no point in pre- t.nding that causal generalization is as institutionalized procedurally as other methods in the social sciences.'We have tried to set the theoretical agendain a systematic way. But we do not expect to have the last word. There is still no explica- tion of causal generalizationequivalent to the empirically produced list of threats to internal validiry and the quasi-experimental designsthat have evolved over 40 years to rule out thesethreats. The agendais set but not complete.<marker type="page" number="77"/><marker type="block"/> causal studies basedon causal modeling practices.Across the social sciences other than economics and statistics, the word quasi-experiment is routinely used to justify causal inferences,even though designsso referred to are so primitive in structure that 'We causal conclusions are often problematic. have to challenge such advoc acy of low-grade quasi-experiments as a valid alternative to the quality of studies we have been calling for in this book. And finally in parts of statistics and epidemiology, and overwhelmingly in econometrics and those parts of sociology and political science that draw from econometrics,the emphasisis more on control through statistical manipulation than on experimental design.I7hen descriptive causal inferencesare the primary concern, all of these alternatives will usually be inferior to experiments.</region>
        <region class="DoCO:FigureBox" id="Fx424">
          <image class="DoCO:Figure" src="62p6.page_075.image_75.png" thmb="62p6.page_075.image_75-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="425" page="76" column="1">NONEXPERIMENTALALTERNATIVES I 499</outsider>
        <region class="unknown" id="427" page="76" column="1">NONEXPE RI M ENTAL ALTE RNATIVES Though this book is about experimental methodsfor answeringquestions about .".rr"l hypotheses, it is a mistaketo believe that only experimental approaches are used for thir p,r.pose.In the following; we briefly consider severalother approaches, indiiating the major reasons why we havenot dwelt on them in detail. basicallSthe reasonis that we believethat, whatevertheir merits for somere- search purposes, they generate less clearcausalconclusions than randomized ex- perimentsor eventhe bestquasi-experiments such as regression-discontinuity or interruptedtime series The nonexperimental alternatives we examineare the major onesto emerge in variousacademic disciplines. In educationand parts of anthropologyand soci- ologg one alternative is intensive qualitativecase studies. In these same fields,and also-in developmental psychologythere is an theory-based</region>
        <region class="DoCO:FigureBox" id="Fx428">
          <image class="DoCO:Figure" src="62p6.page_076.image_76.png" thmb="62p6.page_076.image_76-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="header" id="429" page="77" column="1">500 14.A CR|T|CAL ASSESSMENT OFOUR ASSUMPTTONS</outsider>
        <region class="unknown" id="430" page="77" column="1">|</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="432" confidence="possible" page="77" column="1">Intensive Qualitative Case Studies</h2>
          <region class="DoCO:TextChunk" id="437" page="77" column="1">The call to generate causal conclusions from intensive case studies comes from several sources. One is from quantitative researchersin education who became disenchanted with the tools of their trade and subsequently came to prefer the qualitative methods of the historian and journalist and especiallyof the ethnographer (e.g.,Guba,198l, 1,990;and more tentatively Cronbach, 1986).Another is from those researchersoriginally trained in primary disciplines such as qualitative anthropology (e.g.,Fetterman, 19841or sociology (Patton, 1980). The enthusiasm for case study methods arises for several different reasons. One is that qualitative methods often reduce enough uncertainty about causation to meet stakeholderneeds.Most advocatespoint out that journalists,historians, ethnographers, and lay persons regularly make valid causal inferences using a qualitative processthat combines reasoning, observation, and falsificationist procedures in order to rule out threats to internal validity-even if that kind of language is not explicitly used (e.g.,Becker,1958; Cronbach,1982). A small minority of qualitative theorists go even further to claim that casestudiescan routinely replace experiments for nearly any causal-sounding question they can conceive (e.g.,Lincoln &amp; Guba, 1985). A secondreasonis the belief that suchmethodscan also engagea broad view of causation that permits getting at the many forces in the world and human minds that together influence behavior in much more complex ways than any experiment will uncover.And the third reasonis the belief that case studies are broader than experiments in the types of information they yield. For example, they can inform readers about such useful and diverse matters as how pertinent problems were formulated by stakeholders, what the substantive theories of the intervention are, how well implemented the intervention components were, what distal, as well as proximal, effects have come about in respondents' lives, what unanticipated side effects there have been, and what processes explain the pattern of obtained results.The claim is that intensivecasestudy methods allow probes of an A to B connection, of a broad range of factors condition- ing this relationship, and of a range of intervention-relevant questions that is broader than the experiment allows. <marker type="page" number="78"/><marker type="block"/> Although we agree that qualitative evidence can reduce some uncertainfy about cause-sometimes substantially the conditions under which this occurs are usually rare (Campbell, 1975).In particular, qualitative methods usually produce unclear knowledge about the counterfactual of greatest importance, how those who receivedtreatment would have changedwithout treatment. Adding design featuresto casestudies,such as comparison groups and pretreatmentobser- vations, clearly improves causal inference. But it does so by melding case-study data collection methods with experimental design.Although we consider this as a valuable addition ro ways of thinking about casestudies, many advocatesof the method would no longer recognize it as still being a case study. To our way of thinking, casestudies are very relevant when causation is at most a minor issue; but in most other cases when substantial uncertainry reduction about causation is required, we value qualitative methods within experiments rather than as alternatives to them, in ways similar to those we outlined in Chapter 12.</region>
          <outsider class="DoCO:TextBox" type="footer" id="434" page="77" column="1">I .J</outsider>
          <region class="DoCO:FigureBox" id="Fx435">
            <image class="DoCO:Figure" src="62p6.page_077.image_77.png" thmb="62p6.page_077.image_77-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="436" page="78" column="1">NONEXPERIMENTALALTERNATIVES | 501 I</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="438" confidence="possible" page="78" column="1">Theory-Based Eva luations</h2>
          <region class="DoCO:TextChunk" id="439" page="78" column="1">This approach has beenformulated relatively recently and is describedin various books or specialjournal issues(Chen &amp; Rossi, 1,992;Connell, Kubisch, Schorr,&amp; 'Weiss, 1.995;Rogers,Hacsi, Petrosino,&amp; Huebner, 2000). Its origins are in path analysis and causal modeling traditions that are much older. Although advocates have some differenceswith each other, basically they all contend that it is useful: (1) to explicate the theory of a treatment by detailing the expected relationships among inputs, mediating pfocesses,and short- and long-term outcomes; (2) to measure all the constructs specified in the theory; and (3) to analyzethe data to assessthe extent to which the postulated relationships actually occurred. For shorter time periods, the available data may addressonly the first part of a postulated causal chain; but over longer periods the complete model could be involved. Thus, the priority is on highly specific substantive theorS high-quality measurement,and valid analysisof multivariate explanatory processes as they un- fold in time (Chen &amp; Rossi, 1'987,1,992). Such theoretical exploration is important. It can clarify general issueswith treatments of a particular type, suggestspecific researchquestions,describehow the intervention functions, spell out mediating processes, locate opportunities to remedy implementation failures, and provide lively anecdotes for reporting results ('Weiss, 1'998). All th.r. serveto increasethe knowledge yield, evenwhen such theoretical analysisis done within an experimental framework. There is nothing about the approach that makes it an alternative to experiments. It can clearly be a very important adjunct to such studies,and in this role we heartily endorsethe approach (Cook,2000). However, some authors (e.g., Chen 6c Rossi, 1,987, 1992; Connell et al., 1,99 5l have advocated theory-based evaluation as an attractive alternative to experiments when it comes to testing causal hypotheses.It is attractive for several i.urorrr. First, it requires only a treatment group' not a comparison group whose</region>
          <region class="DoCO:FigureBox" id="Fx440">
            <image class="DoCO:Figure" src="62p6.page_078.image_78.png" thmb="62p6.page_078.image_78-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="441" confidence="possible" page="79" column="1">502 | 14.A CRTT|CAL ASSESSMENT OFOUR ASSUMPTTONS</region>
          <region class="DoCO:TextChunk" id="442" page="79" column="1">agreement to be in the study might be problematic and whose participation in- creasesresearchcosts. Second, demonstrating a match between theory and data suggeststhe validity of the causal theory without having to go through a labori- ous processof explicitly considering alternative explanations. Third, it is often im- practical to measure distant end points in a presumed causal chain. So confirmation of attaining proximal end points through theory-specified processes can be used in the interim to inform program staff about effectiveness to date, to argue for more program resourcesif the program seemsto be on theoretical track, to justify claims that the program might be effective in the future on the as-yet-not- assessed distant criteria, and to defend against premature summative evaluations that claim that an intervention is ineffective before it has been demonstrated that the processes necessaryfor the effect have actually occurred. However, maior problems exist with this approach for high-quality descriptive causalinference(Cook, 2000). First, our experience in writing about the theory of a program with its developer (Anson et al., 1,991)has shown that the theory is not always clear and could be clarified in diverse ways. Second, many theories are linear in their flow, omitting reciprocal feedback or external contingenciesthat might moderate the entire flow. Third, few theories specify how long it takes for a given processto affect an indicator, making it unclear if null results disconfirm a link or suggestthat the next step did not yet occur. Fourth, failure to corroborate a model could stem from partially invalid measuresas opposedto in- validity of the theory. Fifth, many different models can fit a data set (Glymour et a1.,1987;Stelzl, 1986), so our confidencein any given model may be small. Such problems are often fatal to an approach that relies on theory to make strong causal claims. Though some of theseproblems are present in experiments (e.g.,failure to incorporate reciprocal causation, poor measures),they are of far less import because experiments do not require a well-specified theory in constructing causal knowledge. Experimental causal knowledge is less ambitious than theory-based knowledge, but the more limited ambition is attainable.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="443" confidence="possible" page="79" column="1">Weaker Quasi-Experi ments</h2>
          <region class="DoCO:TextChunk" id="444" page="79" column="1">For some researchers,random assignment is undesirable for practical or ethical reasons, so they prefer quasi-experiments. Clearly, we support thoughtful use of quasi-experimentation to study descriptive causal questions. Both interrupted time series and regression discontinuity often yield excellent effect estimates. Slightly weaker quasi-experiments can also yield defensible estimates,especially when they involve control groups with careful matching on stable pretest attributes combined with other design features that have been thoughtfully chosen to addresscontextually plausible threats to validity. However, when a researchercan choose, randomized designsare usually superior to nonrandomized designs. This is especially true of nonrandomized designs in which little thought is given to such matters as the quality of the match when creating control groups,</region>
          <outsider class="DoCO:TextBox" type="footer" id="445" page="79" column="1">j</outsider>
          <region class="DoCO:FigureBox" id="Fx446">
            <image class="DoCO:Figure" src="62p6.page_079.image_79.png" thmb="62p6.page_079.image_79-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="header" id="447" page="80" column="1">NONEXPERIMENTALALTERNATIVES I tOl</outsider>
          <region class="DoCO:TextChunk" id="448" confidence="possible" page="80" column="1">includingmultiple hypothesis tests rather than a singleone' generating data from several pr.tr."t*.nt time points rather than one, or having severalcomparison groupsto createcontrolsthat bracketperformancein the treatmentgroups.In- I..d, when resultsfrom typical quasi-experiments are compared with thosefrom randomizedexperimentson the same topic, several findings emerge.Quasi- experiments frequentlymisestimate effects (Heinsman&amp; Shadish,1'996;Shadish &amp; Ragsdale, t9961. Tirese biases are often large and plausiblydue to selection biases srrch as the self-selection of more distressed clients into psychotherapy treatment conditions(Shadish et al., 2000) or of patientswith a poorer prognosis into controlsin medicalexperiments (Kunz &amp; Oxman,1'9981. Thesebiases are especially prevalent in quasi-experiments that usepoor quality control groupsand have higheiattrition(Heinsmar$c Shadish,'1,996;Shadish 6cRagsdale,l996l. So, if the an"swers obtainedfrom randomized experiments are morecredible than thosefrom quasi-experiments on theoretical groundsand are more accurate empirically, then ,'h. ".g,.r-entsfor randomized experiments are evenstrongerwhenever a high de- gr.. oI uncertainty reductionis requiredabout a descriptive causal claim. Because all quasi-experiments are not equal in their ability to reduceuncer- tainty about."ur., *. -"ttt to draw attention againto a common but unfortu- natepractice in manysocialsciences-tosaythat a quasi-experiment is beingdone in order to provide justificationthat the resultinginference will be valid. Then a quasi-experimental design is described that is so deficientin the desirable structural features noted previously, which promote better inference, that it is probably not worth doing. Indeed,over the yearswe have__repeatedly noted the term quasi-experiment biing usedto justify designs that fell into the classthat Campbell and'stanley (196i) labeled as uninterpretable and that Cook and Campbell (1,9791labeled'as generally uninterpretable. These are the simplest forms of the designs discussed in Chapters 4 and 5. Quasi-experiments cannot be an alternative to randomizedexperiments when the latter are feasible, and poor quasi-experiments can neverbi a substitute for stronger quasi-experiments when_the lat- i., "r. also feasible. Just as Gueron (L999) has remindedus about randomized experiments, good quasi-experiments have to be fought for, too. They are rarely handedout as though on a silverplate.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="449" confidence="possible" page="80" column="1">Statistical Controls</h2>
          <region class="DoCO:TextChunk" id="450" confidence="possible" page="80" column="1">In this book,we haveadvocated that statistical adjustments for groupnonequivalence are best urrd oBt design controlshavealreadybeenusedto the maximum in order to reducenonequivalence to a minimum. So we are not opponents of statisticalad- justmenttechniques such asthoseadvocated by the statisticians and econometricians described in the appendixto Chapter5. Ratheqwe want to usethem as the last re- sort.The positionwe do not like is the assumption that statistical controlsare sowell developeithat they can be usedto obtain confidentresultsin nonexperimental and weak iuasi-e*perimental contexts.As we saw in Chapter 5, research in the past 2</region>
          <region class="DoCO:FigureBox" id="Fx451">
            <image class="DoCO:Figure" src="62p6.page_080.image_80.png" thmb="62p6.page_080.image_80-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="452" confidence="possible" page="81" column="1">504 | ta. a cRtTtcAL AsSEssMENT OF OUR ASSUMPT|ONS</region>
          <region class="unknown" id="453" page="81" column="1">I</region>
          <region class="DoCO:TextChunk" id="454" confidence="possible" page="81" column="1">decadeshas not much supported the notion that a control group can be constructed</region>
          <region class="DoCO:TextChunk" id="455" confidence="possible" page="81" column="1">pretests collected sorrccf,e(J on on the rne same same measures measures tne the posttest, posttest, On on the tne</region>
          <region class="unknown" id="456" page="81" column="1">through matchingfrom somenational or state registrywhen the treatmentgroup comes from a morecircumscribed and localsetting. Nor hasresearch much supported the useof statistical adjustments in longitudinalnationalsurveys in which individuals with differentexperiences are explicitly contrasted in order to estimate the effects of this experience difference. Undermatching is a chronic problem here,as are consequences of unreliabilityin the selection variables, not to speakof specification errors dueto incomplete knowledge 'We of the selection process. In particular, endogeneity prob- lemsarea realconcern. areheartened that more recentwork on statistical adjust- mentsseems to be moving toward the position we represent, with greateremphasis beingplacedon internal controls,on stablematchingwithin suchinternalcontrols, on the desirability of seeking cohort controlsthroughthe useof siblings, on the useof PrstssLs aS Uulrty utiliw of Ot such SUChpretest measures collected at several different times, and on the desirability of studying inter- 'We ventions that areclearlyexogenous shocks to someongoingsystem. arealsoheart- enedby the progress being madein the statistical domainbecause it includes progress on design considerations, as well ason analysis per se(e.g., Rosenbaum, 1999a).Ve areagnostic at this time asto the virtuesof the propensity score andinstrumental variable approaches that predominatein discussions of statisticaladiustmenr. Time will tell</region>
          <region class="DoCO:TextChunk" id="457" page="81" column="1">tell how well well they they pan out relative to the results from randomizedexperiments.'We have surely not heard the last word on this topic.</region>
        </section>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="458" page="81" column="1">CONCLUSION</h1>
        <region class="DoCO:TextChunk" id="459" page="81" column="1">'We cannot point to one new development that has revolutionized field experimentation in the past few decades,yet we have seena very large number of incremental improvements. As a whole, these improvements allow us to create far better field experiments than we could do 40 years ago when Campbell and Stanley (1963) first wrote. In this sense, we are very optimistic about the future. Ve believe that we will continue to see steadg incremental growth in our knowledge about how to do better field experiments. The cost of this growth, howeveq is that field experimentation has become a more specializedtopic, both in terms of knowledge developmentand of the opportunity to put that knowledge into practice in the conduct of field experiments. As a result, nonspecialistswho wish to do a field experiment may greatly benefit by consulting with those with the expertise,especially for large experiments, for experiments in which implementation problems may be high, or for casesin which methodological vulnerabilities will greatly reducecred- ibility. The same is true, of course, for many other methods. Case-studymethods, for example, have become highly enough developed that most researchers would do an amateurishjob of using them without specializedtraining or supervised practice. Such Balkanization of. methodolog)r is, perhaps, inevitable, though none the lessregrettable.\U7e can easethe regret somewhat by recognizingthatwith special- ization may come faster progress in solving the problems of field experimentation.</region>
        <region class="DoCO:FigureBox" id="Fx460">
          <image class="DoCO:Figure" src="62p6.page_081.image_81.png" thmb="62p6.page_081.image_81-thumb.png"/>
        </region>
      </section>
    </body>
  </article>
</pdfx>
