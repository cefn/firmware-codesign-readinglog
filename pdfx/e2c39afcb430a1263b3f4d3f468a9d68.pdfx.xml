<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>fe28af0a7224e6d8e9c37a912bbcafebdda27f3f6010cf6d55d29a877552a02c</job>
    <base_name>62ju</base_name>
    <doi>http://dx.doi.org/10.1145/1145287.1145301</doi>
    <warning>Original PDF was found to be an image-based/possible OCR document. Output quality may be degraded.</warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="header" id="1">Practical Programmer</outsider>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="2">Robert L. Glass</name>
        </contrib>
      </contrib-group>
      <title-group>
        <article-title class="DoCO:Title" id="3">The Standish Report: Does It Really Describe a Software Crisis?</article-title>
      </title-group>
      <region class="unknown" id="4">Reconsidering the relevancy of a frequently cited report on software project failures.</region>
    </front>
    <body class="DoCO:BodyMatter">
      <region class="DoCO:TextChunk" id="5" page="1" column="1">here has been plenty of dis- T cussion over the last several decades about something called “the software crisis.” Those who speak of such a crisis claim software projects are always over budget, behind schedule, and unreliable. The software crisis thinking represents a damning condemnation of software practice. The pic- ture it paints is of a field that cannot be relied upon to produce valid products. But it is important to step back and ask some questions about this crisis thinking:</region>
      <region class="DoCO:TextChunk" id="6" confidence="possible" page="1" column="1">• Does it represent reality? • Is it supported by research findings?</region>
      <outsider class="DoCO:TextBox" type="sidenote" id="7" page="1" column="1">HOEY PETER</outsider>
      <region class="DoCO:TextChunk" id="9" page="1" column="1">In this column, I want to make the point that, based on answers to these questions, there is something seriously flawed in software crisis thinking. The reality is, I would assert, that we are in the midst of what sociologists might <marker type="column" number="2"/><marker type="block"/> call the computing era—an era that would simply not be possible were it not for plentiful successful</region>
      <region class="DoCO:TextChunk" id="10" confidence="possible" page="1" column="2">software projects. Does that reality suggest the software field is really in crisis? Not according to my way of thinking. Specifically, I want to address that second question, the one about research findings. At first glance, there are plenty of publi-</region>
      <region class="DoCO:TextChunk" id="24" page="1" column="3">cations that conclude there really is such a crisis. Many academic studies assert the software crisis is the reason behind the concept the particular study is advocating, a concept that is intended to address and perhaps solve this purported crisis. Software gurus often engage in the same kind of advocacy, and also frame their pet topics as crisis solutions. <marker type="block"/> ut there is an underlying B problem here. Most such academic papers and guru reports cite the same source for their crisis concern—a study pub- lished by the Standish Group more than a decade ago, a study that reported huge failure rates, 70% or more, and minuscule suc- cess rates, a study that con- demned software practice by the title they employed for the pub- lished version of their study, The Chaos Report [<xref ref-type="bibr" rid="R4" id="12" class="deo:Reference">4</xref>]. So the Standish Chaos Report could be considered fundamental to most claims of crisis. What do we really know about that study?<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> That question is of increasing concern to the field. Several researchers, interested in pursuing the origins of this key data, have contacted Standish and asked for a description of their research process, a summary of their latest findings, and in general a schol- arly discussion of the validity of the findings. They raise those issues because most research studies conducted by academic and industry researchers arrive at data largely inconsistent with the Standish findings. Let me say that again. Objec- tive research study findings do not, in general, support those Standish conclusions.<marker type="block"/> epeatedly, those researchers R who have queried Standish have been rebuffed in their quest. It is apparent that Standish has not intended, at least in the past, to share much of anything about where the data used for the Chaos Report comes from. And that, of course, brings the validity of those findings into question. But now there is a significant new thought regarding those Standish findings. One pair of researchers [<xref ref-type="bibr" rid="R3" id="18" class="deo:Reference">3</xref>], combing carefully over that original Standish report, found a key description of where those findings came from. The report says, in Standish’s own words, “We then called and mailed a number of confidential surveys to a random sample of top IT executives, asking them to share failure stories.” Note the words at the end of<marker type="column" number="2"/><marker type="block"/> that sentence: “... share failure stories.” If that was indeed the basis of the contact that Standish made with its survey participants, then the findings of the study are quite obviously biased toward reports of failure. And what does it mean if 70% of projects that are the subject of failure stories even- tually failed? Not much.<marker type="block"/> here is a dramatic case of T déjà vu here. In the 1980s it was popular to support the notion of a software crisis by cit- ing the GAO Study, a report by the U.S. Government Accounting Office that described a terrible failure rate among studied software projects. But in that case, after this had been going on for far too long, one alert researcher [<xref ref-type="bibr" rid="R1" id="21" class="deo:Reference">1</xref>] reread the GAO Study and found that it admitted, quite openly, that it was a study of projects known to be failing at the time the data was gathered. Once this problem was identified, the GAO Study was quite quickly dropped as a citation to support the notion of software crisis. It is interesting that the first Standish study came along not too long afterward. Is it true that the Standish study findings are as biased toward failure as the GAO Study results? The truth of the matter is, we don’t really know. That quoted sentence cited previously certainly suggests so, but it is not at all clear how much of the study was based on the initial contact that sentence describes. And how<marker type="column" number="3"/><marker type="block"/> much of the subsequent study findings (Standish has repeated its survey and updated its Chaos Report several times over the ensu- ing years, see [<xref ref-type="bibr" rid="R2" id="23" class="deo:Reference">2</xref>]) were also based on that same research approach? Once again, it is important to note that all attempts to contact Standish about this issue, to get to the heart of this critical matter, have been unsuccessful. Here, in this column, I would like to renew that line of inquiry. Standish, please tell us whether the data we have all been quoting for more than a decade really means what some have been saying it means. It is too important a topic to have such a high degree of uncertainty associated with it. c</region>
      <outsider class="DoCO:TextBox" type="footer" id="14" page="1" column="3">COMMUNICATIONS OF THE ACM August 2006/Vol. 49, No. 8</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="15" page="1" column="3">15</outsider>
      <outsider class="DoCO:TextBox" type="header" id="16" page="2" column="1">Practical Programmer</outsider>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="25" confidence="possible" page="2" column="3">References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="26" page="2" column="3">1. Blum, B.I. Some very famous statistics. The Software Practitioner (Mar. 1991).</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="27" page="2" column="3">2. Glass, R.L. IT failure rates—70 percent or 10–15 percent? IEEE Software 22, 3 (May–June 2005).</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="28" page="2" column="3">3. Jorgensen, M. and Molokken, K. How large are software cost overruns? A review of the 1994 Chaos Report. Information and Software Tech- nology 48, 4 (Apr. 2006).</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="31" page="2" column="3">4. Standish Group International. The Chaos Report; www.standishgroup.com/sample_ research/PDFpages/Chaos1994.pdf. <marker type="block"/> Robert L. Glass (<email id="30">rlglass@acm.org</email>) is the publisher/editor of The Software Practitioner newsletter and editor emeritus of Elsevier’s Journal of Systems and Software. He is currently an honorary professor in the ARC Center for Complex Systems at Griffith University, Brisbane, Australia.</ref>
        </ref-list>
        <region class="unknown" id="32" page="2" column="3">© 2006 ACM 0001-0782/06/0800 $5.00</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="33" page="2" column="3">16</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="34" page="2" column="3">August 2006/Vol. 49, No. 8 COMMUNICATIONS OF THE ACM</outsider>
        <region class="DoCO:FigureBox" id="Fx35">
          <image class="DoCO:Figure" src="62ju.page_003.image_01.png" thmb="62ju.page_003.image_01-thumb.png"/>
        </region>
      </section>
    </body>
  </article>
</pdfx>
