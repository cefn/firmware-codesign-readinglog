<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>98ef026e89be6ac0a5dbc9e484fe76a39e7706c1f28d3603f20e1959d025657c</job>
    <base_name>62i6</base_name>
    <doi confidence="possible" alt_doi="http://dx.doi.org/10.2172/10167737">http://dx.doi.org/10.1145/1952222.1952266</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="1">Form, Interaction and Function, An Exploratorium for Interactive Products</article-title>
      </title-group>
      <region class="unknown" id="5">J.W.Frens, J.P.Djajadiningrat and C.J.Overbeeke Industrial Design, Technische Universiteit Eindhoven, Den Dolech 2, 5600 MB Eindhoven, the Netherlands, <email id="2">j.w.frens@tue.nl</email>, <email id="3">j.p.djajadiningrat@tue.nl</email>, <email id="4">c.j.overbeeke@tue.nl</email></region>
      <abstract class="DoCO:Abstract" id="6">Abstract: We define an interactive productʼs behaviour through its form, interaction and function. In current interactive products the meaning of form is separated from the meaning of interaction. We feel that tangible interfacing techniques have much potential to enrich the productʼs meaning by adressing not only cognitive skills but also emotional and perceptual-motor skills. Too much cognitive effort is needed to understand and use todays interactive products. ʻSmartʼ products do not exploit all of peopleʼs skills. Furthermore, product design is too preoccupied with the aesthetics of form. In general little attention is paid to the aesthetics of the whole interactive product including the aesthetics of interaction and the relation between form and interaction. Plenty of tools for the exploration of form exist while there are very few tools to explore the aesthetics of interaction and form. In the development of our design tool we are working backwards from the resulting products. We identify requirements for the tool through a product concept for a digital camera. Finally we present our work in progress on the development of the actual tool based on these requirements. Keywords: product design, interaction design, tangible interaction, design tool, exploration</abstract>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="8">rm Fo</name>
        </contrib>
      </contrib-group>
      <region class="DoCO:FigureBox" id="F1">
        <caption class="deo:Caption" id="9">Figure 1: The circle-model of properties</caption>
      </region>
      <region class="DoCO:TextChunk" id="11" confidence="possible">Interactive products increasingly gain complex behaviour. To design this new breed of products designers need new design Function tools. Tools that not only allow for the exploration of form but u r vio also for the exploration of interaction. Here we describe the beha development of a new design tool aimed at the early phase of the product design process. Design tools influence the User appearance and feel of the products that are designed with Int era them. This means that we first have to decide what kind of cti vit interactive products we are targeting with our new tool. y An interactive product can be seen as having three aspects. It has a function, a form, and interactivity. These aspects can be positioned in a circle, see <xref ref-type="fig" rid="F1" id="10" class="deo:Reference">figure 1</xref>. The user has a relation with all three aspects, though of course particularly with interactivity. An important characteristic of the schedule is that the three aspects are related to each other and cannot be designed separately. To design interactive products is to create meaning in form and interaction from a users perspective. Cooper observed that as soon as microprocessors are embedded in products, the computer</region>
      <region class="DoCO:TextChunk" id="14">characteristics of the microprocessor gain control of the interactivity. Interactivity is dictated by mis-matched software logic that has nothing to do with human reasoning. It wonʼt do to design beautiful products with ugly computer-derived interfaces [<xref ref-type="bibr" rid="R1" id="12" class="deo:Reference">1</xref>]. We focus on the exploration of form and interaction, we consider function to be defined. In traditional ʻnon- interactiveʼ products the relation between interaction and form is rather obvious. We can pick up a milk-glass because it fits our hands, and we can open a door because we perceive that the door affords opening [<xref ref-type="bibr" rid="R2" id="13" class="deo:Reference">2</xref>]. We feel that in the current generation of interactive products the relation between form and interaction is more complex. In interactive products functionality is not only defined in form but also in interactive behaviour. For an interactive product to express its function, its form should not only express what can be done with the form as such, it should also express the result of an interaction with that form. This means that not only form but also interaction and the relation between the two should be explored to be able to express functionality of interactive products. In this paper we consider a new computer- supported early-phase product design tool for interactive products. First we describe our view on interaction and give a product example of that view. Then we discuss existing tools and show that they cannot simply be used for the design of interactive products. Finally we describe requirements for a new tool and we discuss the new tool.</region>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="7" page="1" column="1">1. Introduction</h1>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="15" page="2" column="1">2. Interaction, our view</h1>
        <region class="DoCO:TextChunk" id="21" page="2" column="1">Interactive products can have a myriad of functions at low cost because of the advances in electronics. However, electronic technology nowadays is so small it has no ʻhandlesʼ for us humans to control. We feel that unlike product form which is richly varied, interfaces are not. Regardless of their function interfaces are standardized. Furthermore, they have little relationship to our skills. Interfaces have lost meaning completely. People have bodies that are able of doing a wide range of things. Yet, computers and ʻintelligentʼ products are not able to ʻunderstandʼ human skills. On the contrary, intelligent products ask people to make a huge cognitive effort to understand and use them. This seems strange. We feel that manʼs cognitive abilities are emphasized too much and too singularly. People are more than just brains. Overbeeke et al discuss three kinds of skills; cognitive, emotional and perceptual-motor skills. They argue that those skills should be respected [<xref ref-type="bibr" rid="R3" id="16" class="deo:Reference">3</xref>]. Therefore we propose to start designing products that take into account all human skills. If human skills are respected, tangible interaction is a logical consequence [<xref ref-type="bibr" rid="R4" id="17" class="deo:Reference">4</xref>]. Tangible interaction is a topical approach in HCI. It has been researched from various angles including HCI and engineering [<xref ref-type="bibr" rid="R5" id="18" class="deo:Reference">5</xref>], art and design [<xref ref-type="bibr" rid="R6" id="19" class="deo:Reference">6</xref>] and philosophy [<xref ref-type="bibr" rid="R7" id="20" class="deo:Reference">7</xref>]. We like to take tangible interfacing techniques into the field of product design. We emphasize meaningfulness in both form and interaction.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="22" page="2" column="1">3. Products, an example</h1>
        <region class="DoCO:TextChunk" id="24" page="2" column="1">Above we sketched our view on interaction. Here we illustrate our way of thinking with an example. We have a twofold purpose with this. We would like to show that tangible interfacing can be elegant and subtle, but the main goal is to find requirements for an early-phase product design tool. We made a concept-design for a digital camera with a tangible interface, see <xref ref-type="fig" rid="F2" id="23" class="deo:Reference">figure 2</xref>.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="25" page="2" column="1">3.1 Description of functionality</h1>
        <region class="DoCO:TextChunk" id="26" page="2" column="1">The starting-point for the digital camera was a technical description of the functionality. It has the following feature- list. 1. shoot a photo 2. reject a photo 3. store a photo 4. review/play photos 5. control size (pixels) of photo 6. zoom in/zoom out</region>
        <region class="DoCO:FigureBox" id="F2">
          <image class="DoCO:Figure" src="62i6.page_003.image_02.png" thmb="62i6.page_003.image_02-thumb.png"/>
          <image class="DoCO:Figure" src="62i6.page_003.image_01.png" thmb="62i6.page_003.image_01-thumb.png"/>
          <image class="DoCO:Figure" src="62i6.page_003.image_03.png" thmb="62i6.page_003.image_03-thumb.png"/>
          <caption class="deo:Caption" id="28" page="3" column="1">Figure 2: Tangible interfaced camera</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="29" page="3" column="1">3.2 Discussion of the camera</h1>
        <region class="DoCO:TextChunk" id="31" page="3" column="1">The mock-up for the camera is made through foam-core modeling. We start with the question of how a person would use a digital camera. The user-actions that we have in mind were leading the design. Step by step the flow of information through the camera is researched, put into form and tested out. The design process is an iterative process, in testing and changing pre-models meaning is assessed. In the end this leads to the form and interactivity of the camera as it is proposed. <marker type="page" number="4"/><marker type="block"/> Functionality is expressed solely in the form and in the interaction with the form of the camera. The camera does have a screen but that is only used to display pictures. The screen is not used to navigate through menuʼs. The controls of the camera not only express what you can do with them, they also express what will happen when you use them. For example, the trigger expresses that it can be pushed. It also shows that it restrains the screen in the closed position. The screen has two possible positions, it can align with the lens and it can align with a trajectory towards the memory card. So it is clear that when the trigger is pushed the screen will flip in the other position, thus capturing an image.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="32" page="4" column="1">4. Early-phase product design tools</h1>
        <region class="DoCO:TextChunk" id="33" page="4" column="1">Looking at the camera and the process to develop it, it is clear that tools to make meaningful tangible products do not exist. To find requirement for a new tool let us look at the early phase of the design process and the literature.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="34" page="4" column="1">4.1 The early phase of the design process</h1>
        <region class="DoCO:FigureBox" id="Fx35">
          <image class="DoCO:Figure" src="62i6.page_004.image_04.png" thmb="62i6.page_004.image_04-thumb.png"/>
        </region>
        <region class="DoCO:TextChunk" id="39" confidence="possible" page="4" column="1">In the early phase of the product design process exploration is particularly important. All the decisions are still open. The way to go forward is through building and trying out different solutions, see <xref ref-type="fig" rid="F3" id="36" class="deo:Reference">figure 3</xref>. The tools traditionally used support this exploratory process. Tools like pencil and paper or foam modeling enable exploring through ambiguity [<xref ref-type="bibr" rid="R8" id="37" class="deo:Reference">8</xref>]. We wish to stress the point that ambiguity is a key feature of the early phase design process. It is the distinction between the early searching phase and the later form-defining phase of the design process, thus ruling out commonly used CAD-software as early phase design tools [<xref ref-type="bibr" rid="R9" id="38" class="deo:Reference">9</xref>].</region>
        <region class="DoCO:FigureBox" id="F3">
          <caption class="deo:Caption" id="40" page="4" column="1">Figure 3: A designer exploring a design problem through sketches and simple models</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="41" page="4" column="1">4.2 Positioning the tool</h1>
        <region class="DoCO:TextChunk" id="53" page="4" column="1">We presented a model with three aspects of interactive products. Here we extend that model with six fields of exploration, see <xref ref-type="fig" rid="F4" id="42" class="deo:Reference">figure 4</xref>. To position our tool we discuss existing design tools and methods that relate to the fields of form and interaction. Early-phase form exploration tools take many forms. There are traditional tools like pen and paper and foam modeling. Also tools are more and more enhanced by the computer. In combination with a sketch tablet computers can be used as sketching tools, with the added functionality of editability and reusability. Other efforts are less conventional although still running on conventional computers. Igareshiʼs Teddy and Chateau try to combine CAD principles with characteristics of early phase design tools [10][<xref ref-type="bibr" rid="R11" id="43" class="deo:Reference">11</xref>]. And finally there are early phase computer tools that also leave the conventional hardware behind. The ʻsurface drawingʼ system of Schkolne et al is a good example of such a system. Custom drawing and manipulation tools are used in 3D on a virtual reality system to explore form of products [<xref ref-type="bibr" rid="R12" id="44" class="deo:Reference">12</xref>]. Early-phase interaction exploration tools and methods are less common. In literature several role playing and scenario methods with and without the use of props can be found. Matthews et al, use ʻbricksʼ with post-its that describe functional aspects to explore the implications of that functionality when actually ʻusingʼ it in role playing games [<xref ref-type="bibr" rid="R13" id="45" class="deo:Reference">13</xref>]. Buchenau and Fulton Suri describe ʻexperience prototypingʼ. Designers use early prototypes of products to act out scenarios in a correct context to get a feeling for interaction with that product [<xref ref-type="bibr" rid="R14" id="46" class="deo:Reference">14</xref>]. Tools for interaction exploration exist too but are commonly limited to the exploration of screen based interactions. With a tool like DENIM it is possible to explore the structure of new web sites [<xref ref-type="bibr" rid="R15" id="47" class="deo:Reference">15</xref>]. And with prototyping software like Macromediaʼs Director quite advanced software prototypes can be built. In general it is possible to ʻmisuseʼ existing prototyping and presentation tools as tools for the exploration of interaction.<marker type="page" number="5"/><marker type="block"/> Tools or methods to explore both form and interaction are rare. However, in literature some experimental methods can be found. Avrahami and Hudson describe a rapid prototyping tool for interactive products that make use of small physical button-controls that can be attached to form prototypes [<xref ref-type="bibr" rid="R16" id="51" class="deo:Reference">16</xref>]. Greenberg and Fitchett describe ʻphidgetsʼ, the tangible equivalent of widgets. With phidgets it is possible to roughly prototype tangible interfaces [<xref ref-type="bibr" rid="R17" id="52" class="deo:Reference">17</xref>]. Above we discussed tools for the exploration of form, interaction and combinations. As we pointed out earlier we consider form and interaction as strongly interrelated. Form and interaction should be explored together. However, we observed a dichotomy between tools for the exploration of interaction and tools for the exploration of form. And while there are tools that take into consideration both form and interaction we feel that those are not satisfactory. They combine the exploration of form and interaction but it is not possible to explore the effect of the one on the other. It is hard to make quick variations of both form and interaction at the same time. While that is what exploration is about. Moreover it is impossible to explore form and interaction at the same level of elaboration. It seems crucial to have an integrated solution for the exploration of both that will allow ambiguous prototypes in order to support early phase product design for highly interactive tangibly interfaced products.</region>
        <region class="unknown" id="49" page="5" column="1">Function exploration: The exploration of the task the product has to carry out. In case of our camera the main-functionality would be lor ati ful on : T r ela In he ca tio se hink n o f a ha an n d ing - ful making pictures pictures. are defined Sub-functionality afterwards. like scaling the m. t h e pla le n cem ex pres ent an s d e ex s fo p rm. lora Fu nc tio t n i o of n an d Int erac ex plora tion be twee tion and ca m n era Fu mo int of ncti , erac vem for wh th on ent tion at e tak s act exp me ing an fo ion r ani d a e f s xam unc ng na of psh th ti p on. le) e ot. u wo se r uld (t be me Function s t o sh of o w th the e fun sc ree co ct Th i nn o e n n e . a c c t o t ion nsid the the For er b re c ente etwe atio latio exa Fo rlin n rm en m n p is b e l etw e o ex th f at een plo th e r f o a fu r tio m nc n: tio n Th e User pr ac t Int e r Int the th e of bo dy, ch is a ract it co sn m erist app pon o y ic ent. duc or of l t, azy. ion the In c w a sc a s i n ac e ree th d o tio o f n u r t ea ou n cti r ex a c o plo a n mea rat be ion t n w ee : T n he ex era cti vit y Fo rm orat ion f or : m The an the d e m xp b ate lo ea ic rat h ut ri al y ma ion o c a f am ter im f o orm ing era f ial . for sho wi In th ul c ase d all w its e use pa rts a fli ppi n mera g ou t , o i t f ngf he th ul spe e h ed um for plo a m n - rat a n ion d of exploration Form and Interaction of the meaningful exploration: relation The Fo rm ae expl s thet op timis ics ou o r at f wh ca ion mera ere of be au an , tif d wh ul is wh th e ole . between form and interaction. this would mean the exploration of how and if activating the ‘trigger’ should result in the screen coming out of the body of the camera.</region>
        <region class="DoCO:FigureBox" id="F4">
          <caption class="deo:Caption" id="50" page="5" column="1">Figure 4: Fields for exploration</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="54" page="5" column="1">5. Requirements</h1>
        <region class="DoCO:TextChunk" id="57" page="5" column="1">We presented an example of an interactive product. We discussed existing tools and the results that such tools deliver. We located the carriers of meaning that are relevant for such interactive products as we propose. They led to four characteristics of the interactive prototypes that our tool should deliver. <marker type="page" number="6"/><marker type="block"/> (1) Physical structure of product (3D-layout): With the structure of products we mean the way controls and displays of the future product are positioned towards each other in three dimensions. This is important for two reasons. First, an impression of the space a product needs can be obtained. Second, the interactive prototype can be tested in an experiential way. (2) Form of controls &amp; feedback elements: The form of the controls and feedback elements are important for the interactive prototypes because these forms are carriers of meaning. (3) Interactivity: This is both user actions and product reactions. Users actions lead to movements of controls. A product with a tangible interface can react through movement of its parts and/or by changing lights or contents of screens. For instance movements of controls and displays are important because the way to control a product with a tangible interface is by moving its parts. Movement is a carrier of meaning, both for the user and for the product. (4) Relation between actions &amp; reactions: The relation between actions and reactions are important for two reasons. First, the behaviour of the product is in the relation between action and reaction. Second, the nuance of the relation between action and reaction is a carrier of meaning.<marker type="block"/> To come full circle, what is wrong with the current tools? Why do we need a new tool? The answer to this question lies in the results those tools offer. We chose to explore the design of the camera in 3D by means of foam-core modeling. After a lot of pre-models we built a final model of the camera. This model offers (a limited set of) action possibilities but no reactions. It has characteristics (1) and (2) but it has no interactivity or relations between actions and reactions. This is the problem, there are no tools that deliver results that have all characteristics. It is quintessential to be able to quickly vary the form and the interaction to explore the effects they have on each other.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="58" page="6" column="1">6. The Exploratorium: a new tool</h1>
        <region class="DoCO:TextChunk" id="60" page="6" column="1">The new tool is made up of three parts and we dubbed it the exploratorium, see <xref ref-type="fig" rid="F5" id="59" class="deo:Reference">figure 5</xref>. The first part comprises traditional cardboard modeling tools. To be specific: cardboard, a ruler, a pencil, cutting knives and a cutting mat. The second part is all about making the cardboard models interactive through the use of programmable ʻmotion devicesʼ and a programming area. The third and last part is about exploring the relation between action and reaction.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="61" page="6" column="1">6.1 Interactivity: motion devices</h1>
        <region class="DoCO:FigureBox" id="F5">
          <image class="DoCO:Figure" src="62i6.page_006.image_05.png" thmb="62i6.page_006.image_05-thumb.png"/>
          <caption class="deo:Caption" id="63" page="6" column="1">Figure 5: The exploratorium</caption>
        </region>
        <region class="DoCO:TextChunk" id="66" confidence="possible" page="6" column="1">The cardboard models can be made responsive by equipping them with actuators and sensors. We combine an actuator and a sensor in one miniaturised package, we call such a combination a motion device. The motion devices can be used as sensing devices, as actuating devices or as both. They feature two small planes for attaching the devices to the parts of the cardboard model, see <xref ref-type="fig" rid="F6" id="64" class="deo:Reference">figure 6</xref>. When a part is moved the attached motion device senses this movement. Vice versa when the motion device is actuated the part will move. This sets the stage for programming by example. By holding the model equipped with motion devices above the programming area the model becomes receptive for programming input. By moving its parts in the desired ways the model can be programmed, see <xref ref-type="fig" rid="F7" id="65" class="deo:Reference">figure 7</xref>. The motion devices act purely as sensors when they are in programming mode but they will act both as sensors and actuators when in experience mode.</region>
        <region class="DoCO:FigureBox" id="F6">
          <image class="DoCO:Figure" src="62i6.page_006.image_06.png" thmb="62i6.page_006.image_06-thumb.png"/>
          <caption class="deo:Caption" id="68" page="6" column="1">Figure 6: A motion device</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="69" page="7" column="1">6.2 Relating actions and reactions</h1>
        <region class="DoCO:FigureBox" id="F7">
          <image class="DoCO:Figure" src="62i6.page_007.image_07.png" thmb="62i6.page_007.image_07-thumb.png"/>
          <caption class="deo:Caption" id="71" page="7" column="1">Figure 7: Programming a model equipped with motion devices</caption>
        </region>
        <region class="DoCO:FigureBox" id="F8">
          <image class="DoCO:Figure" src="62i6.page_007.image_08.png" thmb="62i6.page_007.image_08-thumb.png"/>
          <caption class="deo:Caption" id="73" page="7" column="1">Figure 8: The timer</caption>
        </region>
        <region class="DoCO:TextChunk" id="75" confidence="possible" page="7" column="1">Programming the behaviour of interactive cardboard models in the way we propose above goes beyond mere definition of the fact that parts of the model move. It also about how and when they move. It is about the ʻfeelʼ of the interaction. To be able to also explore this component of a products behaviour we need to extend the toolset a bit more. First we need a timer to be able to time and scale the actions and reactions of the model, see <xref ref-type="fig" rid="F8" id="74" class="deo:Reference">figure 8</xref>. The two inlets of the timer can be connected to two motion devices, the start, duration and end of the movements of the two devices can now be timed and scaled relative to each other. The timing action is started by connecting the inlets to the rotary time scales. The start-times and durations of the two movements can now be read, relative to each other, on the time scales. The small scalers can be moved to manipulate the start time and end time, and thus the relative duration. Second we need a way to give ʻfeelʼ to the motion devices. We define the feel of a motion through material parameters. We took force-way diagrams as the starting point for the description of the ʻfeelʼ of a material. We developed a tool that can be used to generate such force-way diagrams. We dubbed this tool the ʻhaptic composerʼ for it nuances the movements of a motion device by giving it feel, see figure 9. Material can be clamped into the tongs of the haptic composer. The haptic composer is equipped with pressure sensitive pads and a rotary sensor, thus allowing for the creation of force-way diagrams. A desired feel for a movement can be composed by combining different materials and can be assessed at the handles of the haptic composer. When a feel is composed it can be transferred from the haptic composer to the desired motion device by connecting them through a wire.</region>
      </section>
      <section class="deo:Discussion">
        <h1 class="DoCO:SectionTitle" id="76" page="7" column="1">7. Discussion</h1>
        <region class="DoCO:FigureBox" id="Fx77">
          <image class="DoCO:Figure" src="62i6.page_007.image_09.png" thmb="62i6.page_007.image_09-thumb.png"/>
        </region>
        <region class="DoCO:TextChunk" id="79" page="7" column="1">In this paper we discussed our view on interactive products and design tools that will be needed to develop the products we envision. Here we discuss our approach to developing the new tool. This research started with products, we based the tool proposal on a design for a digital camera with a tangible interface. So we choose not to work from a technologist point of view but to base our work in the design practice. We work from the observation that the appearance and ʻfeelʼ of products are influenced by the tools that are used to design them. That means that by the choice of a tool a designer makes a decision, Figure 9: The haptic composer conscious or not, as to how the product will look and feel. To illustrate this observation we like to give some examples. A product that is designed solely on paper will have a <marker type="page" number="8"/><marker type="block"/> different appearance than products that are designed using foam or clay for making 3D models. Products that are designed from orthogonal views tend to be highly defined in one or two views and quite nondescript in the other views. Think a video recorder. Cars for example are much more designed in 3D. In older cars you can actually see where the knives cut the modeling clay. The same goes for interactive products. When looking at human-product interfaces, apart from the eye-candy that is liberally sprinkled onto the screens that define such products, they are computers. Designed with computers to be computers. Therefore, to build a useful tool we first had to decide how the products would look like that will be designed with it. We chose for interactive products with tangible interfaces. Our tool is meant to design such products. We derived guidelines for the tool from the process of designing the camera. Moreover, the same design principles that led the design of the camera led the design of the tool. To build something useful is to answer the why-question and the how-question. We want to build a new design tool for there is a new breed of interactive products that cannot be designed properly. We do this through designing such products with traditional tools to feel where those tools lack and to taste the design process that is required to design such products. The knowing is in the doing.</region>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="80" page="8" column="1">8. References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="81" page="8" column="1">1. Cooper A. The inmates are running the asylum. SAMS McMillan, Indianapolis (1999).</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="82" page="8" column="1">2. Norman DA. The design of everyday things. Currency/Doubleday, New York (1990).</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="83" page="8" column="1">3. Overbeeke CJ, Djajadiningrat JP, Wensveen SAG, Frens JW. Set Me Free, Give Me Degrees of Freedom. Proceedings of SSGRR2001 Conference. LʼAquila, 6 - 12 August. CD-rom (2001).</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="84" page="8" column="1">4. Djajadiningrat JP, Overbeeke CJ, Wensveen SAG. But how, Donald, tell us how? Proceedings of DIS2002 Conference. London, July (2002).</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="85" page="8" column="1">5. Ullmer B, Ishii H. Emerging frameworks for tangible user interfaces. IBM Systems Journal, vol 39 NOS 3&amp;4 (2000).</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="86" page="8" column="1">6. Strong R &amp; Gaver WW. Feather, Scent, and Shaker: Supporting Simple Intimacy. Proceedings of CSCW ʻ96, ACM Press, 29-30 (1996).</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="87" page="8" column="1">7. Dourish P. Where the Action Is, The Foundations of Embodied Interaction. MIT Press Cambridge, MA (2001).</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="88" page="8" column="1">8. Fish J &amp; Scrivener S. Amplifying the Mindʼs Eye: Sketching and Visual Cognition. Leonardo, Vol. 23, No. 1, 117-126 (1990).</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="89" page="8" column="1">9. Frens JW, Djajadiningrat JP, Overbeeke CJ. Computer Generated Freedom, Searching not Knowing. Proceedings of ITEC2002, Lille, 9-11 April, CD-ROM (2002).</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="90" page="8" column="1">10. Igarashi T, Matsuoka S, Tanaka H. Teddy: A Sketching Interface for 3D Freeform Design. SIGGRAPH 99 Conference Proceedings, Los Angeles, 409-416 (1999).</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="91" page="8" column="1">11. Igarashi T, Hughes JF. A Suggestive Interface for 3D Drawing. Proceedings of the 14th. annual ACM symposium on user interface software and technology, November (2001).</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="92" page="8" column="1">12. Schkolne S, Pruett M, Schröder P. Surface Drawing: Creating Organic 3D Shapes with the Hand and Tangible Tools. SIGCHI ʼ01 Conference Proceedings, Seattle, 261-268 (2001).</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="93" page="8" column="1">13. Matthews B, Brereton M, Buur J. Brick Games in Boardrooms: Making Use Context Tangible. Designing in Context Conference Proceedings, Delft (2001).</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="94" page="8" column="1">14. Buchenau M, Fulton Suri J. Experience Prototyping. DISʼ00 Conference Proceedings, Brooklyn, New York, 424-433 (2000).</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="95" page="8" column="1">15. Lin J, Newman MW, Hong JI, Landay JA. DENIM: Finding a Tighter Fit Between Tools and Practice for Web Site Design. Proceedings of CHI ʻ00, The Hague, 510-517 (2000).</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="96" page="8" column="1">16. Avrahami D, Hudson SE. Forming Interactivity: A Tool for Rapid Prototyping of Physical Interactive Products. Proceedings of DIS ʻ02 Conference, London, 141-146 (2002).</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="97" page="8" column="1">17. Greenberg S, Fitchett C. Phidgets: Easy Development of Physical Interfaces through Physical Widgets. Proceedings of UIST ʻ01, Orlando (2001).</ref>
        </ref-list>
      </section>
    </body>
  </article>
</pdfx>
