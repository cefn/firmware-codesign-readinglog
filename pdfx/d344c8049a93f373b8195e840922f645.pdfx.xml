<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>db947d66b177cb37e2318a85700fceedad9c76f89b0c7389e34556352b64674d</job>
    <base_name>62ne</base_name>
    <doi>http://dx.doi.org/10.1145/503447.503450</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="header" id="1">minneapolis, minnesota, usa • 20-25 april 2002</outsider>
      <outsider class="DoCO:TextBox" type="header" id="2">Paper: Ubiquity</outsider>
      <title-group>
        <article-title class="DoCO:Title" id="3">Making Sense of Sensing Systems: Five Questions for Designers and Researchers</article-title>
      </title-group>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="4">Victoria Bellotti</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="5">Maribeth Back</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="6">W. Keith Edwards</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="7">Rebecca E. Grinter</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="8">Austin Henderson ¥</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="9">Cristina Lopes</name>
        </contrib>
      </contrib-group>
      <region class="unknown" id="11">Xerox Palo Alto Research Center 3333 Coyote Hill Rd. Palo Alto, CA 94304 USA Tel: +1 650 812 4666 Fax: +1 650 812 4471 <email id="10">bellotti@parc.xerox.com</email></region>
      <abstract class="DoCO:Abstract" id="12">This paper borrows ideas from social science to inform the design of novel “sensing” user-interfaces for computing technology. Specifically, we present five design challenges inspired by analysis of human-human communication that are mundanely addressed by traditional graphical user interface designs (GUIs). Although classic GUI conventions allow us to finesse these questions, recent research into innovative interaction techniques such as ‘Ubiquitous Computing’ and ‘Tangible Interfaces’ has begun to expose the interaction challenges and problems they pose. By making them explicit we open a discourse on how an approach similar to that used by social scientists in studying human-human interaction might inform the design of novel interaction mechanisms that can be used to handle human-computer communication accomplishments.</abstract>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="13" confidence="possible" page="1" column="1">Keywords</h2>
      </section>
    </front>
    <body class="DoCO:BodyMatter">
      <region class="DoCO:TextChunk" id="14" page="1" column="1">Ubiquitous Computing, sensing input, design framework, social science, human-machine communication.</region>
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="15" page="1" column="1">INTRODUCTION</h1>
      </section>
      <region class="DoCO:TextChunk" id="29" page="1" column="1">Designers of user interfaces for standard applications, devices, and systems rarely have to worry about questions of the following sort: When I address a system, how does it know I am addressing it? When I ask a system to do something how do I know it is attending? When I issue a command (such as save, execute or delete), how does the system know what it relates to? How do I know the system understands my command and is correctly executing my intended action? How do I recover from mistakes? Familiar GUI mechanisms such as cursors, windows, icons, menus, and drag-and-drop provide pre-packaged answers to <marker type="column" number="2"/><marker type="block"/> these key concerns. For example, a flashing cursor denotes that system is attending and what its focus is (where typed input will go). Such mechanisms have, by now, become conventions of commonplace and accepted genres for interaction. Indeed it is easy to forget that each one had to be carefully designed, before it ever became a convention. By genre here, we mean a set of design conventions anticipating particular usage contexts with their own conventions. Examples of system genres include; games, productivity tools, and appliances and examples of interaction genres include, the GUI, voice activation and the remote control (for home entertainment systems). Genre makes design easier by pre-packaging sets of interaction conventions in a coherent manner that designers can use to leverage user expectations about the purpose and use of a device and to accommodate their existing skills. By sticking to the GUI genre (and other simpler genres for cell-phones, video-recorders, microwaves and so on), using standardized toolkits, and by copying design ideas from existing solutions, designers now assemble myriad UIs for desktop, laptop, hand-held and other devices from pre- existing components without needing to ponder basic interaction issues. (While our discussion, in the rest of this paper applies to all of these established UI genres equally, we will address our arguments in particular towards comparisons with the GUI.) However, those working in areas such as Ubiquitous Computing [<xref ref-type="bibr" rid="R30" id="20" class="deo:Reference">30</xref>], where input is sensed by means other than keys, mouse or stylus (e.g., gesture, voice, or location), have no such well-understood, pre-packaged answers to these questions. Lacking these well-established precedents, designers of sensing systems must constantly confront these basic questions anew. In the rest of this paper we present a framework for addressing the resulting design challenges inherent in sensing systems, drawing on lessons about human-human interaction (HHI) in social science. Our approach is not the same as presenting methods and guidelines for HCI design such as [<xref ref-type="bibr" rid="R21" id="21" class="deo:Reference">21</xref>] or Apple’s well- known Human Interface Guidelines [<xref ref-type="bibr" rid="R2" id="22" class="deo:Reference">2</xref>]. Such texts are useful for designing systems within GUI-style interaction paradigms. Indeed they provide designers with generalizations relating to the parts, rules and meanings<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> constituting human-system dialog. However, these approaches tend to deal in specific interaction mechanisms rather than the general accomplishments they support, they do not fare well when applied to innovative genres of interaction beyond the GUI. Instead, our aim is to revisit and bring together some fundamentals of HCI, borrowing concepts from the social sciences, to provide a systematic framework for the design of sensing systems.</region>
      <region class="unknown" id="17" page="1" column="1">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CHI 2002, April 20-25, 2002, Minneapolis, Minnesota, USA. Copyright 2002 ACM 1-58113-453-3/02/0004...$5.00.</region>
      <region class="unknown" id="19" page="1" column="2">¥ Rivendel Consulting &amp; Design Inc. P.O. Box 334 La Honda, CA 94020 USA Tel: +1 650 747 9201 Fax: +1 650 747 0467 <email id="18">henderson@rivcons.com</email></region>
      <outsider class="DoCO:TextBox" type="footer" id="24" page="1" column="2">Volume No. 1, Issue No. 1</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="25" page="1" column="2">415</outsider>
      <outsider class="DoCO:TextBox" type="header" id="26" page="2" column="1">Paper: Ubiquity</outsider>
      <outsider class="DoCO:TextBox" type="header" id="27" page="2" column="1">CHI</outsider>
      <outsider class="DoCO:TextBox" type="header" id="28" page="2" column="1">changing the world, changing ourselves</outsider>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="30" page="2" column="1">REFRAMING INTERACTION FOR SENSING SYSTEMS</h1>
        <region class="DoCO:TextChunk" id="41" page="2" column="1">We have, in the last decade, seen a number of innovations in interaction mechanisms best characterized overall as sensing systems; including Ubiquitous Computing (Ubicomp) systems [<xref ref-type="bibr" rid="R1" id="31" class="deo:Reference">1</xref>, <xref ref-type="bibr" rid="R30" id="32" class="deo:Reference">30</xref>]; Speech and audio input [<xref ref-type="bibr" rid="R18" id="33" class="deo:Reference">18</xref>, <xref ref-type="bibr" rid="R27" id="34" class="deo:Reference">27</xref>]; Gesture-based input [<xref ref-type="bibr" rid="R31" id="35" class="deo:Reference">31</xref>] Tangible Interfaces or ‘Phicons’ (Physical Icons) [<xref ref-type="bibr" rid="R17" id="36" class="deo:Reference">17</xref>] and Context Aware computing [<xref ref-type="bibr" rid="R1" id="37" class="deo:Reference">1</xref>, <xref ref-type="bibr" rid="R9" id="38" class="deo:Reference">9</xref>, <xref ref-type="bibr" rid="R18" id="39" class="deo:Reference">18</xref>]. These sensing mechanisms have expanded what was previously a key-pressing, point-and- click interaction bottleneck, allowing systems to accept a far wider range of input than was previously possible. However, by definition, designers of these systems cannot simply copy existing precedents for handling input and output, unlike standard GUI designers. The point of their research is to tackle anew the many challenges that had to be addressed in the GUI and its cousins to make it over Norman’s famous gulfs of execution and evaluation [<xref ref-type="bibr" rid="R22" id="40" class="deo:Reference">22</xref>].</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="42" confidence="possible" page="2" column="1">Interaction As Execution and Evaluation</h2>
          <region class="DoCO:TextChunk" id="45" page="2" column="1">Norman [ <xref ref-type="bibr" rid="R22" id="43" class="deo:Reference">22</xref>] proposes an “approximate model” of seven stages of action with respect to system interaction: Forming the goal Forming the intention Specifying an action Executing the action Perceiving the state of the world Interpreting the state of the world Evaluating the outcome It is important to notice that Norman’s theory of action focuses on user cognition. Moreover, it implicitly reflects a difference between HHI and HCI. Humans and computers are not equal partners in dialog. Computers are dumb slaves, have limited functionality, and rarely take the initiative. On the other hand, they have capabilities that humans do not. They can output precise information about their state, perform many rapid calculations simultaneously, emulate a vast range of tools and control multiple complex mechanical systems in parallel, and they can be guided and manipulated in many different ways by users. The clever ploy embodied in the GUI is to exploit the different roles and relative strengths of computer and user and finesse the communication problem by forcing the user (using a display and a pointing and selecting device) to drive interaction, constantly discovering and monitoring which of many possible things the system is capable of and how it is interpreting ongoing action. Norman’s account of HCI as an execution-evaluation cycle works well as long as we stick to the GUI genre that pre-packages solutions to the<marker type="column" number="2"/><marker type="block"/> interaction problem. In this case, the analytic interest then resides mainly in what’s going on in the user’s head.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="46" confidence="possible" page="2" column="2">Interaction as Communication</h2>
          <region class="DoCO:TextChunk" id="60" page="2" column="2">In contrast to Norman, our approach highlights communicative, rather than cognitive aspects of interaction. We agree with the coverage of Norman’s model–from human intent to assessment of system action–but focus our attention on the joint accomplishments of the user and system that are necessary to complete the interaction, rather than the user’s mental model. This stance is driven by a growing appreciation of two developments: The potential value of social science to the field of HCI. However, rather than focusing on the findings of sociologists about the use of technology in social settings [e.g., 7, 16] we are using the kinds of questions addressed by social science in HHI as a model on which to pattern some of the science of HCI. We understand, as we have said, that HHI and HCI cannot be regarded as identical problem spaces; however, we argue that despite the differences, many of the same communication challenges apply and must be recognized by designers. A trend in HCI towards sensing systems that dispense with well-known interaction genres, requiring us to return to the basic communication problems that the pre- packaged GUI interaction solutions so elegantly solved. Goffman, an interaction analyst who has been particularly influential in social science, has written extensively on interpersonal verbal and non-verbal communication [ <xref ref-type="bibr" rid="R12" id="47" class="deo:Reference">12</xref>, <xref ref-type="bibr" rid="R13" id="48" class="deo:Reference">13</xref>, <xref ref-type="bibr" rid="R15" id="49" class="deo:Reference">15</xref>]. He provides a perspective on HHI that elucidates how people manage accomplishments such as addressing, attending to and politely ignoring one another. For example, signals are used to communicate intention to initiate, availability for communication, or that a listener understands what is being said. Surely attention to similar mechanisms for HCI could be valuable. Further Goffman [<xref ref-type="bibr" rid="R14" id="50" class="deo:Reference">14</xref>] also developed a notion of frames that are social constructs (such as a ‘performance,’ a ‘game,’ or a ‘consultation’) that allow us to make sense of what might otherwise seem to be incoherent human actions. Frames in HHI seem to parallel genre in HCI as defined above and may be useful constructs for informing design. From Conversation Analysis, we know that successful conversation demands many basic accomplishments that most humans master. Sacks et al., [<xref ref-type="bibr" rid="R25" id="51" class="deo:Reference">25</xref>] show how turn taking is managed as conversational participants organize their talk in an orderly fashion. Schegloff et al., [<xref ref-type="bibr" rid="R27" id="52" class="deo:Reference">27</xref>] demonstrate how mistakes, and misunderstandings are repaired in communication. Button and Casey, [<xref ref-type="bibr" rid="R8" id="53" class="deo:Reference">8</xref>] examine how people establish a shared topic in conversation. Similarly humans and systems must manage and repair their communications, and must be able to establish a shared topic (e.g., some action). These perspectives provide inspiration for the following five issues that are intended to cover the same ground as<marker type="page" number="3"/><marker type="column" number="1"/><marker type="block"/> Norman’s seven stages of execution, but with the emphasis now being on communication rather than cognition. Address: Directing communication to a system. Attention: Establishing that the system is attending. Action: Defining what is to be done with the system (roughly equivalent to Norman’s ‘Gulf of Execution’). Alignment: Monitoring system response (roughly equivalent to Norman’s ‘Gulf of Evaluation’). Accident: Avoiding or recovering from errors or misunderstandings. These issues may be posed as five questions that a system user must be able to answer to accomplish some action. <xref ref-type="table" rid="T1" id="59" class="deo:Reference">Table 1</xref> shows how each question has a familiar GUI answer. Further, each one poses some challenges that are easily solved by sticking to the existing GUI paradigm and its simpler hand-held counterparts. However, for novel sensing systems, the challenges take center-stage as design issues again and we list some of them here, together with some potential problems caused by not addressing them.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="55" page="2" column="2">416</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="56" page="2" column="2">Volume No. 1, Issue No. 1</outsider>
          <outsider class="DoCO:TextBox" type="header" id="57" page="3" column="1">minneapolis, minnesota, usa • 20-25 april 2002</outsider>
          <outsider class="DoCO:TextBox" type="header" id="58" page="3" column="1">Paper: Ubiquity</outsider>
          <region class="DoCO:TableBox" id="Tx61">
            <content>
              <table class="DoCO:Table" number="1" page="3">
                <thead class="table">
                  <tr class="table">
                    <th class="table"> Basic Question</th>
                    <th class="table"> Familiar GUI Answers</th>
                    <th class="table"> Exposed Challenges</th>
                    <th class="table"> Possible Problems</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="table.strange">
                    <td class="table.strange"> Address: How do I</td>
                    <td class="table.strange"> Keyboard</td>
                    <td class="table.strange"> How to disambiguate signal-to-noise</td>
                    <td class="table.strange"> No response</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> address more) of one many (or</td>
                    <td class="table"> Mouse (point-and-click)</td>
                    <td class="table"> How to disambiguate intended target system</td>
                    <td class="table"> Unwanted response</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> possible devices?</td>
                    <td class="table"> Social access control over physical</td>
                    <td class="table"> How to not address the system</td>
                    <td class="table"></td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Attention: I know the system How do is</td>
                    <td class="table"> Graphical flashing cursor, feedback cursor (e.g., moves</td>
                    <td class="table"> How user can to embody be aware appropriate of the system’s feedback, attention so that the</td>
                    <td class="table"> Wasted system not input attending effort while</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> ready and attending</td>
                    <td class="table"> when mouse moved)</td>
                    <td class="table"></td>
                    <td class="table"></td>
                  </tr>
                  <tr class="table">
                    <td class="table"> to my actions?</td>
                    <td class="table"> Assume monitor user is looking at</td>
                    <td class="table"> How to direct feedback to zone of user attention</td>
                    <td class="table"> Unintended Privacy or security action problems</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Action: effect a meaningful How do I</td>
                    <td class="table"> Click cursor on over objects(s) area around or drag</td>
                    <td class="table"> How action. to identify and select a possible object for</td>
                    <td class="table"> Limited available operations</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> action, extent and control possibly its</td>
                    <td class="table"> object(s). menu (e.g., Select recent objects files). from Select</td>
                    <td class="table"> How to the to object(s) identify and select an action, and bind it</td>
                    <td class="table"> Failure to execute action</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> specify a target or</td>
                    <td class="table"> actions from menu, accelerator</td>
                    <td class="table"> How to avoid unwanted selection.</td>
                    <td class="table"> Unintended (wrong response) action</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> targets action? for my</td>
                    <td class="table"> keys, controls etc. (e.g., Manipulate sliders). graphical</td>
                    <td class="table"> How multiple to handle objects, complex actions, operations and more (e.g., abstract functions graphically, that such are as difficult save). to represent</td>
                    <td class="table"></td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Alignment: How do</td>
                    <td class="table"> GUI graphical presents elements distinctive establishing</td>
                    <td class="table"> How to make system state perceivable and</td>
                    <td class="table"> Inability to differentiate</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> I doing right know thing? (has the done) system the is</td>
                    <td class="table"> a consequences context with of predictable action</td>
                    <td class="table"> persistent How to direct or query-able timely and appropriate feedback</td>
                    <td class="table"> more space than limited action</td>
                  </tr>
                  <tr class="table">
                    <td class="table"></td>
                    <td class="table"> Graphical characters feedback appear, rubber- (e.g.,</td>
                    <td class="table"> How and state to provide (what is distinctive the response) feedback on results</td>
                    <td class="table"> Failure Unintended to execute action action</td>
                  </tr>
                  <tr class="table">
                    <td class="table"></td>
                    <td class="table"> banding)</td>
                    <td class="table"></td>
                    <td class="table"> Difficulty evaluating new</td>
                  </tr>
                  <tr class="table">
                    <td class="table"></td>
                    <td class="table"> Auditory feedback</td>
                    <td class="table"></td>
                    <td class="table"> state Inability to detect</td>
                  </tr>
                  <tr class="table">
                    <td class="table"></td>
                    <td class="table"> Detectable in new position) new state (e.g., icon</td>
                    <td class="table"></td>
                    <td class="table"> mistakes Unrecoverable state</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Accident: avoid mistakes How do I</td>
                    <td class="table"> Control/guide manipulation in direct</td>
                    <td class="table"> How progress to control or cancel system action in</td>
                    <td class="table"> Unintended action</td>
                  </tr>
                  <tr class="table">
                    <td class="table"></td>
                    <td class="table"> Stop/cancel</td>
                    <td class="table"> How to disambiguate what to undo in time</td>
                    <td class="table"> Undesirable Inability to recover result state</td>
                  </tr>
                  <tr class="table">
                    <td class="table"></td>
                    <td class="table"> Undo</td>
                    <td class="table"> How to intervene when user makes obvious error</td>
                    <td class="table"></td>
                  </tr>
                  <tr class="table.strange">
                    <td class="table.strange"></td>
                    <td class="table.strange"> Delete</td>
                    <td class="table.strange"></td>
                    <td class="table.strange"></td>
                  </tr>
                </tbody>
              </table>
            </content>
            <region class="TableInfo" id="62" confidence="possible" page="3" column="1">Basic Question Familiar GUI Answers Exposed Challenges Possible Problems Address: How do I Keyboard How to disambiguate signal-to-noise No response address one (or Mouse (point-and-click) How to disambiguate intended target system Unwanted response more) of many Social control over physical How to not address the system possible devices? access Attention: How do Graphical feedback (e.g., How to embody appropriate feedback, so that the Wasted input effort while I know the system is flashing cursor, cursor moves user can be aware of the system’s attention system not attending ready and attending when mouse moved) How to direct feedback to zone of user attention Unintended action to my actions? Assume user is looking at Privacy or security monitor problems Action: How do I Click on objects(s) or drag How to identify and select a possible object for Limited operations effect a meaningful cursor over area around action. available action, control its object(s). Select objects from How to identify and select an action, and bind it Failure to execute action extent and possibly menu (e.g., recent files). Select to the object(s) Unintended action specify targets for a target my or keys, actions etc. from Manipulate menu, accelerator graphical How to avoid unwanted selection. (wrong response) action? controls (e.g., sliders). How to handle complex operations (e.g., multiple objects, actions, and more abstract functions that are difficult to represent graphically, such as save). Alignment: How do GUI presents distinctive How to make system state perceivable and Inability to differentiate I know the system is graphical elements establishing persistent or query-able more than limited action doing (has done) the a context with predictable How to direct timely and appropriate feedback space right thing? consequences of action How to provide distinctive feedback on results Failure to execute action Graphical feedback (e.g., and state (what is the response) Unintended action characters appear, rubber- Difficulty evaluating new banding) state Auditory feedback Inability to detect Detectable new state (e.g., icon mistakes in new position) Unrecoverable state Accident: How do I Control/guide in direct How to control or cancel system action in Unintended action avoid mistakes manipulation progress Undesirable result Stop/cancel How to disambiguate what to undo in time Inability to recover state Undo How to intervene when user makes obvious error Delete</region>
            <caption class="deo:Caption" id="63" page="3" column="1">Table 1. Five questions posing human-computer communication challenges for interaction design</caption>
          </region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="64" page="3" column="2">ADDRESSING THE CHALLENGES OF INTERACTION</h1>
        <region class="DoCO:TextChunk" id="65" page="3" column="2">In this section, we review some of the ways each of our five questions is mundanely addressed by conventions in familiar GUI applications. We then consider alternative sensing approaches to interaction drawn from a number of recent research prototypes that expose the related challenges and either succeed or fail in addressing them.</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="66" confidence="possible" page="3" column="2">1. Address</h2>
          <region class="DoCO:TextChunk" id="78" page="3" column="2">The first question we raise is so fundamental that it is often taken for granted in UI design: What mechanisms does the user employ to address the system? Analyses of HHI show that humans make use of a formidable array of verbal and non-verbal mechanisms to accomplish or avoid this activity [ <xref ref-type="bibr" rid="R12" id="67" class="deo:Reference">12</xref>]. The GUI Solution In GUI applications, the “system” is a very clear concept; it’s the box sitting on your desk. Designers know that if the user intends to interact with the system, he or she will use the devices, such as a keyboard or mouse, attached to it. There is little possibility for error, barring cables falling out of sockets, or users accidentally touching input devices.<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> Exposed Challenges Such assumptions, however, are invalid when more “ambient” modes of input, such as gesture, are used, as well as when the notion of what precisely constitutes “the system” is a more amorphous concept. In such settings, the following challenges arise: How to disambiguate signal-to-noise. How to disambiguate intended target system. How to not address the system. Augmented Objects [<xref ref-type="bibr" rid="R29" id="74" class="deo:Reference">29</xref>] tackle this challenge by the intuitive use of proximity of Augmented Objects to sensors; objects augmented with RFID (radio frequency identity) tags or IR (infrared) emitters can be waved at pickup sensors to initiate action. Listen Reader [<xref ref-type="bibr" rid="R3" id="75" class="deo:Reference">3</xref>] is an interactive children’s storybook with an evocative soundtrack that the reader “plays” by sweeping hands over the pages. Embedded RFID tags sense what page is open, and capacitive field sensors measure human proximity to the pages. Proximity measurements control volume and other parameters for each page’s sounds. Listen Reader, unlike Augmented Objects, allows users to address the system without using RFID tagged objects or IR emitters. Digital Voices [<xref ref-type="bibr" rid="R19" id="76" class="deo:Reference">19</xref>] is computer-to-computer interaction (CCI) mechanism that uses audible sound as the communication medium. A user can address a suitably equipped system using another Digital Voices-enabled device, as long as the devices can ‘hear’ one another. Moreover, the user hears the communication as it occurs. One problem for sensing input approaches such as these is a risk of failure to communicate with the system if the sensing fails for any reason. The converse problem is avoiding unintended communications with devices that the user does not want to interact with. Simply getting too close can lead to accidental address, and so targets must be well spaced, and use limited sensing ranges or durational thresholds. However, auditory feedback from Digital Voices informs the user which devices are responding and helps them to decide whether the response is appropriate. Accidentally addressing a system could be more than annoying, it could be a serious hazard [<xref ref-type="bibr" rid="R4" id="77" class="deo:Reference">4</xref>]. Potential danger arises when people talk or gesture normally and a system becomes activated unintentionally. For example, a voice activated car phone triggered accidentally could compete for a driver’s attention with serious consequences.</region>
          <outsider class="DoCO:TextBox" type="footer" id="69" page="3" column="2">Volume No. 1, Issue No. 1</outsider>
          <outsider class="DoCO:TextBox" type="page_nr" id="70" page="3" column="2">417</outsider>
          <outsider class="DoCO:TextBox" type="header" id="71" page="4" column="1">Paper: Ubiquity</outsider>
          <outsider class="DoCO:TextBox" type="header" id="72" page="4" column="1">CHI</outsider>
          <outsider class="DoCO:TextBox" type="header" id="73" page="4" column="1">changing the world, changing ourselves</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="79" confidence="possible" page="4" column="1">2. Attention</h2>
          <region class="DoCO:TextChunk" id="86" page="4" column="1">Our second question is related to, but distinct from, the first. The first question focuses only on addressing the system. In addition to this, users must determine whether and when the system is attending to them. Somehow the system must provide cues about attention, analogous to an audience sending signals of their attention (such as gaze and posture) to a human speaker [ <xref ref-type="bibr" rid="R13" id="80" class="deo:Reference">13</xref>].<marker type="column" number="2"/><marker type="block"/> The GUI Solution Mechanisms such as flashing cursors and watch icons, are part of the established genre for communicating whether a system is accepting and responding to input. Such mechanisms assume the user is looking at the display. Exposed Challenges With sensing systems users may well be looking elsewhere than at a display. The design challenges here are: How to embody appropriate feedback so that the user can be aware of the system’s attention. How to direct feedback to zone of user attention. There are inherent problems with sensing UIs. Unobtrusively attached tags and sensors, make it hard for users to distinguish objects that the system is attending to from ones that the system is ignoring (un-augmented objects in the room). Without visible affordances users can unintentionally interact or fail to interact. Further, there could be privacy or security implications from unintended actions such as information being output simply because a user displaces an object and causes a system to become activated. Lack of feedback about system attention is common in many proposed and experimental systems [<xref ref-type="bibr" rid="R6" id="82" class="deo:Reference">6</xref>]. Conference Assistant [<xref ref-type="bibr" rid="R9" id="83" class="deo:Reference">9</xref>] is a system that uses sensing technology to identify a user (or rather, a device they carry) and supply information about the context to that user (such as the current speaker and paper being presented). The system also collects information about the user, including location, session arrival and departure times, and supplies this information to other conference attendees. In this environment, the system is always attending whenever the user is within range. This raises the serious issue of how to keep users aware of what their peers are learning about them. In this design, there is no feedback to users to remind them that their actions are being monitored and recorded; in other words, the system does not provide feedback that it is accepting input from the user. Questions of user privacy have always followed new technologies and will continue to be a tough challenge [<xref ref-type="bibr" rid="R5" id="84" class="deo:Reference">5</xref>]. In contrast to Conference Assistant, EuroPARC’s audio- video media space [<xref ref-type="bibr" rid="R11" id="85" class="deo:Reference">11</xref>] used monitors placed next to cameras in public places to tell inhabitants they were on- camera. In this case, if people saw themselves on the monitor, they could tell that the system was, in some sense, attending to them.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="87" confidence="possible" page="4" column="2">3. Action</h2>
          <region class="DoCO:TextChunk" id="106" page="4" column="2">Even once the user knows how to address the system, and is aware that it is, or is not, attending, more questions remain. The next is about how to effect action: How the user can establish what action she wishes the system to perform, how to control its extent (if it has extent) as well as how to specify (if there are any) targets of that action? In Conversation Analysis, researchers have addressed somewhat similar issues in relation to establishing and maintaining topic [e.g., 7; 25]. Human understanding of <marker type="page" number="5"/><marker type="column" number="1"/><marker type="block"/> what Goffman [<xref ref-type="bibr" rid="R14" id="93" class="deo:Reference">14</xref>] calls a frame, mentioned above, is also relevant, diminishing uncertainty about likely and acceptable actions. We now consider some of the HCI equivalents of these accomplishments. The GUI Solution Graphical items, such as menus, icons, images, text and so on, indicate, in Norman’s Theory of Action terms, what the system is capable of (bridging his ‘Gulf of Execution’). The problem of learning and memorizing how to express a meaningful command to a system (which humans find difficult) is translated into one of choosing from options. Users can explore the UI without changing anything; opening windows, pulling down menus, dragging the scrollbar to inspect contents, and so forth, to get a feel for the range of functionality offered by the application and the objects (such as text or messages) that can be acted on. In Microsoft OutlookTM, for example, a set of menus and toolbars provide access to the functions of the application. These actions can be bound to mail messages and folders, each of which is represented by an item in a list or, alternatively, by an open window. When a message is selected from a list, the user can ascertain which operations are available and which are disallowed for that particular object (disallowed operations are grayed out). In the window view, the set of operations that are allowable for the particular object are grouped together in that window. In most cases, users perform an action on an object by first selecting the object and then selecting which action to apply to it. The patterns exemplified by Outlook are GUI genre conventions common to many graphical applications. Exposed Challenges With sensing systems the major challenges are as follows: How to identify and select a possible object for action. How to identify and select an action, and bind it to the object(s) How to avoid unwanted selection. How to handle complex operations (e.g., multiple objects, actions, and more abstract functions that are difficult to represent graphically, such as save). The first three challenges become apparent as soon as designers attempt to create “invisible interfaces,” in which the UI “disappears” into the environment [<xref ref-type="bibr" rid="R30" id="94" class="deo:Reference">30</xref>]. In such settings the user is not looking at a computer screen, thus genre and conventions cannot be communicated (the user just has to know what to do). How, then, do sensing systems overcome these challenges? Want et al.’s Augmented Objects are tagged so that each one can be permanently bound to a single action that is elicited by waving the object at a sensor. This provides a simple, “unidimensional” input mechanism whereby each object only causes a single action to occur when placed near a particular sensor. The space of possible actions is limited to the “actor” objects present in the environment.<marker type="column" number="2"/><marker type="block"/> The Listen Reader, like the GUI, uses “matrix” input; that is, it combines two kinds of input streams: four proximity sensors combined with an RFID reader. Unique RFID tags are buried within each page, so that the natural action of turning the page triggers the new set of sounds that will be elicited by gestures. The reader doesn’t have to think about selecting new sounds; it’s automatic. In this case, the design is again constrained so that there are no "unwanted selection" or “action binding” issues and the set of possible actions is very small: The range of possible control over the sounds on each page is limited to relative volume, and perhaps pitch shift, but there are no “wrong” responses. This design is aimed at naïve users who will encounter the Listen Reader only once or twice (within what Goffman might call the frame of an exhibition). As long as these objects are distinctive and suggestive of their action (and can be interpreted in terms of the frames for which they are designed), the range of possible actions may be known. Thus Tangible UIs in general [<xref ref-type="bibr" rid="R17" id="96" class="deo:Reference">17</xref>] attempt to use physical traits of an object to communicate its virtual affordances. For example, the physical shape of an object may suggest certain uses for it, certain ways it should be held, and so on. Thus, sensing UIs such as these actually handle the challenges of binding actions to targets, and supporting selection of actions and targets, rather elegantly. By embedding only a limited range of functionality into a set of suggestive physical objects, they provide a natural mechanism for users to bind actions to targets: They simply pick up or gesture at the object(s) of interest. Our question about action here exposes the inherent challenges associated with binding more than limited system actions to physical objects. At the very heart of the vision for Ubicomp, the notion that ”computers [...] vanish into the background” [<xref ref-type="bibr" rid="R30" id="97" class="deo:Reference">30</xref>], lies a serious problem for interaction, which is communicating to the user which objects the potential for possible action is embedded in. Sensor Chair, [<xref ref-type="bibr" rid="R23" id="98" class="deo:Reference">23</xref>] is another gesture-based sound control system. The Sensor Chair was designed for the MIT Media Lab’s ‘Brain Opera.’ Unlike Listen Reader, which is constrained for naïve, one-time users, the Sensor Chair is a musical interface with many layers of complexity and control. It does allow “wrong” responses, typically, an inability to discover effective gestures (to elicit system actions) or a miscalculation of spatial requirements. Systems for expert users, like the Sensor Chair, are difficult to use, require training and often rely on multimodal feedback, such as a variable light indicating strength of signal. Of course, they also support much more complex tasks such as a rich and skillful musical performance. Sensetable [<xref ref-type="bibr" rid="R24" id="99" class="deo:Reference">24</xref>] is a newer Augmented Objects system that, unlike earlier prototypes, is able to support the dynamic binding and unbinding of actions to objects. Sensetable uses augmented ‘pucks’ that are sensed by a tablet surface. Users can assign semantics to the pucks and manipulate<marker type="page" number="6"/><marker type="column" number="1"/><marker type="block"/> them on the table to effect computational actions, for example, by the physical binding of a modifier such as a dial on a puck. The puck may represent something like a molecule and turning the dial represents the action of changing its charge. This is a compelling GUI-Phicon hybrid solution to the challenges related to establishing an action and an object to apply the action to. However, it still leaves open the question of how to apply actions to multiple objects simultaneously. For sensing systems in general a persistent challenge is that abstract operations such as ‘copy’ or ‘find’ are likely to be awkward or severely restricted without some means to specify an argument (e.g., where to copy to and what to save the result as). It may be that such systems simply do not lend themselves to operations that may be best suited to keyboard input. Or it may be that researchers have yet to establish new non-GUI ways to do these things.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="89" page="4" column="2">418</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="90" page="4" column="2">Volume No. 1, Issue No. 1</outsider>
          <outsider class="DoCO:TextBox" type="header" id="91" page="5" column="1">minneapolis, minnesota, usa • 20-25 april 2002</outsider>
          <outsider class="DoCO:TextBox" type="header" id="92" page="5" column="1">Paper: Ubiquity</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="101" page="5" column="2">Volume No. 1, Issue No. 1</outsider>
          <outsider class="DoCO:TextBox" type="page_nr" id="102" page="5" column="2">419</outsider>
          <outsider class="DoCO:TextBox" type="header" id="103" page="6" column="1">Paper: Ubiquity</outsider>
          <outsider class="DoCO:TextBox" type="header" id="104" page="6" column="1">CHI</outsider>
          <outsider class="DoCO:TextBox" type="header" id="105" page="6" column="1">changing the world, changing ourselves</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="107" confidence="possible" page="6" column="1">4. Alignment</h2>
          <region class="DoCO:TextChunk" id="112" page="6" column="1">Sociologists pay a great deal of attention to the mechanisms that support coordination or alignment of speaker and listener as a conversation progresses [ <xref ref-type="bibr" rid="R26" id="108" class="deo:Reference">26</xref>]. Back-channeling is a term used by linguists to refer to feedback a listener gives as to her ongoing understanding, which is monitored by the speaker. Similarly, systems users must be able to monitor system understanding of their input; in other words to bridge Norman’s ‘Gulf of Evaluation.’ The GUI Solution Graphical interfaces display current state, action and results, through feedback mechanisms such as echoing input text and formatting, rubber-banding, wire-frame outlines, progress bars, highlighting changes in a document, listing sent messages and so on. In the rare instances where the system takes the initiative (as in Word’s ‘AutoFormat,’ which monitors user actions and deduces automated formatting), the user sees the results in real time as they work (or don’t, as the case may be). Exposed Challenges The mechanisms above overcome the following challenges: How to make system state perceivable and persistent or query-able. How to direct timely and appropriate feedback. How to provide distinctive feedback on results and state (what is the response). Our first challenge is one of how the user may determine current state. However, by definition, Ubicomp is everywhere, embedded in mundane objects. So the goal of making state perceivable and persistent or query-able seems daunting without something very like a GUI. With Augmented Objects, gestural UIs, ‘sonified’ input- output (I/O) systems like Digital Voices, and other novel sensing systems, the risk is that users will not be able to tell whether the system understands or not what the user is trying to do. Without a GUI equivalent, such as the one provided by Sensetable, how does the user know how the system is responding to their gesture? As long as the failure<marker type="column" number="2"/><marker type="block"/> mode is not problematic, trial and error may be acceptable, but this will certainly restrict the scope of such an interaction style to applications with a more limited space of actions. Augmented Objects, gestural UIs and sonified I/O do not presuppose any mechanism to display state information in a manner that is consistent with the mode of input. With respect to the first and third challenges, if a state change is a part of the function of a system, then these issues must somehow be explicitly addressed. We might propose ongoing projection of graphical or audio information into the space of action. Sensetable takes the former approach, displaying both distinctive and persistent information, however this is presently done at the cost of restricting users to working within the projected area. Ideally, Augmented Objects themselves should be capable of displaying the states they have acquired through action. With respect to the second challenge, Digital Voices has been designed to address the low-end of speed of digital communications, that is, interactions that occur at hundreds of bits per second and that usually take a few seconds to occur. Therefore the timeframe of the machines’ interaction is the same as the people’s timeframe, and the user can perceive the interaction in real-time as it happens. They can also do so without having to watch the system, for example, they might be attending to other matters, thus the audio channel can be an appropriate alternative to visual displays. Likewise, Sensor Chair, in addition to playing sounds in response to user proximity to its sensors, gives additional visual cues, in the form of variable intensity lights. Experts can use this timely feedback to further ensure that the system distinctly senses actions around each of its sensors. As another example of alignment, the Speakeasy framework for Ubicomp [<xref ref-type="bibr" rid="R10" id="110" class="deo:Reference">10</xref>, <xref ref-type="bibr" rid="R20" id="111" class="deo:Reference">20</xref>] provides facilities to query and display the state of devices, such as projectors, PCs and printers, and also services in an environment (such as a lecture theater). Users can discover what these entities are doing, if they’ve failed, if they’re available, and so on.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="113" confidence="possible" page="6" column="2">5. Accident</h2>
          <region class="DoCO:TextChunk" id="127" page="6" column="2">Our final question deals with not only preventing mistakes in the first place, but also informing users about mistakes that have already occurred so they can correct them. Conversation analysts [ <xref ref-type="bibr" rid="R26" id="114" class="deo:Reference">26</xref>, <xref ref-type="bibr" rid="R27" id="115" class="deo:Reference">27</xref>] have dealt extensively with breakdowns and repair in HHI, observing that misunderstandings are much more commonplace than one might expect. Likewise, error is an important and to-be- expected part of normal HCI. Thus, as Norman [<xref ref-type="bibr" rid="R22" id="116" class="deo:Reference">22</xref>] states, “interaction should be treated as a cooperative endeavor between person and machine, one in which misunderstandings can arise on either side.” The GUI Solution Since systems, unlike humans, often perform actions instantaneously, it is not always possible to provide useful feedback that will allow intervention during system action. However, when a user makes a mistake in Word or Outlook<marker type="page" number="7"/><marker type="column" number="1"/><marker type="block"/> text editing, they can usually see the result straight away and applications offer ‘undo’ through a menu item or accelerator key. In Word, certain errors, such as poor speling, can be highlighted or corrected automatically. Many of the actions of the system are also visibly displayed and easily correctable, if the user notices them. These feedback mechanisms occur after the action is completed. Other tasks, such as a long print job or a software compilation, may be long-lived, taking several minutes or even hours to finish. Tools designed to support such work often provide feedback during action to allow users to monitor (and cancel) the task. For example, an animated printer icon on the desktop may show that the printer is working, or has stopped working, and provides controls to allow the user to stop a print job. Some actions, however, are rapid, do not lend themselves well to “preview” feedback, or to easy cancellation, and are inherently undoable. In Outlook it is not possible to retract a message that has been mis-sent (in fact it is very hard to tell this ever happened unless a bounce message arrives). In Word, if the user accidentally saves a document over another document, the application cannot correct the mistake. Experience with such problems means that designers are advised to make risky operations more difficult [<xref ref-type="bibr" rid="R4" id="122" class="deo:Reference">4</xref>, <xref ref-type="bibr" rid="R22" id="123" class="deo:Reference">22</xref>] or to present alert boxes before action to protect users from unrecoverable mistakes; however, alert boxes can be irritating and must be used sparingly. Exposed Challenges How to control or cancel system action in progress How to disambiguate what to undo in time How to intervene when user makes obvious error In order to correct mistakes, they have to be visible in time to take action before it is too late; perhaps during or immediately after a system response, and sometimes even before. Feedback needs to be instantaneous, but without a GUI, ambiguity is a serious problem. Both the action and its object need to be represented in a manner such that they can both be identified and specified as targets for undo. There has been little discussion in the Ubicomp literature so far concerning failure modes and errors. For example, the designers of Augmented Objects [<xref ref-type="bibr" rid="R30" id="124" class="deo:Reference">30</xref>] and Sensetable [<xref ref-type="bibr" rid="R24" id="125" class="deo:Reference">24</xref>] do not even mention the possibility of error! In Listen Reader, a heavily constrained museum piece, error is designed out of the system; the user cannot do anything “wrong.” This is one possible route to go with sensing systems, but it works only in simple interaction situations (e.g., for naive users). More complex systems must allow for error as a trade-off against greater achievement. Digital Voices applications, are appealing in that constant feedback is provided, which should allow the user to cancel an error in progress (for example a transmission that is unwanted). However, it is not clear how users could differentiate system communications that contain erroneous content from correct ones.<marker type="column" number="2"/><marker type="block"/> As things stand in sensing systems, our accident-avoidance challenges, though serious, are largely unaddressed. We believe this is because the field of sensing system research is in its infancy and the existing prototypes have so far been restricted to areas where erroneous behavior has limited consequences. Future, more ambitious systems will most likely need to provide a wide range of mechanisms for dealing with the common problem of error.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="118" page="6" column="2">420</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="119" page="6" column="2">Volume No. 1, Issue No. 1</outsider>
          <outsider class="DoCO:TextBox" type="header" id="120" page="7" column="1">minneapolis, minnesota, usa • 20-25 april 2002</outsider>
          <outsider class="DoCO:TextBox" type="header" id="121" page="7" column="1">Paper: Ubiquity</outsider>
        </section>
      </section>
      <section class="deo:Discussion">
        <h1 class="DoCO:SectionTitle" id="128" page="7" column="2">DISCUSSION</h1>
        <region class="DoCO:TextChunk" id="129" page="7" column="2">We have presented the five questions of our framework for designing interaction with sensing systems. One final and important question is: How do we intend this framework to be used?</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="130" confidence="possible" page="7" column="2">Informing Design</h2>
          <region class="DoCO:TextChunk" id="132" page="7" column="2">We believe that the issues we have raised provide the beginnings for a systematic approach to the design of interactive systems without tried and tested precedents. In particular, we have addressed our arguments to a novel class of systems that obtain user-input through sensing user action, rather than through standard input devices such as the keyboard, mouse or stylus. By considering each of our questions and ensuring they have dealt with the corresponding challenges, designers should be able to avoid a number of potential hazards or pitfalls. As just one example, automobiles are gradually acquiring a growing number of on-board systems such as hands-free phones, navigation and security systems, etc. Looking to the future, we might anticipate a number of problems as the voice channel in the car becomes increasingly overloaded and displays proliferate. Our framework is a starting point for those wishing to find innovative solutions without making hazardous design mistakes [<xref ref-type="bibr" rid="R4" id="131" class="deo:Reference">4</xref>].</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="133" confidence="possible" page="7" column="2">Framing Further Research</h2>
          <region class="DoCO:TextChunk" id="134" page="7" column="2">This paper also represents an invitation to social scientists, in particular, interaction and conversation analysts, to develop and improve on our analysis and to apply their understandings of human-human interaction to help designers develop systems that can communicate more naturally and effectively with people. We are working in a time when systems are rapidly taking on many new forms and functions, faster even than people can find uses for them. With so much design innovation ongoing, there is a wide range of opportunities for social scientists to team up with innovators in academic and commercial research settings to define and refine new mechanisms that will become the conventions of future interaction genres. Our aim here is to open a new discussion on innovative design research for human-machine communication and we look forward to further efforts in this area.</region>
        </section>
      </section>
      <section class="deo:Acknowledgements">
        <h1 class="DoCO:SectionTitle" id="135" page="7" column="2">ACKNOWLEDGMENTS</h1>
        <region class="DoCO:TextChunk" id="136" page="7" column="2">Thanks to Tom Rodden for preliminary communications that inspired some of the thinking behind this paper.</region>
        <outsider class="DoCO:TextBox" type="footer" id="137" page="7" column="2">Volume No. 1, Issue No. 1</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="138" page="7" column="2">421</outsider>
        <outsider class="DoCO:TextBox" type="header" id="139" page="8" column="1">Paper: Ubiquity</outsider>
        <outsider class="DoCO:TextBox" type="header" id="140" page="8" column="1">CHI</outsider>
        <outsider class="DoCO:TextBox" type="header" id="141" page="8" column="1">changing the world, changing ourselves</outsider>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="142" page="8" column="1">REFERENCES</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="143" page="8" column="1">1. Abowd, G. &amp; Mynatt, E. Charting past, present, and future research in ubiquitous computing. ACM Trans. Computer-Human. Interaction. 7, 1 (Mar. 2000), 29-58.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="144" page="8" column="1">2. Apple Computer, Inc. Macintosh Human Interface Guidelines. Reading, MA: Addison-Wesley Publishing Co., 1992. ISBN 0-201-62216-5.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="145" page="8" column="1">3. Back, M., Cohen, J., Gold, R., Harrison, S., &amp; Minneman, S. Listen Reader: An electronically augmented paper-based book. Proceedings of CHI 2001 (Seattle WA, April 2000) ACM Press, 23-29.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="146" page="8" column="1">4. Barfield, L. The Real World: Powerful Functions. SIGCHI Bulletin, 29, 2, April, 1997.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="147" page="8" column="1">5. Bellotti, V. Design for Privacy in Multimedia Computing and Communications Environments. In P. Agre, &amp; M. Rotenberg (eds.). Technology and Privacy: The New Landscape. MIT Press: Cambridge (1998).</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="148" page="8" column="1">6. Bellotti, V &amp; Rodden T. The Perils of Physical Interaction. In Proceedings of W. Mackay (ed.) DARE 2000, Designing Augmented Reality Environments, (Elsinore, Denmark, April 2000) ACM Press.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="149" page="8" column="1">7. Button, G. (ed.). Technology in Working Order. Routledge, London, UK (1993).</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="150" page="8" column="1">8. Button, G., &amp; Casey, N. Generating topic: The use of topic initial elicitors. In J. Atkinson &amp; J. Heritage (eds.) Structures of Social Action. Studies in Conversation Analysis. Cambridge University Press, (1984) 167-189.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="151" page="8" column="1">9. Dey, A., Salber, D, &amp; Abowd, G. A Conceptual Framework and a Toolkit for Supporting the Design of Context Aware Applications. HCI Journal, Special Issue on Context Aware Computing, (in press).</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="152" page="8" column="1">10. Edwards, W.K., Newman, M.W., Sedivy, J.Z. The Case for Recombinant Computing. Xerox PARC Technical Report CSL-01-1. April 2001.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="153" page="8" column="1">11. Gaver, W., Moran, T., MacLean, A., Lovstrand, L., Dourish, P., Carter, K., &amp; Buxton, W. Realizing a Video Environment: EuroPARC’s RAVE System. Proceedings of CHI’92 (Monterey, CA, May 1992). ACM Press 27-35.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="154" page="8" column="1">12. Goffman, E. The Presentation of Self in Everyday Life. Anchor, Doubleday, New York, (1959).</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="155" page="8" column="1">13. Goffman, E. Behavior in Public Places. Free Press, Macmillan, Boston, (1963).</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="156" page="8" column="1">14. Goffman, E. Frame Analysis: An essay on the organization of experience. Northeastern University Press, New York, (1974).</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="157" page="8" column="1">15. Goffman, E. Forms of Talk. Oxford: Basil Blackwell, (1981).</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="158" page="8" column="1">16. Hughes, J., King, V., Rodden, T. &amp; Andersen H. The Role of Ethnography in Interactive Systems Design, Interactions, 2,2. April, 1995, 56-65.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="159" page="8" column="2">17. Ishii, H. &amp; Ullmer, B. Tangible Bits: Towards seamless interfaces between people, bits and atoms. Proceedings of CHI’97 (Atlanta, GA, March 1997) ACM Press, 234- 241.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="160" page="8" column="2">18. Long, S., Aust, D., Abowd, G. &amp; Atkeson, C. Cyberguide: prototyping context-aware mobile applications. Proceedings of CHI’96 (Vancouver, Canada, April, 1996) ACM Press, 293-294.</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="161" page="8" column="2">19. Lopes, C. &amp; Aguiar, P. Aerial Acoustic Communications. Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (Mohonk Moutain Resort, New York. October 2001).</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="162" page="8" column="2">20. Newman, M., Sedivy, J., Edwards, W.K., Hong, J., Izadi, S., Marcelo, K., Neuwirth, C., &amp; Smith, T. Designing for Radical Interoperability: User Experience Issues in Recombinant Computing. (Submitted for publication).</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="163" page="8" column="2">21. Newman, W. &amp; Lamming, M. Interactive System Design. Addison Wesley, (1995).</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="164" page="8" column="2">22. Norman, D. A. The Design of Everyday Things. Doubleday: New York, New York, (1990).</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="165" page="8" column="2">23. Paradiso, Joseph. The Brain Opera Technology: New Instruments and Gestural Sensors for Musical Interaction and Performance. Journal of New Music Research, 28, 2, (1999), 130-149.</ref>
          <ref rid="R24" class="deo:BibliographicReference" id="166" page="8" column="2">24. Patten, J., Ishii, H., Hines, J. &amp; Pangaro, G. Sensetable: A Wireless Object Tracking Platform for Tangible User Interfaces, Proceedings of CHI’01 (Seattle, WA, April 2001), ACM Press, 253-260.</ref>
          <ref rid="R25" class="deo:BibliographicReference" id="167" page="8" column="2">25. Sacks, H., Schegloff, E. &amp; Jefferson G. A simplest systematics for the organisation of turn-taking for conversation", Language 50, (1974) 696-735.</ref>
          <ref rid="R26" class="deo:BibliographicReference" id="168" page="8" column="2">26. Sacks, H., Schegloff, E. &amp; Jefferson G. Lectures on conversation. Oxford: Basil Blackwell (1992).</ref>
          <ref rid="R27" class="deo:BibliographicReference" id="169" page="8" column="2">27. Schegloff, E., Jefferson, G., &amp; Sacks, H. The preference for self-correction in the organization of repair in conversation. Language, 53, (1977), 361-382.</ref>
          <ref rid="R28" class="deo:BibliographicReference" id="170" page="8" column="2">28. Schmandt, C. Conversational Computing Systems. New York, Van Nostrand Reinhold, 1993.</ref>
          <ref rid="R29" class="deo:BibliographicReference" id="171" page="8" column="2">29. Want, R., Fishkin, K., Gujar A, &amp; Harrison, B. Bridging physical and virtual worlds with electronic tags. Proceeding of CHI’99 (Pittsburgh, PA, May 1999). ACM Press, 370-377.</ref>
          <ref rid="R30" class="deo:BibliographicReference" id="172" page="8" column="2">30. Weiser, M. The Computer for the Twenty-First Century. Scientific American, 265, 3 (September 1991), 94-104.</ref>
          <ref rid="R31" class="deo:BibliographicReference" id="173" page="8" column="2">31. Zimmerman, T., Smith, J.R., Paradiso, J.A., Allport, D. and Gershenfeld, N. (1995). Applying Electric Field Sensing to Human-Computer Interfaces. Proceedings of CHI’95, (Denver, CO, May 1995) ACM Press. 280-287.</ref>
        </ref-list>
        <outsider class="DoCO:TextBox" type="page_nr" id="174" page="8" column="2">422</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="175" page="8" column="2">Volume No. 1, Issue No. 1</outsider>
      </section>
    </body>
  </article>
</pdfx>
