<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>9b9aaf9c93fc3bb21722035c4ee3496d64604586abdca5816ba6394958687dcc</job>
    <base_name>62oo</base_name>
    <doi>http://dx.doi.org/10.1145/2470654.2466255</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="header" id="1">Session: Design Research</outsider>
      <outsider class="DoCO:TextBox" type="header" id="2">CHI 2013: Changing Perspectives, Paris, France</outsider>
      <title-group>
        <article-title class="DoCO:Title" id="3">Crossing the Bridge over Norman’s Gulf of Execution: Revealing Feedforward’s True Identity</article-title>
      </title-group>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="4">Jo Vermeulen 1 Kris Luyten 1 Elise van den Hoven</name>
          <aff id="5">1</aff>
        </contrib>
        <contrib contrib-type="author">
          <name id="6">Karin Coninx</name>
        </contrib>
      </contrib-group>
      <footnote class="DoCO:Footnote" id="8">1 Hasselt University – tUL – iMinds, Expertise Centre for Digital Media, Diepenbeek, Belgium 2 University of Technology, Sydney, Australia, Design, Architecture &amp; Building Faculty 3 Eindhoven University of Technology, Eindhoven, The Netherlands, Industrial Design Department 1 { jo.vermeulen, kris.luyten, karin.coninx } @uhasselt.be 2 <email id="7">elise.vandenhoven@uts.edu.au</email></footnote>
      <footnote class="DoCO:Footnote" id="16" page="1" column="1">1</footnote>
      <footnote class="DoCO:Footnote" id="20" page="1" column="1">1 Norman later argued for replacing the term “perceived affordances” with “signifiers” to avoid confusion [<xref ref-type="bibr" rid="R24" id="19" class="deo:Reference">24</xref>]. However, for historical relevance and to accurately reflect what others have written about the relation between feedforward and perceived affordances, we will continue to use “perceived affordance” throughout this paper. All mentions of this term can, however, be replaced with “signifier”.</footnote>
      <abstract class="DoCO:Abstract" id="9">Feedback and affordances are two of the most well-known principles in interaction design. Unfortunately, the related and equally important notion of feedforward has not been given as much consideration. Nevertheless, feedforward is a powerful design principle for bridging Norman’s Gulf of Execution. We reframe feedforward by disambiguating it from related design principles such as feedback and perceived affordances, and identify new classes of feedforward. In addition, we present a reference framework that provides a means for designers to explore and recognize different opportunities for feedforward.</abstract>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="10" page="1" column="1">ACM</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="11" confidence="possible" page="1" column="1">Classification Keywords</h2>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="13" confidence="possible" page="1" column="1">Author Keywords</h2>
        </section>
      </section>
      <region class="DoCO:TextChunk" id="12" page="1" column="1">H.5.2 Information Interfaces and Presentation: User Interfaces</region>
      <region class="DoCO:TextChunk" id="14" confidence="possible" page="1" column="1">feedforward; affordances; feedback; design; theory</region>
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="15" page="1" column="1">INTRODUCTION</h1>
      </section>
      <region class="DoCO:TextChunk" id="43" page="1" column="1">The notions of feedback and (perceived) affordances [ <xref ref-type="bibr" rid="R22" id="17" class="deo:Reference">22</xref>] have emerged over the past few decades as core concepts in interaction design. While these principles are generally recognized and well-known among designers, the same is not true for the related notion of feedforward. The term was first<marker type="column" number="2"/><marker type="block"/> introduced in this context in the HCI community by Djajadiningrat et al. in 2002 [<xref ref-type="bibr" rid="R6" id="22" class="deo:Reference">6</xref>]. Well-designed feedforward is an effective tool for bridging Norman’s Gulf of Execution [<xref ref-type="bibr" rid="R22" id="23" class="deo:Reference">22</xref>] – the difference between the user’s intentions and the allowable actions – as it tells users what the result of their action will be. This is illustrated in <xref ref-type="fig" rid="F1" id="24" class="deo:Reference">Figure 1</xref>. A simple example of feedforward is a label on a button: the label tells users what happens if they push the button. For instance, the design of the iPhone’s lock screen combines perceived affordances and feedforward to tell users how they can unlock the phone, as shown in <xref ref-type="fig" rid="F2" id="25" class="deo:Reference">Figure 2</xref>. However, feedforward also exists in many other forms and guises.<marker type="block"/> Even though few designers are familiar with the term feedforward, almost every designer has – in one way or another – already used feedforward in their designs. Yet, opportunities for feedforward are often left unexplored because designers are not fully aware of this design aspect. An important factor causing this limited awareness is the lack of a well-defined and generally accepted meaning of feedforward. Additionally, there is no existing library of examples and proven so-<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> lutions for applying feedforward when designing new interfaces. With this paper, we want to define feedforward more clearly. In addition, we provide a reference framework for designers that differentiates feedforward from other design principles such as affordances and feedback. As our everyday environments and devices become smarter and more sophisticated, it will become more difficult to convey to users what tasks are supported, what can be accom- plished and how users can achieve their goals. Bellotti et al. [<xref ref-type="bibr" rid="R2" id="37" class="deo:Reference">2</xref>] argue that, unlike designers of GUI interfaces, designers of “sensing systems” – which include context-aware systems, gestural interfaces, tangible interaction and post-WIMP user interfaces [<xref ref-type="bibr" rid="R26" id="38" class="deo:Reference">26</xref>] in general – have few pre-packaged answers available to solve basic interaction design challenges. For example, it has been suggested that context-aware systems should provide support for intelligibility [<xref ref-type="bibr" rid="R3" id="39" class="deo:Reference">3</xref>] – presenting to users what the systems know, how they know it, and what they are doing. Similarly, while one of the strengths of tangible interaction is the fact that physical objects can communicate their purpose through their form and might therefore be easier for users to understand, Hornecker and Buur [<xref ref-type="bibr" rid="R14" id="40" class="deo:Reference">14</xref>] warn against these simple, direct associations. They argue that if tangible interaction is to become useful for complex domains and to scale up to real-world size examples, balancing legibil- ity and computational power is one of the grand challenges in the field. We believe that feedforward is an important design concept to consider for such systems which will help users in knowing what they can expect and thereby bridge Norman’s Gulf of Execution. As recently argued by Norman [<xref ref-type="bibr" rid="R21" id="41" class="deo:Reference">21</xref>], we should not try to avoid complexity, but rather tame complexity through good design. In summary, the contributions we present in this paper are twofold: • We use Hartson’s four types of affordances [<xref ref-type="bibr" rid="R12" id="42" class="deo:Reference">12</xref>] to distinguish feedforward from related design principles such as feedback and affordances and clarify their relationship. By doing so, we raise awareness of feedforward and clear up some of the confusion surrounding the term. • We describe a reference framework for designers that allows them to explore and recognize different opportunities for feedforward.</region>
      <region class="unknown" id="21" page="1" column="1">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CHI 2013, April 27–May 2, 2013, Paris, France. Copyright © 2013 ACM 978-1-4503-1899-0/13/04...$15.00.</region>
      <region class="DoCO:FigureBox" id="F1">
        <image class="DoCO:Figure" src="62oo.page_001.image_01.png" thmb="62oo.page_001.image_01-thumb.png"/>
        <caption class="deo:Caption" id="30" page="1" column="2">Figure 1. The position of perceived affordances (or signifiers [<xref ref-type="bibr" rid="R24" id="28" class="deo:Reference">24</xref>]), feedforward and feedback in Norman’s Stages of Action model (image based on [<xref ref-type="bibr" rid="R22" id="29" class="deo:Reference">22</xref>]).</caption>
      </region>
      <outsider class="DoCO:TextBox" type="page_nr" id="32" page="1" column="2">1931</outsider>
      <outsider class="DoCO:TextBox" type="header" id="33" page="2" column="1">Session: Design Research</outsider>
      <outsider class="DoCO:TextBox" type="header" id="34" page="2" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
      <region class="DoCO:FigureBox" id="F2">
        <image class="DoCO:Figure" src="62oo.page_002.image_02.png" thmb="62oo.page_002.image_02-thumb.png"/>
        <caption class="deo:Caption" id="36" page="2" column="1">Figure 2. A well-designed combination of feedforward and perceived affordances in the iPhone lock screen. The “slide to unlock” label, the slider and its button with an arrow icon, and the subtle animation of light that moves over the slider indicates what users can do to unlock the phone. Feedforward tells users what they can expect when they execute an action – in this case, that the phone will be unlocked when they move the slider to the right. Given their desired goal (unlocking the phone so that it can be used), they can select the appropriate action corresponding to that goal.</caption>
      </region>
      <section class="deo:Background">
        <h1 class="DoCO:SectionTitle" id="44" page="2" column="2">BACKGROUND</h1>
        <region class="DoCO:TextChunk" id="57" page="2" column="2">Although the term feedforward has been used in other domains such as control theory (e.g., [ <xref ref-type="bibr" rid="R20" id="45" class="deo:Reference">20</xref>]), this paper focuses on the usage of the term feedforward in the context of interaction design. The first definition of feedforward in this context was given by Djajadiningrat et al. [6, p. 285]. They have defined feedforward by disambiguating it from related concepts such as feedback and perceived affordances [emphasis ours]: We distinguish between information before the user carries out the action (pre-action), and after the user carries out the action (post-action). These phases correspond with feedforward and feedback. Feedforward informs the user about what the result of his action will be. Invit- ing the appropriate action is a prerequisite for feedforward but it is not sufficient. The product also needs to communicate what the user can expect. Feedback informs the user about the action that is carried out, shows that the product is responding, indicates progress, confirms navigation, etc. Note that, unlike feedforward, perceived affordances do not communicate the purpose of an action. We can situate perceived affordances, feedforward and feedback within Norman’s Stages of Action model [<xref ref-type="bibr" rid="R22" id="46" class="deo:Reference">22</xref>], as shown in <xref ref-type="fig" rid="F1" id="47" class="deo:Reference">Figure 1</xref>. Feedback bridges Norman’s Gulf of Evaluation [<xref ref-type="bibr" rid="R22" id="48" class="deo:Reference">22</xref>] – the amount of effort users must exert to interpret the state of the system and to determine how well the ex- pectations and intentions have been met – by helping users evaluate the state of the system. When evaluating the state of the world, users go through the Stages of Evaluation shown on the right side of <xref ref-type="fig" rid="F1" id="49" class="deo:Reference">Figure 1</xref>. Feedforward, on the other hand, bridges Norman’s Gulf of Execution [<xref ref-type="bibr" rid="R22" id="50" class="deo:Reference">22</xref>] – the difference between the user’s intentions and the allowable actions – by helping users decide what action to take based on that action’s expected outcome. When users act on a certain goal, they go through the Stages of Execution seen on the left side of <xref ref-type="fig" rid="F1" id="51" class="deo:Reference">Figure 1</xref>. Perceived affordances are also used for bridging Norman’s Gulf of Execution, but serve a different purpose: they suggest a particular action to users, such as pressing a button, or turning a knob. It has been argued that Norman’s perceived affordances can be viewed in the context of semiotics [<xref ref-type="bibr" rid="R4" id="52" class="deo:Reference">4</xref>]: an object’s appearance (e.g., the shape of a button) signifies to the user that this object can be pressed. A similar argument could be made for feedforward in the sense that it signifies to the user what they can expect when performing a certain action.<marker type="page" number="3"/><marker type="column" number="1"/><marker type="block"/> In the remainder of this paper we will explore feedforward in depth. We start with current use of the term in practice which is at times inconsistent and thus illustrates the need for a clear and generally accepted definition. Next, we discuss other definitions of feedforward to develop our own reframing of feedforward in which we further detail its relation with perceived affordances and feedback. Finally, we give an overview of what the different definitions cover in terms of feedforward and analyze a few notable examples of feedforward.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="54" page="2" column="2">1932</outsider>
        <outsider class="DoCO:TextBox" type="header" id="55" page="3" column="1">Session: Design Research</outsider>
        <outsider class="DoCO:TextBox" type="header" id="56" page="3" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="58" page="3" column="1">USE OF FEEDFORWARD</h1>
        <region class="DoCO:TextChunk" id="85" page="3" column="1">Djajadiningrat et al. [ <xref ref-type="bibr" rid="R6" id="59" class="deo:Reference">6</xref>] mostly focus on the importance of feedforward for tangible interaction. However, there have also been other application domains in which feedforward has been successfully applied, such as gestural interaction. A common problem of gestural interfaces is their lack of visibility: users lack awareness of the available gestures that are recognized by the system, and what these gestures do. Feedforward can help users in performing the correct gesture by telling them what will happen when a certain gesture is invoked. An early example of the use of feedforward in gestural interaction is Kurtenbach et al.’s concept of marking menus [<xref ref-type="bibr" rid="R17" id="60" class="deo:Reference">17</xref>]. Marking menus are pie menus – circular menu’s that support gestural interaction as shown in <xref ref-type="fig" rid="F3(a)" id="61" class="deo:Reference">Figure 3(a)</xref> – in- tended to accommodate both novice and expert users. They allow a user to perform a menu selection by either popping- up a pie menu, or by making a straight mark in the direction of the desired menu item without popping-up the menu. The pie menu serves as a feedforward display that helps novice users who hesitate when they are unsure of a gesture of command, as shown in <xref ref-type="fig" rid="F3(a)" id="62" class="deo:Reference">Figure 3(a)</xref>. When users become more experienced, they tend to use marks more, although they still look at the feedforward display once in a while to refresh their memory [<xref ref-type="bibr" rid="R16" id="63" class="deo:Reference">16</xref>]. Note that marking menus show both the available gestures (or actions) and what users can expect when they perform one of these gestures.<marker type="block"/> Bau and Mackay developed OctoPocus [<xref ref-type="bibr" rid="R1" id="70" class="deo:Reference">1</xref>], a dynamic guide that combines on-screen feedforward and feedback to help users learn, remember and execute gestures. They state that “feedforward mechanisms provide information about a gestures shape and its association with a particular command, prior to the execution or completion of the gesture.” [<xref ref-type="bibr" rid="R1" id="71" class="deo:Reference">1</xref>]. Like marking menus, OctoPocus takes advantage of possible hes- itation by appearing after a “press and wait gesture”. The<marker type="column" number="2"/><marker type="block"/> set of possible gestures and associated commands are continuously updated while the user is performing a certain gesture (<xref ref-type="fig" rid="F3(b)" id="73" class="deo:Reference">Figure 3(b)</xref>). Another example of feedforward for gestural interaction is ShadowGuides [<xref ref-type="bibr" rid="R8" id="74" class="deo:Reference">8</xref>], which extends Bau and Mackay’s concept of dynamic guides to multi-touch and whole-hand gestures. Additionally, feedforward has also been deemed important for context-aware computing. Context-aware systems [<xref ref-type="bibr" rid="R5" id="75" class="deo:Reference">5</xref>] typically act based on implicit input collected from the environment. Additionally, system actions are usually a result of complex reasoning about context data which might be hard for users to understand and makes the system un- predictable [<xref ref-type="bibr" rid="R3" id="76" class="deo:Reference">3</xref>]. Bellotti and Edwards [<xref ref-type="bibr" rid="R3" id="77" class="deo:Reference">3</xref>] argue that context- aware systems should be intelligible and inform end-users of their capabilities and current understanding. One of the proposed design principles for realizing this is feedback which includes feedforward – an answer to the question “What will happen if I do this?” [<xref ref-type="bibr" rid="R3" id="78" class="deo:Reference">3</xref>]. Although Bellotti and Edwards ad- here to Djajadiningrat et al.’s [<xref ref-type="bibr" rid="R6" id="79" class="deo:Reference">6</xref>] basic idea of feedforward – communicating the purpose or result of an action – they view it as a form of feedback, which is not in line with Djajadiningrat et al.’s ideas. They also provide examples of feedforward in WIMP GUIs that are often taken for granted, but as they argue, are necessary components of the interface that help users know what will happen when a certain action is performed: flashing insertion points; cursors, pointers and handles; window highlighting; and rubberbanding. Lim and Dey have developed a toolkit [<xref ref-type="bibr" rid="R18" id="80" class="deo:Reference">18</xref>] to support “What if?” questions in context-aware systems. This can be seen as an example of the kind of feedforward that Bellotti and Edwards refer to. By comparing Djajadiningrat et al.’s [<xref ref-type="bibr" rid="R6" id="81" class="deo:Reference">6</xref>] original definition of feedforward to the interpretations by Bellotti and Edwards [<xref ref-type="bibr" rid="R3" id="82" class="deo:Reference">3</xref>] and by Bau and Mackay [<xref ref-type="bibr" rid="R1" id="83" class="deo:Reference">1</xref>], it is evident that the term feedforward has not always been used consistently. Different communities seem to interpret the concept in a different way. There are only a handful of HCI textbooks that talk explicitly about feedforward. One of them is “Designing for Interac- tion” [<xref ref-type="bibr" rid="R25" id="84" class="deo:Reference">25</xref>] by Dan Saffer, which refers to the definition by Djajadiningrat. Saffer argues that designers should look out for opportunities to use feedforward – even though he states that it is harder to design into products and services than feedback [25, p. 133].</region>
        <region class="DoCO:FigureBox" id="Fx65">
          <image class="DoCO:Figure" src="62oo.page_003.image_03.png" thmb="62oo.page_003.image_03-thumb.png"/>
        </region>
        <region class="unknown" id="66" page="3" column="1">(a) Marking Menus (b) OctoPocus</region>
        <region class="DoCO:FigureBox" id="F3">
          <caption class="deo:Caption" id="69" page="3" column="1">Figure 3. Examples of feedforward in gestural interaction (images based on [<xref ref-type="bibr" rid="R17" id="67" class="deo:Reference">17</xref>] and [<xref ref-type="bibr" rid="R1" id="68" class="deo:Reference">1</xref>]).</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="86" page="3" column="2">FEEDFORWARD DEFINITIONS</h1>
        <region class="DoCO:TextChunk" id="90" page="3" column="2">In this section, we outline the differences between existing definitions of feedforward. This overview will provide a thor- ough review on the notion of feedforward and its relation with affordances. The <xref ref-type="table" rid="Tin" id="87" class="deo:Reference">table in</xref> <xref ref-type="fig" rid="F4" id="88" class="deo:Reference">Figure 4</xref> lists different definitions and examples of feedforward alongside a number of important dimensions. We will discuss this <xref ref-type="table" rid="Tin" id="89" class="deo:Reference">table in</xref> more detail later.</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="91" confidence="possible" page="3" column="2">Djajadiningrat – Going Beyond Affordances</h2>
          <region class="DoCO:TextChunk" id="106" page="3" column="2">As previously mentioned, Djajadiningrat et al. [ <xref ref-type="bibr" rid="R6" id="92" class="deo:Reference">6</xref>] have defined feedforward by contrasting it with feedback and affordances. Feedback is one of the most well-known design principles in interaction design, along with affordances, visibil-<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> ity, constraints and mappings. Feedback is a message about whether or not a goal was achieved or maintained [<xref ref-type="bibr" rid="R25" id="99" class="deo:Reference">25</xref>] and is typically used to inform the user that the system is responding, to indicate progress or to confirm navigation [<xref ref-type="bibr" rid="R3" id="100" class="deo:Reference">3</xref>]. Djajadiningrat et al. [<xref ref-type="bibr" rid="R6" id="101" class="deo:Reference">6</xref>] state that feedforward, like feedback, returns information about the result of a process or activity. However, while feedback is communicated during or after the action, feedforward is information that is offered before the action takes place. Whereas feedback informs the user about the action that is carried out, feedforward informs the user about what the result of their action will be. Next to feedforward, affordances also provide information to users before they carry out an action. Gibson [<xref ref-type="bibr" rid="R10" id="102" class="deo:Reference">10</xref>, <xref ref-type="bibr" rid="R11" id="103" class="deo:Reference">11</xref>] defined affordances as: All “action possibilities” latent in the environment, ob- jectively measurable and independent of the individual’s ability to recognize them, but always in relation to the actor and therefore dependent on their capabilities. As introduced in the HCI literature by Norman [<xref ref-type="bibr" rid="R22" id="104" class="deo:Reference">22</xref>], perceived affordances (affordances that the user is made aware of through good design) essentially invite the user to a particular action. Affordances therefore “suggest” how one can in- teract with a product or system. Typical examples in HCI are buttons which “afford” pushing, knobs which “afford” turning, or sliders which “afford” moving up and down (or left and right, depending on the orientation). Affording the right actions has been widely regarded as a crucial aspect of usability. Even though perceived affordances are very useful, Djajadiningrat et al. [<xref ref-type="bibr" rid="R6" id="105" class="deo:Reference">6</xref>] argue that inviting the appropriate action is a prerequisite for feedforward, but it is not sufficient. They state that the essence of usability lies not in communicating the necessary action, but the purpose of an action.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="94" page="3" column="2">1933</outsider>
          <outsider class="DoCO:TextBox" type="header" id="95" page="4" column="1">Session: Design Research</outsider>
          <outsider class="DoCO:TextBox" type="header" id="96" page="4" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
          <region class="unknown" id="97" page="4" column="1">Modality Visual Tactile Aural High Definitions Djajadiningrat ! ! Wensveen ! ! ! Gaver / McGrenere and Ho ! ! ! Hartson ! Norman ! Bau and Mackay ! Examples OctoPocus / ShadowGuides ! SpeakCup ! Disney Pixar Cars 2 AppMATes ! Tooltips ! Tangible Video Editor ! TempSticks ! !</region>
          <region class="DoCO:FigureBox" id="F4">
            <caption class="deo:Caption" id="98" page="4" column="1">Figure 4. Summary of the coverage of the feedforward definitions, their differences and an analysis of several feedforward examples in practice.</caption>
          </region>
          <region class="unknown" id="107" page="4" column="2">Detail Hierarchy Time Sequential Average Low Nested Static Discrete Continuous ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="108" confidence="possible" page="4" column="2">Wensveen – Inherent, Augmented &amp; Functional Feedforward</h2>
          <region class="DoCO:TextChunk" id="110" page="4" column="2">Wensveen, Djajadiningrat and Overbeeke [<xref ref-type="bibr" rid="R29" id="109" class="deo:Reference">29</xref>] further elab- orated on their previous definition and distinguish between three different types of feedforward, based on the “form of information” that the user receives about their action: inherent, augmented and functional feedforward.</region>
          <region class="DoCO:FigureBox" id="F5">
            <image class="DoCO:Figure" src="62oo.page_004.image_04.png" thmb="62oo.page_004.image_04-thumb.png"/>
            <caption class="deo:Caption" id="112" page="4" column="2">Figure 5.</caption>
          </region>
          <region class="DoCO:TextChunk" id="114" confidence="possible" page="4" column="2">(a) Inherent (b) Augmented (c) Functional Wensveen’s three types of feedforward. Images from [<xref ref-type="bibr" rid="R27" id="113" class="deo:Reference">27</xref>] reused with permission (Copyright c 2005 Stephan Wensveen).</region>
          <region class="DoCO:TextChunk" id="134" page="4" column="2">Inherent feedforward offers information related to the action possibilities of the product and appeals primarily to the perceptual motor skills of the person. Inherent feedforward communicates what kind of action is possible (e.g., pushing, sliding, rolling; see <xref ref-type="fig" rid="F5(a" id="115" class="deo:Reference">Figure 5(a)</xref>) and how this action can be carried out (the amount of force required, body parts, etc.). Wensveen [<xref ref-type="bibr" rid="R27" id="116" class="deo:Reference">27</xref>] states that inherent feedforward can be viewed as a limited interpretation of Gibson’s affordances [<xref ref-type="bibr" rid="R10" id="117" class="deo:Reference">10</xref>]. Note that Wensveen does not contradict Djajadiningrat et al. [<xref ref-type="bibr" rid="R6" id="118" class="deo:Reference">6</xref>],<marker type="page" number="5"/><marker type="column" number="1"/><marker type="block"/> who did not yet differentiate between different types of feedforward. Feedforward as a whole – the combination of inherent, augmented and functional feedforward – still goes beyond affordances, according to Wensveen [<xref ref-type="bibr" rid="R28" id="123" class="deo:Reference">28</xref>]. The differen- tiation between the three different types of feedforward was mainly made for analysis purposes [<xref ref-type="bibr" rid="R28" id="124" class="deo:Reference">28</xref>]. Augmented feedforward is information from an additional source about the action possibilities of a product or system, or the purpose of these action possibilities [<xref ref-type="bibr" rid="R27" id="125" class="deo:Reference">27</xref>]. This type of feedforward appeals primarily to the cognitive skills of users. <xref ref-type="fig" rid="F5(b)" id="126" class="deo:Reference">Figure 5(b)</xref> shows examples of augmented feedforward, such as on-screen messages indicating what to do (i.e., conveying the action possibilities) and lexical or graphical labels communicating the purpose of the action possibility. Functional feedforward goes beyond the action possibilities and their specific purpose and instead informs the user about the more general purpose of a product and its functional features [<xref ref-type="bibr" rid="R27" id="127" class="deo:Reference">27</xref>]. A possible strategy for functional feedforward is making the functional parts visible (i.e., corresponding to Norman’s notion of visibility [<xref ref-type="bibr" rid="R22" id="128" class="deo:Reference">22</xref>]), as illustrated for example by the candy vending machine in <xref ref-type="fig" rid="F5(c)" id="129" class="deo:Reference">Figure 5(c)</xref> (top), where the available types of candy and the mechanism which delivers the products to the user are clearly visible. Even though the above-mentioned definitions distinguish between feedforward and affordances, there have been a number of frameworks for affordances (e.g., [<xref ref-type="bibr" rid="R9" id="130" class="deo:Reference">9</xref>, <xref ref-type="bibr" rid="R12" id="131" class="deo:Reference">12</xref>, <xref ref-type="bibr" rid="R15" id="132" class="deo:Reference">15</xref>, <xref ref-type="bibr" rid="R19" id="133" class="deo:Reference">19</xref>]) that in- cluded aspects of feedforward without explicitly mentioning the term, further adding to the confusion. In the next sec- tions, we give an overview of these frameworks and explain how they relate to feedforward.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="120" page="4" column="2">1934</outsider>
          <outsider class="DoCO:TextBox" type="header" id="121" page="5" column="1">Session: Design Research</outsider>
          <outsider class="DoCO:TextBox" type="header" id="122" page="5" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="135" confidence="possible" page="5" column="1">Gaver – Technology Affordances</h2>
          <region class="DoCO:TextChunk" id="147" page="5" column="1">In his paper titled “Technology Affordances”, Gaver [ <xref ref-type="bibr" rid="R9" id="136" class="deo:Reference">9</xref>] argues that affordances are not always single, independent en- tities, but can be related to one another. He describes two different relationships between affordances: nesting and se- quence. Nested affordances are grouped in space, while sequential affordances are sequential in time (i.e., acting on an affordance leads to information indicating new affordances). Gaver argues that affordances are not passively perceived, but explored [<xref ref-type="bibr" rid="R9" id="137" class="deo:Reference">9</xref>]. He also hints at the possibility of conveying affordances through different modalities (e.g., sound, tactile information). Nested affordances, in particular, bear resemblance to feedforward. McGrenere and Ho [<xref ref-type="bibr" rid="R19" id="138" class="deo:Reference">19</xref>] have analyzed Gaver’s work, and clarify nested affordances with the example of a button. They state that users are not interested in clicking on a button for its own sake, but are interested in invoking a certain function. The function that will be invoked by a button is usually specified through its label or icon. They explain that here the affordance of “button clickability” is nested within the affordance of “function invokability”. McGrenere and Ho stress that it is important to acknowledge that each of the levels of the affordance hierarchy may or may not map to system functions. They further argue that affordances are not limited to physical aspects of the system (e.g., physical interaction with a mouse, keyboard or screen), as implied by Norman in<marker type="column" number="2"/><marker type="block"/> his clarification of the use of the term affordances [<xref ref-type="bibr" rid="R23" id="140" class="deo:Reference">23</xref>]. They state that application software also provides possible actions. For example, a word processor affords writing and editing at a high level , but also actions such as clicking and scrolling. Given Gaver’s extension of nested affordances, Wensveen’s functional feedforward [<xref ref-type="bibr" rid="R27" id="141" class="deo:Reference">27</xref>] could be seen as a perceptible affordance [<xref ref-type="bibr" rid="R9" id="142" class="deo:Reference">9</xref>] which conveys the general (top-level) function of a system – or what the system affords the user. This top-level affordance can be seen as the root of a hierarchy of affordances. As an example, in <xref ref-type="fig" rid="F5(c)" id="143" class="deo:Reference">Figure 5(c)</xref> (bottom), the shape of the voice recorder conveys its general function to the user. However, to actually record speech, users will also have to be aware of the nested functions and affordances for these functions (e.g., the record button). One could argue that through the concept of functional feedforward, Wensveen im- plicitly suggests that feedforward might also be nested, just like Gaver’s affordances. Gaver also introduced sequential affordances which are only available at certain points in time. This is common in graphical user interfaces since these can be updated during usage. In contrast, physical objects usually have a static physical appearance and cannot update their form over time. The information that specifies an affordance (e.g., a button on the screen), can be quickly updated as new affordances become available (e.g., adding a drop down menu when the button is clicked to allow the user to make a selection) [<xref ref-type="bibr" rid="R9" id="144" class="deo:Reference">9</xref>]. Similarly, feedforward could only be made available at certain points in time or be updated during the user’s action to provide new information. The examples of feedforward in gestural interaction that were discussed previously (marking menus [<xref ref-type="bibr" rid="R17" id="145" class="deo:Reference">17</xref>] and OctoPocus [<xref ref-type="bibr" rid="R1" id="146" class="deo:Reference">1</xref>]), can be seen as examples of sequential feedforward. Sequential feedforward in combination with feedback could further blur the difference between the two concepts. Feedback provided after performing an action might afterwards serve as feedforward for the action that logically follows the previous one.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="148" confidence="possible" page="5" column="2">Kaptelinin and Nardi – Mediated Action &amp; Affordances</h2>
          <region class="DoCO:TextChunk" id="158" page="5" column="2">Kaptelinin and Nardi [ <xref ref-type="bibr" rid="R15" id="149" class="deo:Reference">15</xref>] call for adopting a mediated-action perspective on technology affordances as an alternative for Gibson’s ecological psychology perspective. They differentiate between two types of affordances: instrumental technology affordances which are comprised of a handling affordance and an effecter affordance; and a set of auxiliary tech- nological affordances such as maintenance, aggregation and learning affordances. We mainly focus on instrumental technology affordances – and in particular effecter affordances – here, as these appear to be quite similar to feedforward. The difference between handling and effecter affordances is explained with the example of a knife. A knife consists of two distinct parts: the handle and the blade. The knife handle is used for holding the knife, while the blade is used to manipu- late objects (e.g., an apple). Kaptelinin and Nardi [<xref ref-type="bibr" rid="R15" id="150" class="deo:Reference">15</xref>] argue that this distinction also applies to digital technologies, and, more specifically, to user interface widgets. For example, the ability to drag the scroll box of a scroll bar is the handling affordance, while the ability to display a certain portion of the document in the window is the effecter affordance.<marker type="page" number="6"/><marker type="column" number="1"/><marker type="block"/> Like Djajadiningrat [<xref ref-type="bibr" rid="R6" id="155" class="deo:Reference">6</xref>] and Wensveen [<xref ref-type="bibr" rid="R27" id="156" class="deo:Reference">27</xref>], Kaptelinin and Nardi [<xref ref-type="bibr" rid="R15" id="157" class="deo:Reference">15</xref>] distinguish between the purpose of an action (the effecter affordance) and the action possibility (the handling affordance). Indeed, they provide an example of a dialog where: “handling affordances are clear but the outcomes of user actions (effecter affordances) are not immediately obvious. [...] The user can see that they can select the check- boxes and click the buttons, but the effects of these actions are not directly apparent.” They state that users will be con- fused when handling and effecter affordances are not coupled tightly enough. Kaptelinin and Nardi’s effecter affordances appear to be closely related to feedforward since they convey the outcome of a certain action. The idea of tightly integrated handling and effecter affordances seems to be similar to how perceived affordances and feedforward can be combined to communicate both the action possibilities and the expected outcomes of those actions.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="152" page="5" column="2">1935</outsider>
          <outsider class="DoCO:TextBox" type="header" id="153" page="6" column="1">Session: Design Research</outsider>
          <outsider class="DoCO:TextBox" type="header" id="154" page="6" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="159" confidence="possible" page="6" column="1">Hartson – Feedforward as a Cognitive Affordance</h2>
          <region class="DoCO:TextChunk" id="165" page="6" column="1">Hartson [<xref ref-type="bibr" rid="R12" id="160" class="deo:Reference">12</xref>] further clarified the concept of affordances based on Gaver’s [<xref ref-type="bibr" rid="R9" id="161" class="deo:Reference">9</xref>] and McGrenere and Ho’s [<xref ref-type="bibr" rid="R19" id="162" class="deo:Reference">19</xref>] work. He distinguishes between four types of affordances based on the role they play in supporting users during interaction: Cognitive affordances are considered to be an extension of Norman’s perceived affordances [<xref ref-type="bibr" rid="R22" id="163" class="deo:Reference">22</xref>], helping users with their cognitive actions. Hartson [<xref ref-type="bibr" rid="R12" id="164" class="deo:Reference">12</xref>] defines cognitive affordances as “a design feature that supports, facilitates, or enables thinking and/or knowing about something”. Example: A button label that helps users know what will happen if they click on it .</region>
          <region class="DoCO:TextChunk" id="171" confidence="possible" page="6" column="1">Physical affordances help users with their physical actions, and match with Norman’s real affordances [<xref ref-type="bibr" rid="R22" id="166" class="deo:Reference">22</xref>] (or Gibson’s affordances [<xref ref-type="bibr" rid="R10" id="167" class="deo:Reference">10</xref>, <xref ref-type="bibr" rid="R11" id="168" class="deo:Reference">11</xref>]). According to Hartson [<xref ref-type="bibr" rid="R12" id="169" class="deo:Reference">12</xref>], a physical affordance is “a design feature that helps, aids, supports, facilitates, or enables physically doing something”. Example: A button that is large enough so that users can click on it accurately. Sensory affordances help users with their sensory actions (perceiving information). Hartson [<xref ref-type="bibr" rid="R12" id="170" class="deo:Reference">12</xref>] defines a sensory affordance as “a design feature that helps, aids, supports, facilitates, or enables the user in sensing (e.g., seeing, hear- ing, feeling) something”. Sensory affordances play a supporting role for cognitive and physical affordances. Hartson thus explicitly separates sensing from understanding. Example: A label font size large enough to read easily.</region>
          <region class="DoCO:TextChunk" id="189" page="6" column="1">Functional affordances are a design feature that help users accomplish work. It ties usage to usefulness, and is similar to McGrenere and Ho’s idea of “affordances in soft- ware” [ <xref ref-type="bibr" rid="R19" id="172" class="deo:Reference">19</xref>]. Functional affordances add purpose to a physical affordance. Example: The internal system ability to sort a series of numbers (invoked by users clicking on the Sort button). These four types of affordances are tightly coupled and work together to help users in their interaction. Physical affordances are associated with the “operability” characteristics<marker type="column" number="2"/><marker type="block"/> of user interface artefacts. Cognitive affordances are associated with the semantics or meaning of user interface artefacts. Sensory affordances have a supporting role, and are associated with the “sense-ability” characteristics of user interface artefacts, especially of physical affordances and cognitive affordances. According to Hartson [<xref ref-type="bibr" rid="R12" id="178" class="deo:Reference">12</xref>], it is design that connects sensory affordances to physical and cognitive affordances, so that they can be seen, heard or felt to be used. Moreover, physical affordances carry a mandatory component of utility or purpose – the functional affordance – to which statements about physical affordances should refer. Hartson’s framework significantly broadens the scope of affordances, so that they also include both the notions of feedback and feedforward. Hartson explains a cognitive affordance with the example of “a button label that helps users know what will happen when they click on it”, which essentially explains the purpose of this button, and can thus be seen as feedforward. Hartson explains how the four types of affordances helps user achieve their goals by plugging them into Norman’s Stages of Action Model [<xref ref-type="bibr" rid="R22" id="179" class="deo:Reference">22</xref>], as seen in <xref ref-type="fig" rid="F6" id="180" class="deo:Reference">Figure 6</xref>. Earlier, we positioned feedforward and feedback in Norman’s Stages of Action Model (see <xref ref-type="fig" rid="F1" id="181" class="deo:Reference">Figure 1</xref>). It is interesting to note that Hartson identified the need for cognitive (and sensory) affordances exactly where we situate feedforward and feedback in Norman’s Stages of Action model (<xref ref-type="fig" rid="F6" id="182" class="deo:Reference">Figure 6</xref>), which suggests that both feedback and feedforward are cognitive affordances. Furthermore, Wensveen’s augmented, inherent and functional feedforward [<xref ref-type="bibr" rid="R27" id="183" class="deo:Reference">27</xref>, <xref ref-type="bibr" rid="R29" id="184" class="deo:Reference">29</xref>] can be explained in terms of Hartson’s four types of affordances: Inherent feedforward : Wensveen [<xref ref-type="bibr" rid="R27" id="185" class="deo:Reference">27</xref>] clearly sees inherent feedforward as a limited interpretation of the concept of Gibson’s affordance [<xref ref-type="bibr" rid="R10" id="186" class="deo:Reference">10</xref>, <xref ref-type="bibr" rid="R11" id="187" class="deo:Reference">11</xref>]. It communicates what kind of action is possible and how it can be carried out. This is similar to Hartson’s physical affordance, as a physical design feature that helps users to physically do some- thing [<xref ref-type="bibr" rid="R12" id="188" class="deo:Reference">12</xref>].</region>
          <region class="DoCO:FigureBox" id="F6">
            <image class="DoCO:Figure" src="62oo.page_006.image_05.png" thmb="62oo.page_006.image_05-thumb.png"/>
            <caption class="deo:Caption" id="177" page="6" column="2">Figure 6. The need for physical, cognitive, sensory and functional affordances, and the position of feedback and feedforward in Norman’s Stages of Action model (image based on [<xref ref-type="bibr" rid="R12" id="175" class="deo:Reference">12</xref>], adapted from [<xref ref-type="bibr" rid="R22" id="176" class="deo:Reference">22</xref>]).</caption>
          </region>
          <region class="DoCO:TextChunk" id="190" confidence="possible" page="6" column="2">Augmented feedforward : this is clearly a cognitive affordance, where lexical or graphical labels (e.g., words, pic-</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="191" page="6" column="2">1936</outsider>
          <outsider class="DoCO:TextBox" type="header" id="192" page="7" column="1">Session: Design Research</outsider>
          <outsider class="DoCO:TextBox" type="header" id="193" page="7" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
          <region class="DoCO:TextChunk" id="201" page="7" column="1">tograms, spoken words) act as a sensory affordance to communicate the result of the user’s action in an understandable way. Functional feedforward : informs the user about the more general purpose of a product and its functional features [<xref ref-type="bibr" rid="R27" id="194" class="deo:Reference">27</xref>]. As noted above, Wensveen’s functional feedforward can be seen as a high-level nested affordance in Gaver’s terminology. Even though Hartson does not explicitly state that his four kinds of affordances can be nested, we feel this is implied by several examples in his paper [<xref ref-type="bibr" rid="R12" id="195" class="deo:Reference">12</xref>]. Wensveen’s functional feedforward might be categorized as a cognitive affordance that makes the high- level functionality of the system – or its functional affordance – visible. If this is achieved through physical design, the product’s form can be seen as a sensory affordance which allows users to recognize its functionality. Note that Hartson’s framework unites both Gaver’s [<xref ref-type="bibr" rid="R9" id="196" class="deo:Reference">9</xref>] and McGrenere and Ho’s [<xref ref-type="bibr" rid="R19" id="197" class="deo:Reference">19</xref>] work on affordances, and can be used to explain feedforward according to both Djajadin- ingrat’s [<xref ref-type="bibr" rid="R6" id="198" class="deo:Reference">6</xref>] and Wensveen’s [<xref ref-type="bibr" rid="R27" id="199" class="deo:Reference">27</xref>, <xref ref-type="bibr" rid="R29" id="200" class="deo:Reference">29</xref>] definitions. We will later use Hartson’s framework to reframe feedforward and disam- biguate it from perceived affordances and feedback. We con- clude this section by giving an overview of aspects related to feedforward in Norman’s work.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="202" confidence="possible" page="7" column="1">Norman – Feedforward, Symbols &amp; Constraints</h2>
          <region class="DoCO:TextChunk" id="217" page="7" column="1">While this section started with the assumption that feedforward goes beyond affordances, Hartson [ <xref ref-type="bibr" rid="R12" id="203" class="deo:Reference">12</xref>] argued that feedforward is just a special kind of affordance, namely a cognitive affordance. Norman implies a purpose for a physical affordance (e.g., a doorknob that can be turned in order to open the door), thereby eliminating the need for explicit feedforward. This is confirmed by Hartson [emphasis ours]: In Norman’s Design of Everyday Things world of non-computer devices, a purpose for a physical affordance is always implied. The doorknob is a cognitive and physical affordance for operating the door. The physical affordance offered by a doorknob does not mean merely that the doorknob can be grasped and turned. It means that the doorknob can be grasped and turned in order to operate (e.g., invoke the function or mechanism of opening) the door; the user is enabled to operate the door. In turn, the door itself is a functional affordance that, when invoked, allows passage. In this interaction design view, a physical affordance gives access to functionality, the purpose of the physical affordance used to access it. Hartson [<xref ref-type="bibr" rid="R12" id="204" class="deo:Reference">12</xref>] notes that even though the addition of purpose to the description of a physical affordance is an obvious extension, it should be made explicit to avoid ambiguities in terminology. There are several situations in which it is necessary to explicitly communicate the purpose of an action, especially in elec- tronic products, graphical user interfaces (GUIs) and tangible user interfaces. There is often a great deal of visual similarity present and the same physical affordances are used multiple times (e.g., physical and virtual buttons or sliders in products<marker type="column" number="2"/><marker type="block"/> or GUIs; and the typical cubes or blocks in tangible interaction [<xref ref-type="bibr" rid="R7" id="206" class="deo:Reference">7</xref>]). Moreover, with the integration of sensing into everyday artefacts, it is often even harder for users to reason about the result of actions they undertake. It is important to note that complex systems that use labels and/or icons are not examples of bad design. It is often overlooked that Norman did not disapprove of labels and icons altogether, he only said: “When simple things need pictures, labels, or instructions, the design has failed.” [<xref ref-type="bibr" rid="R22" id="207" class="deo:Reference">22</xref>]. When Norman talks about more complex systems or devices, he provides two ways to help users determine the purpose of a user interface artefact: mappings and a good conceptual model [<xref ref-type="bibr" rid="R22" id="208" class="deo:Reference">22</xref>]. As these mechanisms allow users to know what will happen when they perform an action, they could be seen as examples of feedforward. Mappings allow users to determine the relationships between actions and results, between the controls and their effects and between the system state and what is visible by spatial coupling. The controls are laid out in the same order as the artefacts in the physical world that they control. However, according to Djajadiningrat [<xref ref-type="bibr" rid="R7" id="209" class="deo:Reference">7</xref>], mappings fall short when dealing with abstract data that has no physical counterpart. A good conceptual model allows users to predict what will happen when they perform an action by exploiting consistency. Consistency in the presentation of op- erations and results helps users to form a coherent, consistent system image [<xref ref-type="bibr" rid="R22" id="210" class="deo:Reference">22</xref>]. Two other important design principles proposed by Norman are symbols and constraints [<xref ref-type="bibr" rid="R22" id="211" class="deo:Reference">22</xref>]. Norman argues that these are not affordances and that wording in the label on a button, for example, is symbolic communication. Hartson [<xref ref-type="bibr" rid="R12" id="212" class="deo:Reference">12</xref>] agrees, but states that under his own definition, communication is exactly what makes good wording effective as a cognitive affordance. It helps the user in knowing (e.g., knowing what to click on). In other words, Hartson [<xref ref-type="bibr" rid="R12" id="213" class="deo:Reference">12</xref>] sees symbols, constraints, and conventions as essential underlying mechanisms that make cognitive affordances – and therefore also feedforward and feedback – work. Hartson [<xref ref-type="bibr" rid="R12" id="214" class="deo:Reference">12</xref>] argues that Norman would agree that cognitive affordances play an enor- mously important role in interaction design. According to Hartson [<xref ref-type="bibr" rid="R12" id="215" class="deo:Reference">12</xref>], they are key to answering Norman’s question: “How do you know what to do?”. Hartson mentions that the design of cognitive affordances can indeed depend greatly on cultural conventions (or constraints [<xref ref-type="bibr" rid="R22" id="216" class="deo:Reference">22</xref>]) as a common base for communicating the meaning of cues from designer to user.</region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="218" page="7" column="2">REFRAMING FEEDFORWARD</h1>
        <region class="DoCO:TextChunk" id="227" page="7" column="2">In this section, we reframe feedforward informed by the above discussion of feedforward and related design principles such as affordances and feedback. We further clarify the differences between feedforward, (perceived) affordances and feedback based on Hartson’s four types of affordances [ <xref ref-type="bibr" rid="R12" id="219" class="deo:Reference">12</xref>]. As Hartson not only subsumes Gibson’s affordances [<xref ref-type="bibr" rid="R11" id="220" class="deo:Reference">11</xref>] (or Norman’s real affordances [<xref ref-type="bibr" rid="R22" id="221" class="deo:Reference">22</xref>]) and Norman’s perceived affordances [<xref ref-type="bibr" rid="R22" id="222" class="deo:Reference">22</xref>], but also significantly broadened the scope of affordances to include the notions of feedback and feedforward, we feel his framework is useful to reason about the differences and interrelationships between these design principles. As discussed earlier, Hartson states that physical af-<marker type="page" number="8"/><marker type="column" number="1"/><marker type="block"/> fordances carry a mandatory component of utility or purpose – the so-called functional affordance – to which statements about physical affordances should refer. Hartson’s notion of conveying the purpose of an action is, indeed, nothing else than feedforward.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="224" page="7" column="2">1937</outsider>
        <outsider class="DoCO:TextBox" type="header" id="225" page="8" column="1">Session: Design Research</outsider>
        <outsider class="DoCO:TextBox" type="header" id="226" page="8" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="228" confidence="possible" page="8" column="1">Disambiguation: Affordances,Feedforward &amp; Feedback</h2>
          <region class="DoCO:TextChunk" id="251" page="8" column="1">As discussed before, Hartson situated his four types of affordances into Norman’s Stages of Action Model [ <xref ref-type="bibr" rid="R22" id="229" class="deo:Reference">22</xref>]. Remem- ber that Hartson identified the need for cognitive (and sensory) affordances exactly where we positioned feedforward and feedback in Norman’s Stages of Action model (see Figure 6), which suggests that both feedback and feedforward are cognitive affordances. Hartson later confirmed this in personal email communication [<xref ref-type="bibr" rid="R13" id="230" class="deo:Reference">13</xref>]. Even though it is a cognitive affordance, feedforward is also connected to the three other types of affordances. Our new view on feedforward, feedback and perceived affordances is as follows: Perceived affordances (<xref ref-type="fig" rid="F7(a" id="231" class="deo:Reference">Figure 7(a)</xref>) are cognitive affordances that are understandable through well-defined sensory affordances (e.g. a door’s handle) and reveal a physical affordance (an action possibility), which is coupled to a functional affordance (the action’s purpose). Perceived affordances occur before the user’s action and invite them to an appropriate action. Feedforward (<xref ref-type="fig" rid="F7(b" id="232" class="deo:Reference">Figure 7(b)</xref>) is a cognitive affordance that is understandable through a well-defined sensory affordance (such as a readable, descriptive label or the object’s physical shape) and reveals the functional affordance (the system function) coupled to a physical affordance (the action possibility). Feedforward occurs before the user’s action and tells users what the result of their action will be. Feedback (<xref ref-type="fig" rid="F7(c" id="233" class="deo:Reference">Figure 7(c)</xref>) is a cognitive affordance that is understandable through a well-defined sensory affordance (e.g. an informative message), and provides information about the result of a user’s action, which is offered to the user through the combination of physical and functional affordances. Feedback is provided during or after a user’s action and informs them about the result of performing their action, Feedback can later turn into feedforward again (in combination with a perceived affordance) for another action that logically follows the previous one. <xref ref-type="fig" rid="F7" id="234" class="deo:Reference">Figure 7</xref> illustrates these definitions and shows how perceived affordances, feedforward and feedback relate to each other and are linked to Hartson’s four types of affordances [<xref ref-type="bibr" rid="R12" id="235" class="deo:Reference">12</xref>]. Both perceived affordances and feedforward tell users some- thing about a particular action through a combination of a physical and functional affordances. Perceived affordances and feedforward essentially provide different information about the action that users have to perform to achieve their goals. While perceived affordances reveal the physical affordance, which tells users that there is an physical action available and how to perform it, feedforward reveals the functional affordance, which tells users what will happen when they perform that action. We agree with Hartson [<xref ref-type="bibr" rid="R12" id="236" class="deo:Reference">12</xref>] that<marker type="column" number="2"/><marker type="block"/> Norman always implied a purpose (or functional affordance) for a physical affordance (e.g., a doorknob that can be turned in order to open the door). As the purpose was implied, there was no need for explicit feedforward. Similarly, Norman always implied that perceived affordances were provided with a well-defined sensory affordance. As mentioned before, Hartson states that the addition of purpose should be made explicit to avoid ambiguities in terminology [<xref ref-type="bibr" rid="R12" id="240" class="deo:Reference">12</xref>]. Additionally, we argue that the more complex a system or interaction context gets, the larger the need will be for elaborate feedforward in order to aid users in achieving their goals. Note that feedback provided after performing an action might afterwards again serve as feedforward for the action that logically follows the previous one (<xref ref-type="fig" rid="F7(c)" id="241" class="deo:Reference">Figure 7(c)</xref>). Wensveen refers to feedback that turns into feedforward as inherent traces of action [<xref ref-type="bibr" rid="R29" id="242" class="deo:Reference">29</xref>]. In its simplest form, it is “nothing more than evi- dence for the user that he has acted on the action possibilities, as if it were a trace of the bygone action.”. An example of feedback turning into feedforward is a physical light switch. When users flick the switch, feedback consists of the changed position of the switch, and, of course, also the light bulb that produces light (Wensveen’s notion of functional feedback [<xref ref-type="bibr" rid="R27" id="243" class="deo:Reference">27</xref>]). However, the user’s action also changed the possibilities for action, as the light cannot be turned on again, it can only be turned off. In essence, the feedback of the glowing light bulb and the state of the switch, becomes feedforward indicating that flicking the switch again will reverse the state of the light bulb and thereby turn it off. Another example of feedback that turns into feedforward can be found in marking menus [<xref ref-type="bibr" rid="R17" id="244" class="deo:Reference">17</xref>]. Once a function in the marking menu is invoked, its label is changed to the corresponding inverse function. This inverse function label, at first, serves as feedback to indicate that the previous function has been invoked,<marker type="page" number="9"/><marker type="column" number="1"/><marker type="block"/> and secondly, as feedforward for invoking the reverse function (thereby undoing the earlier action again).</region>
          <region class="DoCO:FigureBox" id="F7">
            <image class="DoCO:Figure" src="62oo.page_008.image_06.png" thmb="62oo.page_008.image_06-thumb.png"/>
            <caption class="deo:Caption" id="239" page="8" column="2">Figure 7. An overview of how perceived affordances, feedforward and feedback can be explained using Hartson’s four types of affordances. C, S, F and PH refer to Hartson’s Cognitive, Sensory, Functional and Physical affordances respectively. The functional and physical affordances together constitute a possibility for a purposeful action. While perceived affordances and feedforward provide information before the user’s action (pre-action), feedback occurs after the user’s action.</caption>
          </region>
          <outsider class="DoCO:TextBox" type="page_nr" id="246" page="8" column="2">1938</outsider>
          <outsider class="DoCO:TextBox" type="header" id="247" page="9" column="1">Session: Design Research</outsider>
          <outsider class="DoCO:TextBox" type="header" id="248" page="9" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
          <region class="DoCO:FigureBox" id="F8">
            <image class="DoCO:Figure" src="62oo.page_009.image_07.png" thmb="62oo.page_009.image_07-thumb.png"/>
            <caption class="deo:Caption" id="250" page="9" column="1">Figure 8. False and hidden feedforward.</caption>
          </region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="252" confidence="possible" page="9" column="1">Hidden and False Feedforward</h2>
          <region class="DoCO:TextChunk" id="257" page="9" column="1">Gaver also discerns between affordances – as in Gibson’s original definition [<xref ref-type="bibr" rid="R10" id="253" class="deo:Reference">10</xref>, <xref ref-type="bibr" rid="R11" id="254" class="deo:Reference">11</xref>] – and the perceptual information available about them (so-called apparent affordances), which corresponds to what Norman defined as perceived affordances [<xref ref-type="bibr" rid="R22" id="255" class="deo:Reference">22</xref>]. Based on this distinction, Gaver introduces the concept of false and hidden affordances, where the apparent information about the affordance is incorrect or missing respectively. A similar reasoning could be applied to feedforward and how an action is coupled to a system function, as shown in <xref ref-type="fig" rid="F8" id="256" class="deo:Reference">Figure 8</xref>. Feedforward can be false when it conveys incorrect information about what system function the action performs. When feedforward is missing, it hides how the action is related to the system function (e.g., a button without a label). Although undesirable, false and hidden feedforward might be useful notions to consider in interaction design. A simple example of a false feedforward in a graphical user interface is a button with an incorrect label (an effective technique which is often employed by malicious software to trick the user in invoking certain destructive actions).</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="258" confidence="possible" page="9" column="1">Nested and Sequential Feedforward</h2>
          <region class="DoCO:TextChunk" id="268" page="9" column="1">Gaver [ <xref ref-type="bibr" rid="R9" id="259" class="deo:Reference">9</xref>] proposed the idea of sequential affordances and nested affordances for complex actions. We argue that feedforward can also be nested or sequential. In his discussion of sequential affordances, Gaver also mentions that affordances can be conveyed through multiple modalities (e.g., visual, tactile, auditory information). Similarly, we think that feedforward could also be provided using different modalities, as confirmed by Wensveen [<xref ref-type="bibr" rid="R27" id="260" class="deo:Reference">27</xref>] and Djajadiningrat et al. [<xref ref-type="bibr" rid="R6" id="261" class="deo:Reference">6</xref>]. However, some modalities (e.g., tactile) will be better suited to exploratory actions as they cannot be perceived through what Gaver calls “relatively passive perception” [<xref ref-type="bibr" rid="R9" id="262" class="deo:Reference">9</xref>]. As previously discussed, Wensveen’s functional feedforward [<xref ref-type="bibr" rid="R27" id="263" class="deo:Reference">27</xref>] conveys the general (top-level) function of a system (as seen in e.g., the voice recorder in <xref ref-type="fig" rid="F5(c)" id="264" class="deo:Reference">Figure 5(c)</xref>), and can be seen as the root of a nested feedforward hierarchy. It can be combined with feedforward that is provided for lower- level (or nested) functions. Functional feedforward typically<marker type="column" number="2"/><marker type="block"/> uses the shape of an object to inform users about its general purpose. Gaver’s idea of sequential affordances, also applies to feedforward. Notable examples of sequential feedforward are systems that use feedforward to make gestural interfaces easier to use, such as Bau et al.’s OctoPocus [<xref ref-type="bibr" rid="R1" id="266" class="deo:Reference">1</xref>] and Freeman et al.’s Shadowguides [<xref ref-type="bibr" rid="R8" id="267" class="deo:Reference">8</xref>]. OctoPocus and ShadowGuides continuously issue dynamic feedforward and gradual feedback during input. While performing a gesture, users are provided with information about their current set of possible gestures (or action possibilities) and the expected result of those gestures (using a simple label), together with feedback about how well the current gesture has been recognized. Feedforward could also be made available at discrete points in time, instead of being updated continuously. Dynamically updating feedforward is probably easiest to achieve in software. Sequential feedforward in combination with feedback could further blur the difference between the two concepts since feedback might afterwards serve as feedforward for the user’s next actions.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="269" confidence="possible" page="9" column="2">Retrospect: Definitions and Examples</h2>
          <region class="DoCO:TextChunk" id="281" page="9" column="2">The <xref ref-type="table" rid="Tin" id="270" class="deo:Reference">table in</xref> <xref ref-type="fig" rid="F4" id="271" class="deo:Reference">Figure 4</xref> shows which aspects of feedforward are covered by different definitions and also analyzes how feedforward is used in a number of notable examples. As pointed out before, feedforward can be provided using multiple modalities. Unfortunately, designers mostly rely on visual information to convey feedforward, apart from a few exceptions, such as Djajadiningrat et al.’s TempSticks [<xref ref-type="bibr" rid="R6" id="272" class="deo:Reference">6</xref>]. Bau and Mackay have introduced the level of detail as a useful criterium for classifying feedforward mechanisms. Usu- ally, feedforward is provided in a low to average amount of detail. However, there might be situations in which feedforward can be provided with lots of details, for example to re- assure the user when they have to trust the system (e.g., for an e-commerce application or a smart home). Feedforward can be nested in a hierarchy. There are a number of examples that use nested feedforward, such as Disney AppMATes and the Tangible Video Editor (TVE) [<xref ref-type="bibr" rid="R31" id="273" class="deo:Reference">31</xref>]. Nested feedforward tends to rely on the object’s shape to convey its general function (i.e., functional feedforward), combined with lower-level types of feedforward information. Disney’s AppMATes is a children’s toy which uses tangible toy cars which can be used on a tablet. In this case, the shape of the toy car serves as high-level feedforward that explains the general purpose of the object. When children place the toy car on the screen, the car’s lights will be shown on the display. This acts as additional feedforward information indicating that the car and display are linked, after which children can try to move the car on the display. In case of the TVE, the shape of the different building blocks indicate their function. Finally, feedforward can be either be static or updated over time (sequential feedforward). An example of static feedforward is a fixed label in a tooltip. Examples of sequential feedforward are the OctoPocus [<xref ref-type="bibr" rid="R1" id="274" class="deo:Reference">1</xref>] and ShadowGuides [<xref ref-type="bibr" rid="R8" id="275" class="deo:Reference">8</xref>] dynamic guides, the TVE and SpeakCup [<xref ref-type="bibr" rid="R30" id="276" class="deo:Reference">30</xref>]. When users com- bine different building blocks with the TVE, this arrangement<marker type="page" number="10"/><marker type="column" number="1"/><marker type="block"/> of blocks indicates new information that serves as feedforward for knowing how the movie clips will be combined together. SpeakCup is a digital voice recorder that is shaped like a small rubber disc with holes in its center. When the holes are pressed in, forming a small cup, SpeakCup absorbs sound. When the holes are pressed out, the stored sounds are released. SpeakCup uses its shape to communicate to users what they can expect. As this shape changes over time when the disc is pressed in or out, this is another example of sequential feedforward.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="278" page="9" column="2">1939</outsider>
          <outsider class="DoCO:TextBox" type="header" id="279" page="10" column="1">Session: Design Research</outsider>
          <outsider class="DoCO:TextBox" type="header" id="280" page="10" column="1">CHI 2013: Changing Perspectives, Paris, France</outsider>
        </section>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="282" page="10" column="1">CONCLUSIONS</h1>
        <region class="DoCO:TextChunk" id="285" page="10" column="1">With this work, we reaffirm the importance of feedforward as a powerful tool for bridging Norman’s Gulf of Execution. We strongly believe that, like affordances, feedforward is not a concept that can be easily defined. Although designers do use feedforward in many cases, they do so unknowingly and based on their experiences and skills. We have reframed feedforward in terms of Hartson’s four types of affordances: (1) we disambiguated feedforward from related design principles such as feedback and perceived affordances; and (2) we identified four new classes of feedforward: hidden, false, sequential and nested feedforward. Our reframing of feedforward, together with the <xref ref-type="table" rid="Tin" id="283" class="deo:Reference">table in</xref> <xref ref-type="fig" rid="F4" id="284" class="deo:Reference">Figure 4</xref> provides a means for designers to explore and recognize different opportunities for feedforward.</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="286" confidence="possible" page="10" column="1">Acknowledgements</h2>
          <region class="DoCO:TextChunk" id="287" page="10" column="1">We would like to thank Stephan Wensveen for the interesting discussions we had on feedforward. We also want to express our gratitude to H. Rex Hartson for answering our questions about his work and sharing his reasoning on feedforward ver- sus affordances. Many thanks to Johannes Schöning and Steven Houben for their suggestions and feedback on early drafts of this paper. We also thank Don Norman for his in- sightful suggestions. Finally, we are indebted to the anony- mous reviewers whose comments further improved this paper.</region>
        </section>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="288" page="10" column="1">REFERENCES</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="289" page="10" column="1">1. Bau, O., and Mackay, W. E. OctoPocus: a dynamic guide for learning gesture-based command sets. In Proceedings of the 21st annual ACM symposium on User interface software and technology, ACM (Monterey, CA, USA, 2008), 37–46.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="290" page="10" column="1">2. Bellotti, V., Back, M., Edwards, W. K., Grinter, R. E., Henderson, A., and Lopes, C. Making sense of sensing systems: five questions for designers and researchers. In Proceedings of the SIGCHI conference on Human factors in computing systems: Changing our world, changing ourselves, ACM (Minneapolis, Minnesota, USA, 2002), 415–422.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="291" page="10" column="1">3. Bellotti, V., and Edwards, K. Intelligibility and accountability: human considerations in context-aware systems. Hum.-Comput. Interact. 16, 2 (2001), 193–212.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="292" page="10" column="1">4. De Souza, C. The Semiotic Engineering of Human-Computer Interaction. Acting with Technology Series. MIT Press, 2005.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="293" page="10" column="1">5. Dey, A. K. Understanding and using context. Personal Ubiquitous Comput. 5, 1 (2001), 4–7.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="294" page="10" column="1">6. Djajadiningrat, T., Overbeeke, K., and Wensveen, S. But how, donald, tell us how?: on the creation of meaning in interaction design through feedforward and inherent feedback. In Proc. of Designing Interactive Systems, DIS ’02, ACM (London, England, 2002), 285–291.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="296" page="10" column="1">7. Djajadiningrat, T., Wensveen, S., Frens, J., and Overbeeke, K. Tangible products: redressing the balance between appearance and action. Personal Ubiquitous Comput. 8, 5 (2004), 294–309. <marker type="column" number="2"/><marker type="block"/> Interactive Tabletops and Surfaces, ITS ’09, ACM (New York, NY, USA, 2009), 165–172.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="297" page="10" column="2">9. Gaver, W. W. Technology affordances. In Proceedings of the SIGCHI conference on Human factors in computing systems: Reaching through technology, ACM (New Orleans, Louisiana, United States, 1991), 79–84.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="298" page="10" column="2">10. Gibson, J. J. The theory of affordances. Perceiving, acting and knowing: toward an ecological psychology (1977), 6782.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="299" page="10" column="2">11. Gibson, J. J. The Ecological Approach to Visual Perception, new edition ed. Lawrence Erlbaum, Sept. 1986.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="300" page="10" column="2">12. Hartson, R. Cognitive, physical, sensory, and functional affordances in interaction design. Behaviour &amp; Information Technology 22, 5 (2003), 315.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="301" page="10" column="2">13. Hartson, R. personal communication, 2010.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="302" page="10" column="2">14. Hornecker, E., and Buur, J. Getting a grip on tangible interaction: a framework on physical space and social interaction. In Proceedings of the SIGCHI conference on Human Factors in computing systems, ACM (Montral, Qubec, Canada, 2006), 437–446.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="303" page="10" column="2">15. Kaptelinin, V., and Nardi, B. Affordances in hci: toward a mediated action perspective. In Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems, CHI ’12, ACM (New York, NY, USA, 2012), 967–976.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="304" page="10" column="2">16. Kurtenbach, G., and Buxton, W. User learning and performance with marking menus. In Proceedings of the SIGCHI conference on Human factors in computing systems: celebrating interdependence, CHI ’94, ACM (New York, NY, USA, 1994), 258–264.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="305" page="10" column="2">17. Kurtenbach, G. P., Sellen, A. J., and Buxton, W. A. S. An empirical evaluation of some articulatory and cognitive aspects of marking menus. Hum.-Comput. Interact. 8, 1 (1993), 1–23.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="306" page="10" column="2">18. Lim, B. Y., and Dey, A. K. Toolkit to support intelligibility in context-aware applications. In Proceedings of the 12th ACM international conference on Ubiquitous computing, Ubicomp ’10, ACM (New York, NY, USA, 2010), 13–22.</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="307" page="10" column="2">19. McGrenere, J., and Ho, W. Affordances: Clarifying and evolving a concept. In Proceedings of Graphics Interface 2000 (2000), 179—186.</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="308" page="10" column="2">20. Meckl, P. H., and Seering, W. P. Feedforward control techniques to achieve fast settling time in robots. In American Control Conference, 1986 (june 1986), 1913 –1918.</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="309" page="10" column="2">21. Norman, D. Living With Complexity. Mit Press, 2011.</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="310" page="10" column="2">22. Norman, D. A. The Psychology Of Everyday Things. Basic Books, New York, USA, June 1988.</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="311" page="10" column="2">23. Norman, D. A. Affordance, conventions, and design. interactions 6, 3 (1999), 38–43.</ref>
          <ref rid="R24" class="deo:BibliographicReference" id="312" page="10" column="2">24. Norman, D. A. The way I see it: Signifiers, not affordances. interactions 15, 6 (Nov. 2008), 18–19.</ref>
          <ref rid="R25" class="deo:BibliographicReference" id="313" page="10" column="2">25. Saffer, D. Designing for Interaction: Creating Innovative Applications and Devices (2nd Edition), 2 ed. New Riders Press, Aug. 2009.</ref>
          <ref rid="R26" class="deo:BibliographicReference" id="314" page="10" column="2">26. van Dam, A. Post-WIMP user interfaces. Commun. ACM 40, 2 (1997), 63–67.</ref>
          <ref rid="R27" class="deo:BibliographicReference" id="315" page="10" column="2">27. Wensveen, S. A tangibility approach to affective interaction. Doctoral dissertation, TU Delft, Delft, The Netherlands (2005).</ref>
          <ref rid="R28" class="deo:BibliographicReference" id="316" page="10" column="2">28. Wensveen, S. personal communication, 2010.</ref>
          <ref rid="R29" class="deo:BibliographicReference" id="317" page="10" column="2">29. Wensveen, S. A. G., Djajadiningrat, J. P., and Overbeeke, C. J. Interaction frogger: a design framework to couple action and function through feedback and feedforward. In Proc. of Designing Interactive Systems, DIS ’04, ACM (Cambridge, MA, USA, 2004), 177–184.</ref>
          <ref rid="R30" class="deo:BibliographicReference" id="318" page="10" column="2">30. Zigelbaum, J., Chang, A., Gouldstone, J., Monzen, J. J., and Ishii, H. SpeakCup: simplicity, BABL, and shape change. In Proceedings of the 2nd international conference on Tangible and embedded interaction, TEI ’08 (2008), 145–146.</ref>
          <ref rid="R31" class="deo:BibliographicReference" id="319" page="10" column="2">31. Zigelbaum, J., Horn, M. S., Shaer, O., and Jacob, R. J. K. The tangible video editor: collaborative video editing with active tokens. In Proceedings of the 1st international conference on Tangible and</ref>
        </ref-list>
        <outsider class="DoCO:TextBox" type="footer" id="320" page="10" column="2">embedded interaction, TEI ’07, ACM (New York, NY, USA, 2007), 43–46.</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="321" page="10" column="2">8. Freeman, D., Benko, H., Morris, M. R., and Wigdor, D. Shadowguides: visualizations for in-situ learning of multi-touch and whole-hand gestures. In Proceedings of the ACM International Conference on</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="322" page="10" column="2">1940</outsider>
      </section>
    </body>
  </article>
</pdfx>
