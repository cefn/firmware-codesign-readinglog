<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>8fa332b9e379758911f14b212bb39bf0d019afef3db58feab2b4d009f55ab1b9</job>
    <base_name>62j1</base_name>
    <doi>http://dx.doi.org/10.1145/1281700.1281710</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="1">The User In Experimental Computer Systems Research</article-title>
      </title-group>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="2">Peter A. Dinda Gokhan Memik Robert P. Dick Bin Lin Arindam Mallik Ashish Gupta Samuel Rossoff { pdinda</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="3">g-memik</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="4">dickrp</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="5">b-lin</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="6">arindam</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="7">ashish</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="8">s-rossoff } @northwestern.edu Department of Electrical Engineering</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="9">Computer Science Northwestern University</name>
        </contrib>
      </contrib-group>
      <abstract class="DoCO:Abstract" id="10">Experimental computer systems research typically ignores the end-user, modeling him, if at all, in overly simple ways. We argue that this (1) results in inadequate performance evaluation of the systems, and (2) ignores opportunities. We summarize our experiences with (a) directly evaluating user satisfaction and (b) incorporating user feedback in different areas of client/server computing, and use our experiences to motivate principles for that domain. Specifically, we report on user studies to measure user satisfaction with resource borrowing and with different clock frequencies in desktop computing, the development and evaluation of user interfaces to integrate user feedback into scheduling and clock frequency decisions in this context, and results in predicting user action and system response in a remote display system. We also present initial results on extending our work to user control of scheduling and mapping of virtual machines in a virtualization-based distributed computing environment. We then generalize (a) and (b) as recommendations for incorporating the user into experimental computer systems research.</abstract>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="11" confidence="possible" page="1" column="1">Categories and Subject Descriptors</h2>
      </section>
      <region class="DoCO:TextChunk" id="12" confidence="possible">D.4 [Software]: Operating Systems; C.4 [Computer Systems Organization]: Performance of Systems; H.5.1 [Information Systems]: User/Machine Systems (HCI)</region>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="13" confidence="possible" page="1" column="1">General Terms</h2>
      </section>
      <section class="DoCO:Section">
        <h2 class="DoCO:SectionTitle" id="15" confidence="possible" page="1" column="1">Keywords</h2>
      </section>
    </front>
    <body class="DoCO:BodyMatter">
      <region class="DoCO:TextChunk" id="14" page="1" column="1">Human Factors, Design, Experimentation, Measurement, Performance, Autonomic Systems</region>
      <region class="DoCO:TextChunk" id="16" page="1" column="1">User Comfort With Resource Borrowing, User-driven Scheduling, User-driven Power Management, Speculative Remote Display, Human Directed Adaptation This work is in part supported by the NSF (awards ANI-0093221, ANI-0301108, CNS-0347941, CNS-0551639, IIS-0613568, IIS-0536994, CCF-0541337, CCF-0444405, and EIA-0224449), the DOE (award DE-FG02-05ER25691 and via an ORNL subcontract), and by gifts from VMware, Dell, and Symantec. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ExpCS 13-14 June 2007, San Diego, CA Copyright 2007 ACM 978-1-59593-751-3/07/06 ... $ 5.00.</region>
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="17" page="1" column="2">1. INTRODUCTION</h1>
      </section>
      <region class="DoCO:TextChunk" id="20" page="1" column="2">Computer systems research in all of its forms has tradi- tionally focused on the development of services and infras- tructure to make it possible to more easily build and scale applications, as well as to enable new kinds of applications. The user has been kept at a considerable remove from the systems software and hardware. We think of the user as interacting with the application, not with the computer system supporting the application. Although the satisfaction that the user garners depends in large part on the decisions made by the system, the system has only a myopic view of the user through the application workload. While the use of utility functions to represent the user has been proposed for over a decade, the reality is that how well these functions operate as a model of the user, and how well they actually represent diverse user sets, is largely unknown. Over the past three years, we have investigated several computer systems problems with careful consideration and direct analysis of the end-user. These problems are in the context of client/server computing and include resource borrowing in volunteer computing systems, scheduling of desktop replacement virtual machines in utility computing systems, power management in laptop computers, and latency in remote display systems. We have also considered adaptation in more general distributed systems. Based on this wide range of work, we believe that is important to advocate the following. 1. Experimental computer systems researchers should incorporate user studies into the evaluation of their systems. It is true that user studies are challenging, time consuming, often require institutional review board interaction, and generally produce small data sets. However, we have repeatedly found surprising results that would not have been apparent through typical performance evaluation. In particular, user satisfaction with the behavior of a system is extremely difficult to measure by proxy. 2. Experimental computer systems researchers should consider approaches to systems problems that directly incorporate feedback from the end-user. The system need not be completely invisible to the end-user, and even tiny amounts of end-user input can lead to very different system designs that produce much improved levels of measured user satisfaction. 1 1 Although the experience reports given in this paper focus on direct explicit feedback from the user, we do not dismiss implicit feedback. What is critical is the concept of using <marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> Although we believe these two points are applicable to vir- tually any computer system that involves human users, our experimental work, on which we elaborate in Section 2, has focused on systems problems encountered in client/server systems, such as:</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="19" page="1" column="2">1</outsider>
      <region class="DoCO:TextChunk" id="29" confidence="possible" page="2" column="1">• Heavyweight clients that run applications locally and have intermittent connectivity to the network to re- trieve and synchronize files This is the common use of laptop and desktop computers today. • Heavyweight clients on which the user’s personal virtual machine(s) can be downloaded, cached, and executed. This is the mode of operation suggested by the CMU/Intel Internet Suspend/Resume project [<xref ref-type="bibr" rid="R48" id="21" class="deo:Reference">48</xref>] and the Stanford Collective project [<xref ref-type="bibr" rid="R6" id="22" class="deo:Reference">6</xref>]. • Desktop replacement systems in which dumb thin clients interact through VNC [<xref ref-type="bibr" rid="R44" id="23" class="deo:Reference">44</xref>], Remote Desktop [<xref ref-type="bibr" rid="R40" id="24" class="deo:Reference">40</xref>], or similar protocols [<xref ref-type="bibr" rid="R25" id="25" class="deo:Reference">25</xref>, <xref ref-type="bibr" rid="R2" id="26" class="deo:Reference">2</xref>] with multiuser operating systems or virtual machines running in centralized clusters. AJAX-based web applications are somewhat similar. • Distributed computing systems based on virtual machines. Specifically, we report on initial results in human-directed mapping and scheduling of virtual machines in a distributed computing environment, such as in Virtuoso [<xref ref-type="bibr" rid="R55" id="27" class="deo:Reference">55</xref>] or VioCluster [<xref ref-type="bibr" rid="R47" id="28" class="deo:Reference">47</xref>].</region>
      <region class="DoCO:TextChunk" id="30" page="2" column="1">In systems such as these, as well as in “client-only” systems like PCs running Windows, there exists a tension between performance, resource use, and energy consumption that must be resolved. Resolving this tension in an optimal or at least acceptable way is the job of system-level mechanisms such as scheduling and resource management. The system tries to choose an operating point that optimizes a constrained function of these costs. All functions in interactive systems include some notion of user satisfaction. Current systems rely upon the following assumptions when optimizing the configuration to meet user satisfaction requirements:</region>
      <region class="DoCO:TextChunk" id="31" confidence="possible" page="2" column="1">• Users are considered to be identical with respect to satisfaction. In other words, the system optimizes for a canonical user, not for specific users. • User satisfaction can be measured implicitly. The system, in fact, optimizes for proxies of user satisfaction, such as bounded latency and jitter, minimal power consumption, or minimal price. Furthermore, the system chooses how to combine the many possible metrics of user satisfaction.</region>
      <region class="DoCO:TextChunk" id="33" page="2" column="1">These assumptions are widely held not only within the domain of our experimental work, but across experimental computer systems in general. In our work, we have concluded that this model of user satisfaction is simply untenable. We have found that the user feedback to customize system behavior on a per-user basis. We believe that explicit feedback is a powerful tech- nique and that learning techniques can reduce the interac- tion rate. However, if implicit feedback can produce similar results, they are to be preferred. Notice also that explicit feedback techniques provide a yardstick against which im- plicit techniques can be compared. <marker type="column" number="2"/><marker type="block"/> following set of principles are important in considering the optimization problems in client/server systems. Although many are counterintuitive, we have strong evidence that the following principles are correct. 1. User variation. User satisfaction with a given operating point varies dramatically between users. We have demonstrated that user satisfaction with varying degrees of contention or restriction of CPU, disk, and memory resources when using common interactive desktop applications varies considerably (Section 2.1). Similarly, we have found that user satisfaction with differing CPU frequencies varies dramatically (Section 2.2). 2. User-specified performance. Users can specify their personal metric for satisfaction as a function of system performance. In many cases, a user is best qualified to resolve the tension among quality metrics in a way that satisfies the user. While users vary in satisfaction, they are not ineffable. We have demonstrated how na ̈ ıve users can control their own CPU schedules in a desktop replacement scenario to trade off price and interactive performance (Section 2.3). Similarly, we have demonstrated an interface for letting na ̈ ıve users control CPU frequency to tradeoff between performance and power (Section 2.4). 3. User-system interface. The system software that does optimization should provide simple user interfaces to allow the user to explicitly indicate how well the system is trading off the desired quality metrics. Explicit expression of the user’s cost function is also helpful. Sections 2.3 and 2.4 illustrate such interfaces. Section 2.5 presents initial results on an interface (in the form of a game) for a more complex problem: mapping and scheduling of a collection of VMs on a set of hosts. 4. Learning. Explicit user interfaces to the system must, over time, require decreasing interaction. They should exploit explicit user interaction to learn user- specific preferences while simultaneously driving on-line tradeoffs among system quality metrics such as performance, power consumption, and correctness. The systems described in Sections 2.3 and 2.4 include simple learning techniques that reduce the amount of explicit user input. We also report briefly on our work evaluating the prospects for speculative remote display systems (Section 2.6), which shows how more costly learning algorithms can be fruitfully employed to predict user actions and responses. We elaborate on these principles in Section 3, and touch on how they relate to our experimental work in the relevant sections. Building on the experiences and principles, we next argue that using user studies to evaluate systems, and direct user input to inform them, can be generalized over experimental computer systems research. We then provide advice for experimenters who want to apply these ideas (Section 4). Section 5 concludes the paper.</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="34" page="2" column="2">2</outsider>
      <region class="DoCO:FigureBox" id="Fx35">
        <image class="DoCO:Figure" src="62j1.page_003.image_01.png" thmb="62j1.page_003.image_01-thumb.png"/>
      </region>
      <region class="DoCO:TextChunk" id="36" confidence="possible" page="3" column="1">(a) GUI</region>
      <region class="DoCO:FigureBox" id="Fx37">
        <image class="DoCO:Figure" src="62j1.page_003.image_02.png" thmb="62j1.page_003.image_02-thumb.png"/>
      </region>
      <region class="DoCO:TextChunk" id="38" confidence="possible" page="3" column="1">(b) Discomfort button</region>
      <region class="DoCO:FigureBox" id="F1">
        <caption class="deo:Caption" id="39" page="3" column="1">Figure 1: User comfort with resource borrowing interface.</caption>
      </region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="40" page="3" column="1">2. EXPERIENCE</h1>
        <region class="DoCO:TextChunk" id="41" page="3" column="1">We have practiced what we preach, (a) using user studies to evaluate systems, and (b) using direct user input to inform systems. In the following, we summarize the results (and cite the original work for those interested in learning more) from six distinct projects, all emerging from the domain of client/server systems, and more general distributed systems. Our goal is to illustrate that our claims about the efficacy of (a) and (b) hold in diverse areas. The results also give examples of the principles we have specifically dis- tilled for the client/server environment, which we elaborate on further in Section 3.</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="42" page="3" column="1">2.1 Measuring and understanding user comfort with resource borrowing</h2>
          <region class="DoCO:TextChunk" id="58" page="3" column="1">Many computers are highly under-utilized [<xref ref-type="bibr" rid="R41" id="43" class="deo:Reference">41</xref>, <xref ref-type="bibr" rid="R10" id="44" class="deo:Reference">10</xref>, <xref ref-type="bibr" rid="R1" id="45" class="deo:Reference">1</xref>], a fact that many widely-used systems rely on to harvest spare resources for other purposes, a technique we refer to as resource borrowing. Examples in scientific computing include Condor [<xref ref-type="bibr" rid="R33" id="46" class="deo:Reference">33</xref>, <xref ref-type="bibr" rid="R16" id="47" class="deo:Reference">16</xref>], Entropia [<xref ref-type="bibr" rid="R8" id="48" class="deo:Reference">8</xref>], <email id="49">SETI@Home</email> [<xref ref-type="bibr" rid="R53" id="50" class="deo:Reference">53</xref>], Pro- tein Folding at Home [<xref ref-type="bibr" rid="R26" id="51" class="deo:Reference">26</xref>], DESChall [<xref ref-type="bibr" rid="R9" id="52" class="deo:Reference">9</xref>], and the Google Toolbar [<xref ref-type="bibr" rid="R18" id="53" class="deo:Reference">18</xref>]. Such systems are deployed on hundreds of thousands (<email id="54">SETI@Home</email>) to millions (Google) of computers. The definition of “spare” in these systems is extremely con- servative because the foreground user can turn the sharing systems off if they become irritating. For example, the default for both Condor and <email id="55">SETI@Home</email> is to run only when the screen saver is on and there is no other significant load on the machine. The assumption is that resource borrowing systems must place few restrictions on the resources provided to the interactive user when the user is active. But is this true? How restricted can an interactive user’s resources become before causing discomfort? To address this question, we conducted the first-ever in- depth study of user comfort with resource borrowing [<xref ref-type="bibr" rid="R20" id="56" class="deo:Reference">20</xref>, <xref ref-type="bibr" rid="R19" id="57" class="deo:Reference">19</xref>]. We provided a qualitative and quantitative analysis of direct measurements of user comfort with controlled CPU, disk, and physical memory contention. In essence, the user</region>
          <region class="DoCO:TextChunk" id="59" confidence="possible" page="3" column="2">CDF for CPU feedback values - ALL contexts 1 Prob Exhausted Region 0.8 0.6 Prob Discomfort Region 0.4 0.2 0 0 2 4 6 8 10 Contention Level ( DfCount=295, Excount=47, f_d=0.862) (a) CPU CDF for Mem feedback values - ALL contexts 1 Prob 0.8 0.6 Prob 0.4 Exhausted Region 0.2 Discomfort Region 0 0 0.2 0.4 0.6 0.8 1 Contention Level ( DfCount=25, Excount=96, f_d=0.206) (b) Memory CDF for Disk feedback values - ALL contexts 1 Prob 0.8 0.6 Prob 0.4 Exhausted Region 0.2 Discomfort Region 0 0 2 4 6 8 10 Contention Level ( DfCount=47, Excount=94, f_d=0.333) (c) Disk</region>
          <region class="DoCO:FigureBox" id="F2">
            <caption class="deo:Caption" id="60" page="3" column="2">Figure 2:</caption>
          </region>
          <region class="DoCO:TextChunk" id="61" confidence="possible" page="3" column="2">User comfort with resource borrowing: CDF of discomfort with resource contention. Windows XP desktop.</region>
          <region class="DoCO:TextChunk" id="67" page="3" column="2">is faced with an increasing degree of resource contention until they finally press a “discomfort button” ( <xref ref-type="fig" rid="F1" id="62" class="deo:Reference">Figure 1</xref>) The carefully controlled study examined 33 users operating word processors, presentation software, web browsers, and games. <xref ref-type="fig" rid="F2" id="63" class="deo:Reference">Figure 2</xref> gives examples of our quantitative measurements. Our papers go into much more depth. The figures show CDFs for CPU, memory, and disk aggregated over<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> all the tasks in our study (word processing with Microsoft Word, presentation creation with Microsoft Powerpoint, web browsing with Internet Explorer, and game-playing with Quake, a first person shooter game). The horizontal axis is the level of contention for each resource. The vertical axis is the cumulative fraction of users experiencing discomfort. As the level of borrowing increases, interactivity is increasingly likely to be affected. This is the discomfort region. Some users do not experience discomfort in the range of levels explored. We refer to this as the exhausted region. A run is a controlled buildup of contention for a given user, application, and resource that either ends in the user pressing the button at or after a high contention level is reached. There is also some probability that a user will feel discomfort even when no resource borrowing is occurring. We introduced blank runs in which no contention is applied to measure this effect. This background discomfort is the noise floor. Our study addressed many aspects of user comfort with resource borrowing and their implications. However, the most important result, which can readily be seen in the data in <xref ref-type="fig" rid="F2" id="66" class="deo:Reference">Figure 2</xref>, is the high variation. This variation is largely accounted for by two dominant factors: the application and the user. Obviously in a real desktop environment, it is the user who is the independent factor, as it is the user who chooses the application to run. User variation within an application is also very large.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="65" page="3" column="2">3</outsider>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="68" page="4" column="1">2.2 Measuring and understanding user comfort with lower clock frequencies</h2>
          <region class="DoCO:TextChunk" id="74" page="4" column="1">Having seen that there is tremendous variation in user tolerance for restrictions on CPU, disk, and memory resources, and its implications for resource borrowing systems, consider now a more prosaic context: power management in laptop computers. On the processors used in these machines, the operating system can change the clock frequency (and corresponding voltage) to trade off power consumption and performance. Similar to resource borrowing systems, most software (such as the Windows Dynamic Voltage and Frequency Scaling (DVFS) algorithm) makes the assumption that once any load is placed on the CPU (e.g., the user does anything), the frequency should be maximized. But is this true? To understand the variation in user tolerance for differing frequencies (on an IBM ThinkPad T43 running Windows XP), we conducted a small (n = 8) randomized user study, comparing four processor frequency strategies including dynamic, static low frequency (1.06 GHz), static medium frequency (1.33 GHz), and static high frequency (1.86 GHz). The dynamic strategy is the default DVFS policy used in Windows XP. Our target processor has a maximum frequency of 2.13 GHz. We allowed the users to acclimatize to the full-speed performance of the machine and its applications and then had them create a presentation (Powerpoint), watch an animation (Shockwave), and play a game (FIFA Soccer). Users verbally ranked their experiences after each task/strategy pair on a scale of 1 (discomfort) to 10 (very comfortable). A detailed description of the study and its results are available elsewhere [<xref ref-type="bibr" rid="R38" id="69" class="deo:Reference">38</xref>, <xref ref-type="bibr" rid="R39" id="70" class="deo:Reference">39</xref>, <xref ref-type="bibr" rid="R32" id="71" class="deo:Reference">32</xref>], but we summarize the salient points here. <xref ref-type="fig" rid="F3" id="72" class="deo:Reference">Figure 3</xref> illustrates the results of the study in the form of overlapping histograms of the participants’ reported comfort level for each of four strategies. Consider <xref ref-type="fig" rid="F3(a)" id="73" class="deo:Reference">Figure 3(a)</xref>, which shows results for the PowerPoint task.</region>
          <region class="DoCO:TextChunk" id="75" confidence="possible" page="4" column="2">5 A (dynam ic) 2 (1.86GHz) 4 4 (1.33GHz) 5 (1.06GHz) count 3 User 2 1 0 1 2 3 4 5 6 7 8 9 10 Comfort level (a) Presentation 5 A (dynam ic) 2 (1.86GHz) 4 4 (1.33GHz) 5 (1.06GHz) count 3 User 2 1 0 1 2 3 4 5 6 7 8 9 10 Comfort level (b) Animation 5 A (dynam ic) 2 (1.86GHz) 4 4 (1.33GHz) 5 (1.06GHz) count 3 User 2 1 0 1 2 3 4 5 6 7 8 9 10 Comfort level (c) Game</region>
          <region class="DoCO:FigureBox" id="F3">
            <caption class="deo:Caption" id="76" page="4" column="2">Figure 3: User comfort with lower clock frequencies: typical Windows laptop.</caption>
          </region>
          <region class="DoCO:TextChunk" id="77" page="4" column="2">The horizontal axis displays the range of comfort levels allowed in the study and the vertical axis displays the count of the number of times that level was reported. The other graphs are similar. As one might expect, user comfort with any given strategy (or frequency in the case of the three static strategies) is highly dependent on the application. More importantly, however, is that comfort with given strategy is strongly user- dependent. For any given strategy, there is a considerable spread in the reported comfort levels. Some users will be completely happy with a low frequency even in a demand-</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="78" page="4" column="2">4</outsider>
          <region class="DoCO:FigureBox" id="Fx79">
            <image class="DoCO:Figure" src="62j1.page_005.image_03.png" thmb="62j1.page_005.image_03-thumb.png"/>
            <image class="DoCO:Figure" src="62j1.page_005.image_04.png" thmb="62j1.page_005.image_04-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="80" confidence="possible" page="5" column="1">(a) GUI (b) Joystick</region>
          <region class="DoCO:FigureBox" id="F4">
            <caption class="deo:Caption" id="81" page="5" column="1">Figure 4: User-driven virtual machine scheduling.</caption>
          </region>
          <region class="DoCO:TextChunk" id="82" page="5" column="1">ing application like a game, while others will be displeased with anything less than the highest frequency for even the most undemanding application.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="83" page="5" column="1">2.3 User-driven scheduling</h2>
          <region class="DoCO:TextChunk" id="96" page="5" column="1">In the Virtuoso 2 project [ <xref ref-type="bibr" rid="R49" id="84" class="deo:Reference">49</xref>, <xref ref-type="bibr" rid="R55" id="85" class="deo:Reference">55</xref>], we seek to develop tools and techniques for distributed computing environments based on the use of virtualization. One application of Virtuoso is a desktop replacement scenario in which a thin client interacts with a virtual machine (VM) running on a remote server. A natural question is how should the user’s VM be scheduled on the server? Clearly we seek to keep the interactive user happy while minimizing his utilization of the server to leave room for more VMs. We have demonstrated that it is feasible to make effective use of direct user interaction in the scheduling process; the user can guide the scheduling process to a solution that balances user comfort and the resource use. Because the resources needed to keep a user happy are highly dependent on the application and the user (Section 2.1), we believe user-specific adaptation is essential. We have designed, implemented, and evaluated two schemes for incorporating direct user interaction in scheduling virtual machines (VMs) within the Virtuoso system. The first extends the “discomfort button” feedback mechanism of the user comfort study, and is discussed elsewhere [<xref ref-type="bibr" rid="R28" id="86" class="deo:Reference">28</xref>]. Our second scheme [<xref ref-type="bibr" rid="R30" id="87" class="deo:Reference">30</xref>, <xref ref-type="bibr" rid="R31" id="88" class="deo:Reference">31</xref>] uses the periodic real-time model. We have developed a user-level scheduling tool [<xref ref-type="bibr" rid="R29" id="89" class="deo:Reference">29</xref>] that does earliest deadline first (EDF) scheduling of periodic tasks [<xref ref-type="bibr" rid="R34" id="90" class="deo:Reference">34</xref>, <xref ref-type="bibr" rid="R35" id="91" class="deo:Reference">35</xref>], letting us run a VM for a given slice within a period of time. We make the period and slice directly configurable by the user through a straightforward human interface: a precision non-centering joystick. The software interface shows the user the current efficiency of their VM (% of allocated cycles actually being used) and the price (linear function of the utilization (slice/period)). <xref ref-type="fig" rid="F4" id="92" class="deo:Reference">Figure 4</xref> illustrates the interface. We ran a comprehensive user study, described in detail in the cited papers, in which the 18 participants used the interface with the goal of finding a comfortable setting of lowest cost while they used a range of Windows applications. We found that almost all users felt that they were able to 2 <ext-link ext-link-type="uri" href="http://virtuoso.cs.northwestern.edu" id="93">http://virtuoso.cs.northwestern.edu</ext-link><marker type="column" number="2"/><marker type="block"/> find a comfortable setting, as well as a comfortable setting that they believed was of lowest cost. <xref ref-type="fig" rid="F5" id="95" class="deo:Reference">Figure 5</xref> summarizes user responses from our study. For each task, the user was asked to find a comfortable setting (I Comfort), to find a comfortable setting of the lowest possible cost (II Comfort+Cost), and to repeat the latter in a deceptive context to eliminate the obvious bias towards choosing a low cost, but uncomfortable setting (III Com- fort+Cost+External). The results for II and III are virtu- ally identical. The upshot is that the vast majority of users, even na ̈ ıve users, are able to quickly understand the control mechanism and use it to find comfortable schedules that balance comfort and cost. We have studied the statistics of the costs of the schedule that the users chose, and the amount of time they required to find an appropriate schedule. Our first principle, that there is a wide variation in user satisfaction with a given operating point, is reflected in the wide variation in the costs of the schedules that users chose. The majority of users were able to find a setting that makes them comfortable, but there wasn’t just one setting. The time for a user to find a reasonable schedule is, on average, quite small (&lt; 1 minute), has little variation, and is likely to decline even further as a user becomes more familiar with the system. Our system is able to use a small amount of direct human interaction to help a diverse range of users find a satisfactory schedule. As far as we are aware, this was the first ever demonstration of the principle of using direct human interaction to inform and guide a low-level scheduling process.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="97" page="5" column="2">2.4 User-driven frequency scaling</h2>
          <region class="DoCO:TextChunk" id="112" page="5" column="2">Dynamic Voltage and Frequency Scaling (DVFS) is a widely used technique for controlling power and energy use in modern processors. These processors allow the dynamic selection of clock frequency and voltage. Reducing clock frequency reduces power in approximately linear proportion, while decreasing voltage decreases it in quadratic proportion. Further, the lowest voltage at which the processor can reliably run depends on the clock frequency, with higher frequencies requiring higher voltages. DVFS techniques typically use event-driven algorithms to set clock frequency, and then set voltage based on the chosen frequency setting. Existing DVFS techniques ignore the user, assuming that CPU utilization or the OS events prompting it are sufficient proxies. A high CPU utilization leads to a high frequency and high voltage, regardless of the user’s satisfaction or ex- pectation of performance. To remedy this limitation, we developed User-Driven Frequency Scaling (UDFS), a technique that uses direct user feedback to drive an online control algorithm which determines the processor frequency. UDFS automatically adapts OS power management to user preferences. Extensive details on UDFS (and a companion technique not used here, process-driven voltage scaling) can be found elsewhere [ <xref ref-type="bibr" rid="R38" id="98" class="deo:Reference">38</xref>, <xref ref-type="bibr" rid="R39" id="99" class="deo:Reference">39</xref>, <xref ref-type="bibr" rid="R32" id="100" class="deo:Reference">32</xref>]. The effect of processor frequency is directly visible to the end-user as it determines the resulting performance. There is considerable variation among users with respect to the satisfactory performance level for a given workload mix, as we have illustrated in Section 2.1, and for a given workload mix and clock frequency combination, as illustrated in Section 2.2. We exploit these variations to customize frequency control policies dynamically to individual users. To investigate the feasibility of UDFS, we developed two<marker type="page" number="6"/><marker type="column" number="1"/><marker type="block"/> schemes to control the CPU frequency by considering direct user feedback. Both of these schemes try to find the ideal operating point by reducing frequency until user feedback is provided via a button press. UDFS1 is an adaptive algorithm that can be viewed as an extension/variant of the TCP congestion control algorithm [<xref ref-type="bibr" rid="R51" id="105" class="deo:Reference">51</xref>, <xref ref-type="bibr" rid="R59" id="106" class="deo:Reference">59</xref>, <xref ref-type="bibr" rid="R4" id="107" class="deo:Reference">4</xref>, <xref ref-type="bibr" rid="R14" id="108" class="deo:Reference">14</xref>]. UDFS2, on the other hand, tries to find the lowest frequency at which the user feels comfortable and then stabilize there. For each frequency level t i possible in the processor, we assign an interval t i , the time for the algorithm to stay at that level. If no user feedback is received during the interval, the algorithm reduces the frequency by one level. If a user expresses irritation, the interval for the corresponding control level is increased. To investigate the impact of UDFS schemes, we performed a study with 20 users. The user study took around 45 minutes for each user, during which time each user operated a laptop running three different applications using the native Windows DVFS, UDFS1, and UDFS2. The laptop was connected to a National Instruments 6034E data acquisition board attached to the PCI bus of a host workstation running Linux, enabling power measurement. For the three applications studied (Shockwave, FIFA, and Powerpoint), the power consumption of the system can be reduced by 22.1%, averaged across all users. In addition to this analysis, we have measured the static power consumption of the CPU by monitoring its frequency. The static power consumption results are summarized in <xref ref-type="fig" rid="F6" id="109" class="deo:Reference">Figure 6</xref>, which presents both individual user results and average results for UDFS1 and UDFS2 for three different<marker type="column" number="2"/><marker type="block"/> applications. The vertical axis show the percentage improvement for power over the Windows native DVFS scheme. On average, the power consumption can be reduced by 24.9% over existing DVFS scheme for all three applications using the UDFS2 algorithm. Similar to the results for user-driven scheduling, the results for user-driven frequency scaling illustrate the utility of having even a small amount of user feedback within systems software.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="102" page="5" column="2">5</outsider>
          <region class="unknown" id="103" page="6" column="1">Task Sub-task Question Acclim. Adaptation I Do you feel you are familiar with the performance of this computer? Are you comfortable with these applications? Adaptation II Do you feel that you understand the control mechanism? Do you feel that you can use the control mechanism? Word I Comfort Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? II Comfort+Cost Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? III Comfort+Cost+Ext Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? Powerpoint I Comfort Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? II Comfort+Cost Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? III Comfort+Cost+Ext Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? Web I Comfort Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? II Comfort+Cost Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? III Comfort+Cost+Ext Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? Game I Comfort Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? II Comfort+Cost Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable? III Comfort+Cost+Ext Did you find that the joystick control was understandable in this task? Were you able to find a setting that was comfortable?</region>
          <region class="DoCO:FigureBox" id="F5">
            <caption class="deo:Caption" id="104" page="6" column="1">Figure 5: Summary of user responses in study of user-driven scheduling of interactive virtual machines.</caption>
          </region>
          <region class="unknown" id="111" page="6" column="2">Yes No NA Yes/Total 95% CT 18 0 0 1 (1,1) 17 1 0 0.94 (0.84, 1.05) 18 0 0 1.00 (1,1) 18 0 0 1.00 (1,1) 17 1 0 0.94 (0.84, 1.05) 18 0 0 1.00 (1,1) 17 1 0 0.94 (0.84, 1.05) 18 0 0 1.00 (1,1) 18 0 0 1.00 (1,1) 18 0 0 1.00 (1,1) 16 2 0 0.89 (0.74, 1.03) 18 0 0 1.00 (1,1) 17 1 0 0.94 (0.84, 1.05) 17 1 0 0.94 (0.84, 1.05) 16 1 0 0.89 (0.74, 1.03) 17 1 0 0.94 (0.70, 1.08) 16 2 0 0.89 (0.74, 1.03) 13 4 1 0.72 (0.52, 0.93) 17 1 0 0.94 (0.84, 1.05) 16 2 0 0.89 (0.74, 1.03) 17 1 0 0.94 (0.84, 1.05) 16 1 1 0.89 (0.74, 1.03) 18 0 0 1.00 (1, 1) 16 2 0 0.89 (0.74, 1.03) 17 1 0 0.94 (0.84, 1.05) 14 3 1 0.78 (0.59, 0.97) 17 1 0 0.94 (0.84, 1.05) 16 2 0 0.89 (0.74, 1.03)</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="113" page="6" column="2">2.5 User-driven scheduling of collections of virtual machines</h2>
          <region class="DoCO:TextChunk" id="120" page="6" column="2">We are currently working to extend the work described in Section 2.3 to the more general forms of adaptation optimization problems that occur in distributed and parallel systems like Virtuoso [<xref ref-type="bibr" rid="R56" id="114" class="deo:Reference">56</xref>, <xref ref-type="bibr" rid="R57" id="115" class="deo:Reference">57</xref>, <xref ref-type="bibr" rid="R54" id="116" class="deo:Reference">54</xref>], particularly those which are difficult for users to formalize. The initial work [<xref ref-type="bibr" rid="R27" id="117" class="deo:Reference">27</xref>] uses game-like user interfaces to let users optimize the performance of collections of virtual machines by manipulating their mappings to hosts and their schedules. <xref ref-type="fig" rid="F7" id="118" class="deo:Reference">Figure 7</xref> illustrates a preliminary user interface for the VM scheduling game (mapping is fixed). Here, the user is running a collection of VMs that contain a bulk synchronous parallel [<xref ref-type="bibr" rid="R17" id="119" class="deo:Reference">17</xref>] application (although the user does not know this). The interface allows the user to manipulate the periodic real-time schedule of each VM by moving its represen- tation in the control area. The goal is to either maximize the efficiency of the application within a bounded CPU share. The current and historical efficiency is shown in the first target area and the history area. The total computation is</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="121" page="6" column="2">6</outsider>
          <region class="unknown" id="122" page="7" column="1">20 15 10 5 0 -5 -10</region>
          <region class="DoCO:FigureBox" id="Fx123">
            <image class="DoCO:Figure" src="62j1.page_007.image_05.png" thmb="62j1.page_007.image_05-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="124" confidence="possible" page="7" column="1">-15 (a) PowerPoint 80 70</region>
          <region class="unknown" id="125" page="7" column="1">60 50 40 30 20 10 0</region>
          <region class="DoCO:FigureBox" id="Fx126">
            <image class="DoCO:Figure" src="62j1.page_007.image_06.png" thmb="62j1.page_007.image_06-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="127" confidence="possible" page="7" column="1">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Mean -10 (b) 3D Shockwave 70</region>
          <region class="unknown" id="128" page="7" column="1">60 50 40 30 20 10</region>
          <region class="DoCO:FigureBox" id="Fx129">
            <image class="DoCO:Figure" src="62j1.page_007.image_07.png" thmb="62j1.page_007.image_07-thumb.png"/>
          </region>
          <region class="DoCO:TextChunk" id="130" confidence="possible" page="7" column="1">0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Mean -10 (c) FIFA Game</region>
          <region class="DoCO:FigureBox" id="F6">
            <caption class="deo:Caption" id="131" page="7" column="1">Figure 6:</caption>
          </region>
          <region class="DoCO:TextChunk" id="132" confidence="possible" page="7" column="1">UDFS: Percentage power improvement over Windows DVFS. The horizontal axes indicate individual users, and the mean. The vertical axes are percentage improvements.</region>
          <region class="DoCO:TextChunk" id="134" page="7" column="1">shown in the second target area, along with a target line. <xref ref-type="fig" rid="F8" id="133" class="deo:Reference">Figure 8</xref> shows a preliminary result for a small user study where the game was parameterized for four VMs, each mapped to a distinct host, and the goal of maximizing efficiency within a bounded amount of CPU. The figure is a histogram where the horizontal axis is the efficiency and the vertical axis is the number of users. Users are differentiated by whether they claimed familiarity with distributed and parallel systems (“Yes”) or not (“No”). The maximum possible efficiency is also shown. It is not 100% due to communication. The effects of random scheduling decisions are also shown. The upshot is that by playing the game, most users can schedule this collection of VMs with at least some efficacy. Although the results for the VM scheduling and mapping games are still extremely preliminary, they suggest that it is possible to develop user interface systems for more complex systems-level decision-making than those described in the previous sections.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="135" page="7" column="1">2.6 Prospects for speculative remote display</h2>
          <region class="DoCO:TextChunk" id="148" page="7" column="1">Remote display systems such as VNC [ <xref ref-type="bibr" rid="R44" id="136" class="deo:Reference">44</xref>], Windows RDP [<xref ref-type="bibr" rid="R40" id="137" class="deo:Reference">40</xref>, <xref ref-type="bibr" rid="R45" id="138" class="deo:Reference">45</xref>], and others allow the interactive, graphi-<marker type="column" number="2"/><marker type="block"/> cal use of a remote computer or virtual machine. At its simplest, such a system can be thought of abstractly as a framebuffer on the client on which the server draws. User events (keystrokes, mouse movements, etc.) flow from client to server, and screen events (commands to update the framebuffer) flow from server to client. Unfortunately, current remote display systems, even current research systems like THINC [<xref ref-type="bibr" rid="R2" id="146" class="deo:Reference">2</xref>], suffer when the network latency is high and/or variable [<xref ref-type="bibr" rid="R25" id="147" class="deo:Reference">25</xref>]. We are exploring extending remote display systems in line with the client/server principles given in the introduction. In our model, the client predicts screen events based on past user and screen events. If the predicted screen events re-</region>
          <region class="DoCO:FigureBox" id="F7">
            <image class="DoCO:Figure" src="62j1.page_007.image_08.png" thmb="62j1.page_007.image_08-thumb.png"/>
            <caption class="deo:Caption" id="141" page="7" column="2">Figure 7: Preliminary user interface for the VM scheduling game.</caption>
          </region>
          <region class="unknown" id="142" page="7" column="2">4 VMs with 1-to-1 VM-to-host mapping 4.5 Users who answered NO 4 Users who answered YES Offiline simulation with random input 3.5 Offline simulation with best schedules users 3 of 2.5 Number 1.5 2 1</region>
          <region class="DoCO:FigureBox" id="Fx143">
            <image class="DoCO:Figure" src="62j1.page_007.image_10.png" thmb="62j1.page_007.image_10-thumb.png"/>
          </region>
          <region class="unknown" id="144" page="7" column="2">0.5 0 (0.1, 0.2] (0.2, 0.3] (0.3, 0.4] (0.4, 0.5] (0.5, 0.6] (0.6, 0.7] (0.7, 0.8] (0.8, 0.9] 0.925 Average global efficiency [0.0 - 1.0]</region>
          <region class="DoCO:FigureBox" id="F8">
            <caption class="deo:Caption" id="145" page="7" column="2">Figure 8: Example user study results for the VM scheduling game.</caption>
          </region>
          <outsider class="DoCO:TextBox" type="page_nr" id="149" page="7" column="2">7</outsider>
          <region class="DoCO:TextChunk" id="150" confidence="possible" page="8" column="1">Client Server Keystrokes, Mouse ops Bitmaps, Color tables, Blit Cache ops, Ordering ops, Drawing ops, etc (a) RDP User Sensitivity Client Server Undo Log Keystrokes, Mouse ops Cache User Event Predictor Screen Event Color Bitmaps, Predictor tables, Blit ops, Speculative ops and parameters Ordering ops, Undo Requests, Compare Drawing Bitmaps, Color tables ops, etc (b) Speculative RDP</region>
          <region class="DoCO:FigureBox" id="F9">
            <caption class="deo:Caption" id="151" page="8" column="1">Figure 9: RDP and its proposed speculative variant.</caption>
          </region>
          <region class="DoCO:TextChunk" id="158" page="8" column="1">fer to locally cached constructs (e.g., bitmaps), the client speculatively executes them, undo-logging them as it goes. Most user events, especially those corresponding to fine- grain interactions like typing and menu operation, have their responses computed locally, avoiding the round-trip time. The speculatively executed screen events are compared with those being returned by the server. A difference results in the undo log being used to restore the display to the point preceding the incorrectly executed event, at which point the actual event is executed. The extent of such repair operations and their frequency depends on how aggressive the predictor is, which will be set by the user. <xref ref-type="fig" rid="F9(a)" id="152" class="deo:Reference">Figure 9(a)</xref> shows the current structure of an RDP client and server in simpli- fied form, while <xref ref-type="fig" rid="F9(b)" id="153" class="deo:Reference">Figure 9(b)</xref> illustrates the structure of a speculative variant of RDP. Although we have not yet completed a speculative remote display prototype, we have demonstrated the excellent prospects of the concept. We instrumented the open-source rdesktop [<xref ref-type="bibr" rid="R7" id="154" class="deo:Reference">7</xref>] client for the the RDP protocol to allow us to capture traces of user events and screen events (including drawing commands like bit blit, text drawing, line drawing, etc) that pass between client and server. We then ran a study in which four randomly selected participants in our resource borrowing study (Section 2.1) repeated their tasks. This gave us trace data for word processing, presentation creation, web browsing, and game playing. We studied the prediction of the user and screen event streams using simple, state-limited, k-th order Markov models, configured simply to predict the next event. The modeling was extremely simple, treating both the type of an event and its parameters collectively as a string. Our preliminary work only examined predicting the next event. Nonetheless, we were extremely surprised to find that we were able to predict extremely well, as shown in <xref ref-type="fig" rid="F10" id="155" class="deo:Reference">Figure 10</xref>. Here, we are showing the percentage of correct predictions as a function<marker type="column" number="2"/><marker type="block"/> of k, given the current event has been seen at least once before. More detailed results are available elsewhere [<xref ref-type="bibr" rid="R46" id="157" class="deo:Reference">46</xref>].</region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="159" page="8" column="2">3. PRINCIPLES FOR THE CLIENT/SERVER CONTEXT</h1>
        <region class="DoCO:TextChunk" id="173" page="8" column="2">We now elaborate on the principles for optimization problems within the client/server context that we summarized in the introduction, building on the experiences reported in Section 2, and on the results and experiences of others. User variation: There is considerable variation in user satisfaction with any given operating point. This principle is directly supported by the evidence in Sections 2.1 and 2.2. It is also born out by work studying user annoyance with interfaces [<xref ref-type="bibr" rid="R22" id="160" class="deo:Reference">22</xref>, <xref ref-type="bibr" rid="R43" id="161" class="deo:Reference">43</xref>] and latency tolerance [<xref ref-type="bibr" rid="R23" id="162" class="deo:Reference">23</xref>, <xref ref-type="bibr" rid="R12" id="163" class="deo:Reference">12</xref>] within the human-computer interaction community. Other results include limited user-level customization of GUIs [<xref ref-type="bibr" rid="R37" id="164" class="deo:Reference">37</xref>, <xref ref-type="bibr" rid="R11" id="165" class="deo:Reference">11</xref>]. The systems community has used latency to evaluate operating systems [<xref ref-type="bibr" rid="R13" id="166" class="deo:Reference">13</xref>], and developed initial models for interactive user workload [<xref ref-type="bibr" rid="R3" id="167" class="deo:Reference">3</xref>]. User-specified performance: The user can and should inform the systems software of his satisfaction with the current delivered performance, which results from the current operating point. This principle follows from the first principle. By adapting to the individual user, systems software can choose an operating point that increases user satisfaction and most efficiently uses available resources. We first explained this concept in an HPDC 2004 paper [<xref ref-type="bibr" rid="R20" id="168" class="deo:Reference">20</xref>]. In Sections 2.3 and 2.4 we illustrated two specific systems that directly incorporate user-specified performance or satisfaction. The concept has also begun to see some interest within the adaptive systems community [<xref ref-type="bibr" rid="R50" id="169" class="deo:Reference">50</xref>], and recent power management work has sought to support per-user information to some extent. In the power management community, measured response times are typically used as a proxy for the user [<xref ref-type="bibr" rid="R36" id="170" class="deo:Reference">36</xref>, <xref ref-type="bibr" rid="R60" id="171" class="deo:Reference">60</xref>] and in modern systems like Vertigo [<xref ref-type="bibr" rid="R15" id="172" class="deo:Reference">15</xref>] these measurements can be inferred from unmodified applications and optimized in the context of a per-user profile. Our notion of user-specified performance differs from that proposed in prior work in two fundamental ways. First, we interact directly with users at run-time to measure their satisfaction with performance. Second, we do not decouple user perceivable measurements from satisfaction. One can argue that a multi-step process exists: operating point → OS- level performance metrics (e.g., message timings) → user- level performance metrics (e.g., latency) → user satisfaction. Our principle is simpler: operating point → user satisfaction. As a consequence, it eliminates numerous sources of error from the control–feedback loop connecting user and operating point control system. User-system interface: The interface through which the user interacts with the systems software must be simple, elegant, and understandable even by na ̈ ıve users. Because the user is to be involved in the online systems-level decision- making that determines performance and correctness, ease of use is paramount. Note that we are asking the user to provide input to the lowest level operating system services. Sections 2.3 and 2.4 gave specific examples of such interfaces, as well as their evaluation. While the topic of user interface design and evaluation is a deep and complex one, it is important to note that any effective user interface for systems software will have to be very simple and thin—the systems interface must minimally distract from the applica-</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="174" page="8" column="2">8</outsider>
        <region class="DoCO:TextChunk" id="175" confidence="possible" page="9" column="1">(a) User Events</region>
        <region class="DoCO:FigureBox" id="F10">
          <caption class="deo:Caption" id="176" page="9" column="1">Figure 10: Performance of simple, 1000 state, k-th order Markov model in RDP.</caption>
        </region>
        <region class="DoCO:TextChunk" id="177" page="9" column="1">tion interface. This reduces the design space considerably, and makes evaluation easier. In cases where applications are themselves non-interactive, the user interface for this mode of operation can be much thicker (Section 2.5). Learning: The interface through which the user interacts with the systems software must learn user actions and preferences so that interactions become rarer over time. The importance of learning, the final principle, follows also from the interface requirements. Even an elegant interface that is understandable by na ̈ ıve users would be intrusive if the user had to interact with it frequently. We believe it is necessary to develop and apply machine learning techniques to, over time, learn the individual user’s operating point → satisfaction characteristics. Sections 2.3 and 2.4 illustrated systems that use very simple learning techniques to reduce interaction rates. Section 2.6 illustrated the use of more complex and expensive techniques to predict user-visible system events. Learning techniques and interface designs interact in complex ways. We are only at the first stages at determining just how parsimoniously we can use the user’s attention.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="178" page="9" column="1">4. GENERALIZATION AND ADVICE</h1>
        <region class="DoCO:TextChunk" id="179" page="9" column="1">Although the experiences we reported (Section 2) and the principles we drew from them (Section 3) are specific to the optimization problems in the client/server environment, we believe that it is possible to generalize from them to the broader context of experimental computer systems research. In particular, to reiterate the second paragraph of the introduction, we advocate that researchers should (1) incorporate user studies into the evaluation of their systems, and (2) consider approaches to systems problems that leverage direct input from the user. In the following, we first elaborate on these two themes, and then offer advice to those who would like to operationalize them in their own work.</region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="180" page="9" column="1">4.1 Generalizing</h2>
          <region class="DoCO:TextChunk" id="181" page="9" column="1">In our experience reports, we focused on user satisfaction in the client/server context. Of course, user satisfaction, and the idea of explicitly eliciting it as feedback from the end-user, can be broadly applied. Even for systems that operate outside the timescale of human attention, we can collect trace information, compute metrics on it, and elicit user satisfaction with those metrics. A key finding that we are sure resonates well beyond the client/server context is that of considerable variation</region>
          <region class="DoCO:TextChunk" id="182" confidence="possible" page="9" column="2">(b) Screen Events</region>
          <region class="DoCO:TextChunk" id="183" page="9" column="2">among users in their satisfaction with any given configuration choice. If there is any single take-away message, it is that systems researchers need to consider the individual user. For those systems that can be modeled as, or include a control system element, the individual user can be thought of in at least three ways. First, the user can provide the “set point” for the system. Second, the user can provide the “error signal” of the system. The latter is essentially the approach we have taken in the reported work on power management (Section 2.4). Finally, the user can be a part of, or the whole of the control mechanism itself, determining not only when the “error” is too large, but also determining the configuration that will reduce it. It is this model that we used in the reported work in scheduling VMs (Section 2.3) and in the broader adaptation problem (Section 2.5). Regardless of how or even whether explicit or implicit user feedback is incorporated into systems, it is clear that as systems increasingly face users, it is vital that they be evaluated, at least in part, through user studies.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="184" page="9" column="2">4.2 Advice</h2>
          <region class="DoCO:TextChunk" id="192" page="9" column="2">Evaluating a system via a user study is a different chal- lenge from evaluating it using a synthetic or trace workload and straightforwardly visible metrics. Having done a number of such studies so far, we can offer the following advice: • You should engage an expert in psychological studies or human computer interaction. Effective user studies require extremely careful controls and structure because human subjects approach an experiment at many different cognitive levels, and independent goals. At minimum, refer to the literature (for example, [ <xref ref-type="bibr" rid="R42" id="185" class="deo:Reference">42</xref>, <xref ref-type="bibr" rid="R5" id="186" class="deo:Reference">5</xref>, <xref ref-type="bibr" rid="R24" id="187" class="deo:Reference">24</xref>]). The advice of Don Norman, Benjamin Watson, and Bruce Gooch was very important to our work. • Institutional review boards (IRBs) may have to be engaged depending on the nature of the user study. When explaining our work to colleagues, we often have heard expressed the fear that IRB involvement could become a colossal time sink. While this is possible, it is important to note a few things. First, basic IRB certification is relatively standardized and quite easy to acquire. Second, user studies in the systems context are generally classified as social science-based studies, which require considerably less paperwork than physical, interventionist studies, which is where most of<marker type="page" number="10"/><marker type="column" number="1"/><marker type="block"/> the horror stories lie. Third, the paperwork required for IRB review, although tedious, tends to be quite reusable. Finally, in many cases, because a user study in the systems context is patently unlikely to lead to psychological damage, full review is unnecessary. • User studies are invariably much smaller than the kinds of evaluations that we, as systems researchers, are familiar with. This limits the range of what can be studied, and it requires that small sample size, robust statistics [<xref ref-type="bibr" rid="R21" id="190" class="deo:Reference">21</xref>], or full data reporting, are necessary. In our experience up to now, the effects that are measured have been quite large, which makes it possible to draw strong conclusions despite the small sample size. • Although it is nearly impossible to engage a random subject population, there are techniques, like subsam- pling, that can be used to estimate selection biases. • It is vital, especially when measuring user satisfaction, to differentiate between the actual effect and the background effect. The measured satisfaction (through sur- veying, level eliciting, etc) integrates satisfaction with your system with the user’s general satisfaction. The equivalent of a placebo is needed to differentiate the two. • It is vital that a user study be double-blinded to the greatest extent possible. Users and proctors can inad- vertently produce overly optimistic or pessimistic behaviors if they can infer the “desired” outcome of the test. • Ideally, we want to correlate systems-level quantities that can be easily measured with user study results in order to validate the latter. However, this is often difficult, if not impossible, due to the considerable variation in user responses. For example, as we have seen, user satisfaction with particular level of resources tends to have tremendous variation (Section 2.1). Even when we do not have a system-level quantity to measure, we can use the technique of de- ception [<xref ref-type="bibr" rid="R52" id="191" class="deo:Reference">52</xref>] to convince the user that we do. For example, in the results of Section 2.3, we video-taped users in some tests, and claimed that a fictitious psychology collaborator would analyze the video tape to produce an independent assessment of user satisfaction. Com- paring results in which we deceive the user into this belief, with those where we do not, helps us discount the possibility that the user is being uncooperative. • Eliminate all user-visible extraneous information during any user study. While it is tempting for us to build interfaces that provide as much detail as the user wants, this can dramatically skew results. For example, in the preparatory work for the study described in Section 2.1, we noticed users on one of our two identical test machines were generally more irritated by disk bandwidth borrowing. It turned out that this machine had a visible hard disk access light, while the other did not. Our second claim is that systems researchers should consider using direct user input in their systems. We have the following advice regarding the interfaces for doing so.</region>
          <outsider class="DoCO:TextBox" type="page_nr" id="189" page="9" column="2">9</outsider>
          <region class="DoCO:TextChunk" id="194" confidence="possible" page="10" column="2">• We have generally found that “out-of-band” input devices tend to work best. By out-of-band, we mean that we add input hardware or use existing input hardware that is not used for any other purpose. The interface to the system software will (ideally) be infrequently used. If we layer it on top of an existing input device, the user has to essentially perform a “cognitive context switch” to use it, unlearning the normal purpose of the interface. • It is tempting to ask for a lot of input (in terms of frequency or dimensionality or both) from the user, but this should be avoided. Use as little input as possible, and evaluate the tradeoff between the amount of input and its utility very carefully. • Similarly, the output portion of the interface should be as thin as possible. In some cases, it can be nonexistent as it is the performance of the system that serves as implicit output. • It is important to understand, and account for, the fact that explicit user input can itself be a source of user dissatisfaction. When measuring the efficacy of a system based on explicit user feedback, the experimenter must have an independent gauge of this source. • For some systems and some users, it may be possible to eliminate all user input through the use of implicit measures of the user, for example by recognizing pat- terns in system-level events [<xref ref-type="bibr" rid="R58" id="193" class="deo:Reference">58</xref>]. Note that explicit feedback systems can be used as a yardstick in evaluating implicit ones. Furthermore, these are not ei- ther/or propositions. There is a spectrum that ranges from explicit feedback, to explicit feedback with learning, to implicit feedback. The latter can fall back on the former.</region>
        </section>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="195" page="10" column="2">5. CONCLUSION</h1>
        <region class="DoCO:TextChunk" id="196" page="10" column="2">We have advocated that experimental computer systems researchers should (a) incorporate user studies into the evaluation of our systems, and (b) consider approaches to systems problems that draw on feedback or other input from the end-user. Through our experiences in applying these ideas in six different systems projects in client/server computing and related areas, we illustrated how the ideas can help us find and exploit new, and often surprising, opportunities and principles. Two particular principles we derived is that there is considerable variation in user satisfaction with any given operating point, and that this variation can be exploited through interfaces that use direct user feedback about satisfaction. We then generalized our results and of- fered advice to practitioners who want to apply (a) and (b) to their own systems and domains.</region>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="197" page="10" column="2">6. REFERENCES</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="198" page="10" column="2">[1] Anderson, T. E., Culler, D. E., and Patterson, D. A. A case for networks of workstations. IEEE Micro (February 1995).</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="199" page="10" column="2">[2] Barratto, R., Kim, L., and Nieh, J. Thinc: A virtual display architecture for thin-client computing. In Proceedings of the 20th ACM Symposium on Operating Systems Principles (SOSP) (October 2005).</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="201" page="11" column="1">[3] Bhola, S., and Ahamad, M. Workload modeling for highly interactive applications. In ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems (1999), pp. 210–211. Extended version as Technical Report GIT-CC-99-2, College of Computing, Georgia Tech.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="202" page="11" column="1">[4] Brakmo, L. S., O’Malley, S. W., and Peterson, L. L. TCP Vegas: New Techniques for Congestion Detection and Avoidance. In Proceedings of the Conference on Communications Architectures, Protocols and Applications (1994), pp. 24–35.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="203" page="11" column="1">[5] Card, S., Moran, T., and Newell, A. The Psychology of Human-Computer Interaction. Lawrence Erlbaum Publishers, 1986.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="204" page="11" column="1">[6] Chandra, R., Zeldovich, N., Sapuntzakis, C., and Lam, M. The collective: A cache-based system management architecture. In Proceedings of the 2nd Symposium on Networked Systems Design and Implementation (NSDI) (May 2005).</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="206" page="11" column="1">[7] Chapman, M. rdesktop: A remote desktop protocol client for accessing windows nt terminal server. <ext-link ext-link-type="uri" href="http://www.rdesktop.org" id="205">http://www.rdesktop.org</ext-link>.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="207" page="11" column="1">[8] Chien, A. A., Calder, B., Elbert, S., and Bhatia, K. Entropia: architecture and performance of an enterprise desktop grid system. Journal of Parallel and Distributed Computing 63, 5 (2003), 597–610.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="208" page="11" column="1">[9] Curtin, M., and Dolske, J. A brute force search of DES keyspace. ;login: (May 1998).</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="209" page="11" column="1">[10] Dinda, P. A. The statistical properties of host load. Scientific Programming 7, 3,4 (1999). A version of this paper is also available as CMU Technical Report CMU-CS-TR-98-175. A much earlier version appears in LCR ’98 and as CMU-CS-TR-98-143.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="210" page="11" column="1">[11] Dourish, P. Evolution in the adoption and use of collaborative technologies.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="211" page="11" column="1">[12] Embley, D. W., and Nagy, G. Behavioral aspects of text editors. ACM Computing Surveys 13, 1 (January 1981), 33–70.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="212" page="11" column="1">[13] Endo, Y., Wang, Z., Chen, J. B., and Seltzer, M. Using latency to evaluate interactive system performance. In Proceedings of the 1996 Symposium on Operating Systems Design and Implementation (1996).</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="213" page="11" column="1">[14] Fall, K., and Floyd, S. Simulation-based comparisons of Tahoe, Reno and SACK TCP. SIGCOMM Computer Communication Review 26, 3 (1996), 5–21.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="215" page="11" column="1">[15] Flautner, K., and Mudge, T. Vertigo: Automatic Performance-setting for Linux. SIGOPS Oper. Syst. Rev. 36, SI (2002), 105–116. <ext-link ext-link-type="uri" href="http://doi.acm.org/10.1145/844128.844139." id="214">http://doi.acm.org/10.1145/844128.844139.</ext-link></ref>
          <ref rid="R16" class="deo:BibliographicReference" id="216" page="11" column="1">[16] Frey, J., Tannenbaum, T., Foster, I., Livny, M., and Tuecke, S. Condor-g: A computation management agent for multi-institutional grids. In Proceedings of the 10th International Symposium on High Performance Distributed Computing (HPDC 2001) (2001), pp. 55–66.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="217" page="11" column="1">[17] Gerbessiotis, A. V., and Valiant, L. G. Direct bulk-synchronous parallel algorithms. Journal of Parallel and Distributed Computing 22, 2 (1994), 251–267.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="219" page="11" column="1">[18] Google Corporation. Google compute. <ext-link ext-link-type="uri" href="http://toolbar.google.com/dc/." id="218">http://toolbar.google.com/dc/.</ext-link></ref>
          <ref rid="R19" class="deo:BibliographicReference" id="220" page="11" column="2">[19] Gupta, A., Lin, B., and Dinda, P. A framework and toolkit for understanding user comfort with resource borrowing. Tech. Rep. NWU-CS-04-28, Department of Computer Science, Northwestern University, February 2004.</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="221" page="11" column="2">[20] Gupta, A., Lin, B., and Dinda, P. A. Measuring and understanding user comfort with resource borrowing. In Proceedings of the 13th IEEE International Symposium on High Performance Distributed Computing (HPDC 2004) (June 2004).</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="222" page="11" column="2">[21] Huber, P. Robust Statistics. Wiley and Sons, 2003.</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="223" page="11" column="2">[22] Klein, J. T. Computer response to user frustration. Master’s thesis, Massachusetts Institute of Technology, 1999.</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="224" page="11" column="2">[23] Komatsubara, A. Psychological upper and lower limits of system response time and user’s preferance on skill level. In Proceedings of the 7th International Conference on Human Computer Interaction (HCI International 97) (August 1997), G. Salvendy, M. J. Smith, and R. J. Koubek, Eds., vol. 1, IEE, pp. 829–832.</ref>
          <ref rid="R24" class="deo:BibliographicReference" id="225" page="11" column="2">[24] Kuniavsky, M. Observing the User Experience: A Practioner’s Guide to User Research. Morgan Kaufmann, 2003.</ref>
          <ref rid="R25" class="deo:BibliographicReference" id="226" page="11" column="2">[25] Lai, A., and Nieh, J. Limits of wide-area thin-client computing. In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems (2002).</ref>
          <ref rid="R26" class="deo:BibliographicReference" id="229" page="11" column="2">[26] Larson, S. M., Snow, C. D., Shirts, M., and Pande, V. S. <email id="227">Folding@home</email> and <email id="228">genome@home</email>: Using distributed computing to tackle previously intractable problems in computational biology. In Computational Genomics, R. Grant, Ed. Horizon Press, 2002.</ref>
          <ref rid="R27" class="deo:BibliographicReference" id="230" page="11" column="2">[27] Lin, B. Human-directed adaptation. Thesis Proposal, Department of Electrical Engineering and Computer Science, Northwestern University, October 2005.</ref>
          <ref rid="R28" class="deo:BibliographicReference" id="231" page="11" column="2">[28] Lin, B., and Dinda, P. User-driven scheduling of interactive virtual machines. In Proceedings of the Fifth International Workshop on Grid Computing (November 2004).</ref>
          <ref rid="R29" class="deo:BibliographicReference" id="232" page="11" column="2">[29] Lin, B., and Dinda, P. Vsched: Mixing batch and interactive virtual machines using periodic real-time scheduling. In Proceedings of ACM/IEEE SC (Supercomputing) (November 2005).</ref>
          <ref rid="R30" class="deo:BibliographicReference" id="233" page="11" column="2">[30] Lin, B., and Dinda, P. Putting the user in direct control of cpu scheduling. Tech. Rep. NWU-EECS-06-07, Department of Electrical Engineering and Computer Science, Northwestern University, July 2006.</ref>
          <ref rid="R31" class="deo:BibliographicReference" id="234" page="11" column="2">[31] Lin, B., and Dinda, P. Towards scheduling virtual machines based on direct user input. In Proceedings of the 1st International Workshop on Virtualization Technology in Distributed Computing (VTDC) (2006).</ref>
          <ref rid="R32" class="deo:BibliographicReference" id="235" page="11" column="2">[32] Lin, B., Mallik, A., Dinda, P., Memik, G., and Dick, R. Power reduction through measurement and modeling of users and cpus: Summary. In Proceedings of the 2007 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems (June 2007).</ref>
          <ref rid="R33" class="deo:BibliographicReference" id="238" page="11" column="2">[33] Litzkow, M., Livny, M., and Mutka, M. W. Condor — a hunter of idle workstations. In <marker type="page" number="12"/><marker type="column" number="1"/><marker type="block"/> Proceedings of the 8th International Conference of Distributed Computing Systems (ICDCS ’88) (June 1988), pp. 104–111.</ref>
          <ref rid="R34" class="deo:BibliographicReference" id="239" page="12" column="1">[34] Liu, C. L., and Layland, J. W. Scheduling algorithms for multiprogramming in a hard real-time environment. Journal of the ACM 20, 1 (January 1973), 46–61.</ref>
          <ref rid="R35" class="deo:BibliographicReference" id="240" page="12" column="1">[35] Liu, J. Real-time Systems. Prentice Hall, 2000.</ref>
          <ref rid="R36" class="deo:BibliographicReference" id="241" page="12" column="1">[36] Lorch, J. R., and Smith, A. J. Using User Interface Event Information in Dynamic Voltage Scaling Algorithms. In Technical Report UCB/CSD-02-1190, Computer Science Division, EECS, University of California at Berkeley, August (2002). citeseer.ist.psu.edu/lorch03using.html.</ref>
          <ref rid="R37" class="deo:BibliographicReference" id="242" page="12" column="1">[37] MacLean, A., Carter, K., Lovstrand, L., and Moran, T. User-tailorable systems: pressing the issues with buttons. In CHI ’90: Proceedings of the SIGCHI conference on Human factors in computing systems (New York, NY, USA, 1990), ACM Press, pp. 175–182.</ref>
          <ref rid="R38" class="deo:BibliographicReference" id="243" page="12" column="1">[38] Mallik, A., Lin, B., Dinda, P., Memik, G., and Dick, R. Process and user driven dynamic voltage and frequency scaling. Tech. Rep. NWU-EECS-06-11, Department of Electrical Engineering and Computer Science, Northwestern University, August 2006.</ref>
          <ref rid="R39" class="deo:BibliographicReference" id="244" page="12" column="1">[39] Mallik, A., Lin, B., Memik, G., Dinda, P., and Dick, R. User-driven frequency scaling. IEEE Computer Architecture Letters 5, 2 (2006).</ref>
          <ref rid="R40" class="deo:BibliographicReference" id="245" page="12" column="1">[40] Microsoft. Remote desktop protocol (rdp) features and performance. Tech. rep., 2000.</ref>
          <ref rid="R41" class="deo:BibliographicReference" id="246" page="12" column="1">[41] Mutka, M. W., and Livny, M. The available capacity of a privately owned workstation environment. Performance Evaluation 12, 4 (July 1991), 269–284.</ref>
          <ref rid="R42" class="deo:BibliographicReference" id="247" page="12" column="1">[42] Proctor, R., and Van Zandt, T. Human Factors in Simple and Complex Systems. Allyn and Bacon, 1993.</ref>
          <ref rid="R43" class="deo:BibliographicReference" id="249" page="12" column="1">[43] Reynolds, C. J. The sensing and measurement of frustration with computers. Master’s thesis, Massachusetts Institute of Technology Media Laboratory, 2001. <ext-link ext-link-type="uri" href="http://www.media.mit.edu/" id="248">http://www.media.mit.edu/</ext-link>∼carsonr/pdf/sm thesis.pdf.</ref>
          <ref rid="R44" class="deo:BibliographicReference" id="250" page="12" column="1">[44] Richardson, T., Stafford-Fraser, Q., Wood, K., and Hopper, A. Virtual network computing. IEEE Internet Computing 2, 1 (January/February 1998).</ref>
          <ref rid="R45" class="deo:BibliographicReference" id="251" page="12" column="1">[45] Romano, P. Itu-t recommendation t.128 (application sharing). Tech. rep., ITU, March 1997.</ref>
          <ref rid="R46" class="deo:BibliographicReference" id="252" page="12" column="1">[46] Rossoff, S., and Dinda, P. Prospects for speculative remote display. Tech. Rep. NWU-EECS-06-08, Department of Electrical Engineering and Computer Science, Northwestern University, August 2006.</ref>
          <ref rid="R47" class="deo:BibliographicReference" id="253" page="12" column="1">[47] Ruth, P., McGachey, P., Jiang, X., and Xu, D. Viocluster: Virtualization for dynamic computational domains. In Proceedings of the IEEE International Conference on Cluster Computing (Cluster) (September 2005).</ref>
          <ref rid="R48" class="deo:BibliographicReference" id="254" page="12" column="1">[48] Satyanarayanan, M., Kozuch, M., Helfrich, C., and O’Hallaron, D. Towards seamless mobility on pervasive hardware. Pervasive and Mobile Computing 1, 2 (June 2005), 157–189.</ref>
          <ref rid="R49" class="deo:BibliographicReference" id="255" page="12" column="2">[49] Shoykhet, A., Lange, J., and Dinda, P. Virtuoso: A system for virtual machine marketplaces. Tech. Rep. NWU-CS-04-39, Department of Computer Science, Northwestern University, July 2004.</ref>
          <ref rid="R50" class="deo:BibliographicReference" id="256" page="12" column="2">[50] Sousa, J., Balan, R., Poladian, V., Garlan, D., and Satyanarayanan, M. Giving users the steering wheel for guiding resource-adaptive systems. Tech. Rep. CMU-CS-05-198, Department of Computer Science, Carnegie Mellon University, December 2005.</ref>
          <ref rid="R51" class="deo:BibliographicReference" id="257" page="12" column="2">[51] Stevens, W. TCP Slow Start, Congestion Avoidance, Fast Retransmit and Fast Recovery Algorithms. In Internet RFC 2001 (1997).</ref>
          <ref rid="R52" class="deo:BibliographicReference" id="258" page="12" column="2">[52] Stricker, L. J. The true deceiver. Psychological Bulletin, 68 (1967), 13–20.</ref>
          <ref rid="R53" class="deo:BibliographicReference" id="259" page="12" column="2">[53] Sullivan, W. T., Werthimer, D., Bowyer, S., Cobb, J., Gedye, D., and Anderson, D. A new major seti project based on project serendip data and 100,000 personal computers. In Proceedings of the Fifth International Conference on Bioastronomy (1997), C. Cosmovici, S. Bowyer, and D. Werthimer, Eds., no. 161 in IAU Colloquim, Editrice Compositori, Bologna, Italy.</ref>
          <ref rid="R54" class="deo:BibliographicReference" id="260" page="12" column="2">[54] Sundararaj, A. Automatic, Run-time, and Dynamic Adaptation of Distributed Applications Executing in Virtual Environments. PhD thesis, Northwestern University, December 2006. Technical Report NWU-EECS-06-18, Department of Electrical Engineering and Computer Science.</ref>
          <ref rid="R55" class="deo:BibliographicReference" id="261" page="12" column="2">[55] Sundararaj, A., Gupta, A., and Dinda, P. Increasing application performance in virtual environments through run-time inference and adaptation. In Proceedings of the 14th IEEE International Symposium on High Performance Distributed Computing (HPDC) (July 2005).</ref>
          <ref rid="R56" class="deo:BibliographicReference" id="262" page="12" column="2">[56] Sundararaj, A., Sanghi, M., Lange, J., and Dinda, P. An optimization problem in adaptive virtual environments. In Proceedings of the Seventh Workshop on Mathematical Performance Modeling and Analysis (MAMA) (June 2005).</ref>
          <ref rid="R57" class="deo:BibliographicReference" id="263" page="12" column="2">[57] Sundararaj, A., Sanghi, M., Lange, J., and Dinda, P. Hardness of approximation and greedy algorithms for the adaptation problem in virtual environments. In Proceedings of the 3rd IEEE International Conference on Autonomic Computing (ICAC) (2006).</ref>
          <ref rid="R58" class="deo:BibliographicReference" id="264" page="12" column="2">[58] Theocharous, G., Mannor, S., Shah, N., Gandhi, P., Kveton, B., Siddiqi, S., and Yu, C.-H. Machine learning for adaptive power management. Intel Technology Journal 10, 4 (Nov. 2006).</ref>
          <ref rid="R59" class="deo:BibliographicReference" id="265" page="12" column="2">[59] Wang, Z., and Crowcroft, J. Eliminating Periodic Packet Losses in the 4.3-Tahoe BSD TCP Congestion Control Algorithm. In ACM Computer Communications Review (1992).</ref>
          <ref rid="R60" class="deo:BibliographicReference" id="266" page="12" column="2">[60] Yan, L., Zhong, L., and Jha, N. K. User-perceived Latency based Dynamic Voltage Scaling for Interactive Applications. In Proceedings of ACM/IEEE Design Automation Conference (2005).</ref>
        </ref-list>
        <outsider class="DoCO:TextBox" type="page_nr" id="200" page="10" column="2">10</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="237" page="11" column="2">11</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="267" page="12" column="2">12</outsider>
      </section>
    </body>
  </article>
</pdfx>
