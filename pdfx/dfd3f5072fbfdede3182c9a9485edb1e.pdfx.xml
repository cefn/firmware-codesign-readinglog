<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>20a06b461da8081304888ad432305900cb4dece69fe041ed68df94e7393889fb</job>
    <base_name>62r6</base_name>
    <doi>http://dx.doi.org/10.1109/mc.2005.314</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="header" id="1">COVER FEATURE</outsider>
      <title-group>
        <article-title class="DoCO:Title" id="2">Test-Driven Development: Concepts, Taxonomy, and Future Direction</article-title>
      </title-group>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="3">David Janzen Simex LLC Hossein Saiedian</name>
        </contrib>
      </contrib-group>
      <region class="unknown" id="4">Test-driven development creates software in very short iterations with minimal upfront design. Poised for widespread adoption, TDD has become the focus of an increasing number of researchers and developers. T The test aspect</region>
      <region class="unknown" id="5">University of Kansas</region>
    </front>
    <body class="DoCO:BodyMatter">
      <region class="DoCO:TextChunk" id="6" page="1" column="1">he test-driven development strategy requires writing automated tests prior to developing functional code in small, rapid iterations. Although developers have been applying TDD in various forms for several decades, 1 this software development strategy has continued to gain increased attention as one of the core extreme programming practices. XP is an agile method that develops object-ori- ented software in very short iterations with little upfront design. Although not originally given this name, TDD was described as an integral XP practice necessary for analysis, design, and testing that also enables design through refactoring, collective ownership, continuous integration, and programmer courage. Along with pair programming and refactoring, TDD has received considerable individual attention since XP’s introduction. Developers have created tools specifically to support TDD across a range of languages, and they have written numerous books explaining how to apply TDD concepts. Researchers have begun to examine TDD’s effects on defect reduction and quality improvements in academic and professional practitioner environments, and educators have started to examine how to inte- grate TDD into computer science and software engineering pedagogy. Some of these efforts have been implemented in the context of XP projects, while others are independent of them.</region>
      <region class="DoCO:TextChunk" id="7" confidence="possible" page="1" column="1">TEST-DRIVEN DEVELOPMENT DEFINED Although its name implies that TDD is a testing</region>
      <region class="DoCO:FigureBox" id="Fx8">
        <image class="DoCO:Figure" src="62r6.page_001.image_01.png" thmb="62r6.page_001.image_01-thumb.png"/>
      </region>
      <region class="DoCO:TextChunk" id="10" page="1" column="2">method, a close examination of the term reveals a more complex picture. <marker type="block"/> In addition to testing, TDD involves writing automated tests of a program’s individual units. A unit is the smallest possible testable software component. There is some debate about what exactly constitutes a unit in software. Even within the realm of object- oriented programming, both the class and method have been suggested as the appropriate unit. Generally, however, the method or procedure is the smallest possible testable software component. Developers frequently implement test drivers and function stubs to support the execution of unit tests. Test execution can be either a manual or automated process and can be performed by developers or des- ignated testers. Automated testing involves writing unit tests as code and placing this code in a test harness or framework such as JUnit. Automated unit testing frameworks minimize the effort of testing, reducing a large number of tests to a click of a button. In contrast, during manual test execution developers and testers must expend effort proportional to the number of tests executed. Traditionally, unit testing occurred after developers coded the unit. This can take anywhere from a few minutes to a few months. The unit tests might be written by the same programmer or by a desig- nated tester. With TDD, the programmer writes the unit tests prior to the code under test. As a result, the programmer can immediately execute the tests after they are written.</region>
      <outsider class="DoCO:TextBox" type="footer" id="11" page="1" column="2">0018-9162/05/$20.00 © 2005 IEEE</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="12" page="1" column="2">P u b l i s h e d b y t h e I E E E C o m p u t e r S o c i e t y</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="13" page="1" column="2">September 2005</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="14" page="1" column="2">43</outsider>
      <region class="DoCO:TextChunk" id="15" confidence="possible" page="2" column="1">The driven aspect Some definitions of TDD imply that it is TDD assumes primarily a testing strategy. For example, in that the software JUnit in Action (Manning Publications, design is either 2003), Vincent Massol and Ted Husted stated incomplete that or pliable Test-driven development (TDD) is a program- and open ming practice that instructs developers to write to changes. new code only if an automated test has failed, and to eliminate duplication. The goal of TDD is ‘clean code that works.’</region>
      <region class="DoCO:TextChunk" id="61" page="2" column="1">However, according to XP and TDD pioneer Ward Cunningham, “Test-first coding is not a testing technique.” TDD is known by various names including test-first programming, test-driven design, and test-first design. The driven in test-driven development focuses on how TDD leads analysis, design, and programming decisions. TDD assumes that the software design is either incomplete or pliable and open to changes. In the context of XP, TDD subsumes many analysis decisions. The customer should be “on-site” in XP. Test writing is one of the first steps in deciding what the program should do, which is essentially an analysis step. The Agile Alliance offers another definition that cap- tures this idea (www.agilealliance.org/programs/ roadmaps/Roadmap/tdd/tdd_index.htm): <marker type="block"/> To promote testing to an analysis and design step the practice of refactoring must be introduced. Refactoring changes the structure of an existing body of code without changing its external behav- ior. A test may pass, but the code may be inflexible or overly complex. By refactoring the code, the test should still pass but the code will be improved. Understanding that TDD is more about analysis and design than it is about testing is one of the most challenging conceptual shifts for new adopters of the practice. Program testing has traditionally assumed the existence of a program. The TDD idea that a test can be written before the program or that test can aid in deciding what program code to write<marker type="column" number="2"/><marker type="block"/> and what that program’s interface should look like is a radical concept for most software developers.<marker type="block"/> The development aspect Intended to aid in constructing software, TDD is not in itself a software development methodology or process model. It is a practice, a way of developing software to be used with other practices, in a particular order and frequency and in the context of a process model. TDD has emerged within a particular set of process models. It can be applied as a microprocess within the context of many different process models. TDD produces a set of automated unit tests that provide some side effects in the development process. The practice assumes the automated tests will not be thrown away once a design decision is made. Instead, the tests become a vital component of the development process, providing quick feedback to any changes to the system. If a change causes a test to fail, the developer knows immediately after making the change, while the test is still fresh in the developer’s mind. Among the drawbacks, that developer must now maintain both the production code and the automated tests.<marker type="block"/> TDD’S HISTORICAL AND MODERN CONTEXTS Despite the lack of attention in undergraduate curriculum and inconclusive reports of usage in industry, a wide range of software tools exist to support testing, making TDD’s emergence possible.<marker type="block"/> Software development methodologies A software development process or methodology defines the order, control, and evaluation of the basic tasks involved in creating software. Software process methodologies range in complexity and control from largely informal to highly structured. Developers classify these methodologies as pre- scriptive or agile and label the specific types as waterfall, spiral, incremental, or evolutionary. When an organization states that it uses a particular methodology, it often applies a combination of smaller, finer-grained methodologies on a project scale instead. For example, an organization might apply an incremental development model, building small, cumulative slices of a project’s fea- tures. In each increment, the developers could apply a waterfall or linear method of determining requirements, designing a solution, coding, testing, and then integrating. Depending on the size of the increments and the waterfall’s time frame, the process could be labeled differently, with potentially very different results in terms of quality and developer<marker type="page" number="3"/><marker type="column" number="1"/><marker type="block"/> satisfaction. If we break a software project into N increments where I i represents each increment, then the equation Σ N i = 1 I i can represent the entire project. If N is reasonably large, we can label this as an incremental project. However, if N ≤ 2, we would label this as a waterfall project. If the increments require modifying a significant amount of overlapping software, we can say that our methodology is more iterative in nature. Specifically, for project P consisting of code C and iterations I=Σ N i = 1 I i , if C i is the code affected by iter- ation I i , and if project P is iterative, then C i ∩ C i + 1 ≠ Θ for most i such that 1 &lt; i &lt; N. Similarly, with incremental and waterfall approaches, we expect a formal artifact, such as a spec- ification document, for documenting the increment’s requirements. If the artifact is informal—say, some whiteboard drawings or an incomplete set of UML diagrams and was generated quickly—we would be working in the context of an agile process. The approach and perspective of the architecture or the design would cause us to label the process aspect- oriented, component-based, or feature-driven. Some individual software developers and smaller teams apply even finer-grained models such as the personal software process or the collaborative software process. The time, formality, and intersection of the steps in software construction can determine the way developers categorize a process methodology. The order in which construction tasks occur influences a project’s label and its quality. Natural and logical, the traditional ordering is requirements elic- itation, analysis, design, code, test, integration, deployment, and maintenance. We could, however, consider some possible reorderings even though most do not make sense. For example, we would never maintain a system that hasn’t been coded. Similarly, we would never code something for which we have no requirements. Requirements do not necessarily imply formal requirements. A requirement can be as simple as an idea in a programmer’s head. Programmers have applied t he prototyping approach when requirements are fuzzy or incomplete. With this approach, we may do very little analysis and design before coding, but ultimately might have to discard the proto- type even though it was a useful tool in determining requirements and evaluating design options. When we closely examine the design, code, and test phases, we see many finer-grained activities. For example, various types of testing take place, including unit, integration, and regression testing. The timing, frequency, and granularity of these tests<marker type="column" number="2"/><marker type="block"/> can vary widely. Some testing can be conducted early—concurrent with other coding The order activities. Test-driven development reorders in which these steps to an advantage. By placing fine- grained unit tests before just enough code to construction satisfy that test, TDD can affect many aspects tasks occur of a software development methodology. influences a project’s label TDD’s historical context and its quality. Test-driven development has emerged in conjunction with the rise of agile process models. Both have roots in the iterative, incremental, and evolutionary process models used as early as the 1950s. In addition, tools have evolved to play a significant role in supporting TDD. Early test, early examples. Research on testing has generally assumed the existence of a program to be tested, implying a test-last approach. Moving tests from the end of coding to the beginning, however, is nothing new. Software and test teams commonly develop tests early in the software development process, often with the program logic. The evaluation and prevention life-cycle models integrated testing early in the software development process nearly two decades ago. Introduced in the 1980s, the Cleanroom approach to software engineering included formal verification of design elements early in the development process. Some claim that NASA’s Project Mercury applied a form of TDD as early as the 1950s. 1 Prior to the introduction of XP in 1998, little had been written about the concept of letting small incremental automated unit tests drive software development and design processes. Despite the lack of published documentation, many developers have probably used a test-first approach informally. Kent Beck claims he “learned test-first programming as a kid while reading a book on programming. It said that you program by taking the input tape ... and typing in the output tape you expect. Then you program until you get the output tape you expect.” 2 Some argue that TDD merely gives a name and definition to a practice that has been sporadically and informally applied for some time. TDD is more than this. As Beck states, XP takes the known best practices and “turns the knobs all the way up to ten.” 2 Many developers might have been thinking and coding in a test-first manner, but TDD does this in an extreme way: always writing tests before code, making tests as small as possible, and never letting code degrade. TDD fits within a process model, and the development of incremental, iterative, and evolutionary process models has been vital to its emergence.<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> Iterative, evolutionary, and incremental development. Iterative development involves repeat- TDD developed ing a set of development tasks, generally within the on an expanding set of requirements. 1 context of Evolutionary approaches involve adaptive iterative, and lightweight iterative development. Being adaptive refers to using feedback from incremental, previous iterations to improve the software. and evolutionary Being lightweight refers to the lack of complete models. specifications at the beginning of development, thus allowing feedback from previous iterations and from customers to guide future iterations. Lightweight can also refer to other aspects such as a process’s level of formality and degree of documentation. The spiral model is an evolutionary approach that incorporates prototyping and the cyclic nature of iterative development with risk-driven-iterations and anchor point milestones. The incremental model produces a series of releases, called increments, that provide more functionality with each increment. TDD developed within the context of such iterative, incremental, and evolutionary models. TDD works because these approaches provide the pre- requisite process models. Beck claims that to implement XP, developers must apply all of the incumbent practices—leaving some out weakens the model and can cause it to fail. 3 TDD requires that design decisions be delayed and flexible to influence software design. Each new test might require refactoring and a design change. Automated tests give programmers the courage to change any code and the information they need to know quickly if something has broken, enabling collective ownership.<marker type="block"/> Automated testing Software tools have become important factors in the development of modern software systems. Tools ranging from compilers, debuggers, and integrated development environments to modeling and com- puter-aided software engineering tools have improved and increased developer productivity. Tools have played an important role in the emergence of TDD, which assumes the existence of an automated unit testing framework. Such a framework simplifies both the creation and execution of software unit tests. Test harnesses, basically automated testing frameworks, provide a combination of test drivers, stubs, and interfaces to other sub- systems. Often such harnesses are custom-built, although commercial tools do exist to assist with test harness preparation. Erich Gamma and Kent Beck developed JUnit, an automated unit testing framework for Java.<marker type="column" number="2"/><marker type="block"/> Essential for implementing TDD with Java, JUnit is arguably responsible for much of TDD and XP’s wide popularity. JUnit-like frameworks have been implemented for several different languages, creating a family of frameworks referred to as xUnit. Generally, xUnit lets a programmer write sets of automated unit tests that initialize, execute, and make assertions about the code under test. Individual tests are independent of one another so test order does not matter. The programmer reports the total number of successes and failures. xUnit tests are written in the same language as the code under test and thus serve as first-class clients of the code, while tests can actually serve as documentation for it. On the other hand, because developers implement xUnit in the target language, that language determines the tool’s relative simplicity and flexi- bility. For example, JUnit is simple and portable partly because it takes advantage of Java’s porta- bility through the bytecode/virtual machine architecture. It uses Java’s ability to load classes dynamically and exploits Java’s reflection mecha- nism to automatically discover tests. In addition, JUnit provides a portable graphical user interface that has been integrated into popular integrated development environments such as Eclipse. A wide range of additional tools have emerged to support automated testing, particularly in Java. Some tools simplify the creation of mock objects, or stubs. The stubs replace the needed collaborat- ing objects so that developers can test a particular object. They can use other tools such as Cactus and Derby with JUnit to automate tests that involve J2EE components or databases. The proliferation of software tools that support TDD shows that it has widespread support and will likely become an established approach. JUnit’s simplicity and elegance have been a significant factor in TDD’s use, particularly in the Java community. Programmers can develop unit tests easily and execute large test suites with a single button click, yielding quick results about the system’s state.<marker type="block"/> Early testing in academia The undergraduate computer science and software engineering curriculum provides one indicator of a software practice’s widespread acceptance. Sometimes academia has led practice in the field, sometimes it has followed. Software engineering, iterative development, and TDD all seem to follow this latter model. Much software engineering research originated in academia and then found its way into common<marker type="page" number="5"/><marker type="column" number="1"/><marker type="block"/> practice. The undergraduate computer science and software engineering curriculum, however, tends to reflect or even lag behind common practice in industry. The choice of programming language has commonly followed business needs. Process models developed in practice later become reflected in curricula. The 1991 ACM Curriculum Guidelines recommended giving fewer than eight hours each of lec- ture and lab time to iterative development processes and verification and validation. The 2001 guide- lines recommended giving an even smaller amount of time to development processes and software val- idation—two and three hours each, respectively. Undergraduate texts give little attention to com- parative process models. Texts provide limited coverage of software design and testing techniques. The topics of software design and testing are often relegated to a software engineering course that may not be mandatory for all students. Extreme programming’s place in undergraduate education has been the topic of much debate. Some argue strongly for using XP to introduce software engineering to undergraduates. Others argue that XP and agile methods offer only limited benefits. Given the different opinions on using XP in the undergraduate curriculum, TDD has received limited exposure at this level. Some educators have called for increased design and testing coverage. Others see TDD as an opportunity to incorporate testing in the curriculum, rather than relegating it to an individual course. TDD tools have, however, found their way into early programming education. BlueJ, a popular environment for learning Java, incorporates JUnit and adds help for building test cases at an early stage in a programmer’s learning cycle. Proponents have advocated using JUnit for early learning of Java because it abstracts the bootstrapping mech- anism of main(), allowing the student to concen- trate on the use of objects early. TDD has yet to achieve widespread acceptance in academia, at least partly because faculty who do not specialize in software engineering are not likely to be familiar with it. TDD instructional materials that target undergraduate courses remain basically nonexistent.<marker type="block"/> Recent context XP and agile methods have received much attention in the past few years. Even though conclusive documentation is lacking, anecdotal evidence indicates that TDD usage is rising. Agile methods. These methods clearly have roots in the incremental, iterative, and evolutionary<marker type="column" number="2"/><marker type="block"/> methods. Pekka Abrahamsson and col- leagues 4 provide an evolutionary map of nine agile methods and describe how they XP proposes focus on simplicity and speed while empha- using TDD as sizing people over processes. an integral Probably the most well-known agile component method, XP is often used in combination for developing with other agile methods such as Scrum. XP proposes using TDD as an integral compo- high-quality nent for developing high-quality software. software. The highly disciplined practice of TDD and the simple, lightweight nature of agile processes give rise to an interesting conflict. Potential TDD adopters often express concern regarding the time and cost of writing and main- taining unit tests. Although he concedes that automated unit tests are not necessary for absolutely everything, Beck insists that XP cannot work without TDD because it provides the glue that holds the process together. 3 Adoption measures. Measuring the use of a particular software development methodology is hard. Many organizations might be using the methodology without talking about it. Others might claim to be using a methodology when in fact they are misapplying it. Worse yet, they might be advertis- ing its use falsely. Surveys might be conducted to gauge a method’s usage, but often only those who are much in favor or much opposed to the methodology will respond. A 2002 survey reported that out of 32 survey respondents across 10 industry segments, 14 firms used an agile process. 5 Of these, five were in the e-business industry. Most of the projects using agile processes were small, involving 10 or fewer participants and lasting one year or less. A 2003 survey reported that 131 respondents claimed they used an agile method. 6 Of these, 59 percent claimed to be using XP and implied they were using TDD. Both surveys revealed positive results from applying agile methods, with increases in productivity and quality and reduced or minimal changes in costs. XP has accumulated a substantial body of litera- ture. Most of this involves the promotion of XP or explains how to implement it. Many experience reports present only anecdotal evidence of XP’s benefits and drawbacks. Although the existence of these reports indicates that XP is being adopted in many organizations, it remains unclear if these same organizations will continue to use XP over time or, if they have, if they will move on to other methods. Although XP’s popularity implies a growing adoption of TDD, we have no idea how widely it is being used. Organizations may be using XP with-<marker type="page" number="6"/><marker type="column" number="1"/><marker type="block"/> out adopting all of its practices or they may be applying the practices inconsistently. On a project at ThoughtWorks, Jonathan Rasmusson, an early XP adopter, estimates one-third of the code was developed using TDD. 7 In the same report, Rasmusson stated, “If I could only recommend one coding practice to software developers, those who use XP or otherwise, it would be to write unit tests.” In this ThoughtWorks project, developers used 16,000 lines of automated unit tests on 21,000 lines of production code. Many tests were written in both test-first and test-last iterations. Despite the possibility of adopting XP without it, TDD seems to be a core XP practice. Anecdotal evidence indicates that TDD is commonly included when only a subset of XP is adopted. The use of xUnit testing frameworks provides another possible indicator of TDD’s use. JUnit, the first such framework, has enjoyed widespread popularity (www.junit.org). No JUnit adoption statis- tics are directly available. The Eclipse core distribution, a popular integrated development environment primarily used for Java development includes JUnit. A press release issued in February 2004 on the Eclipse Web site states that the Eclipse platform has recorded more than 18 million down- load requests since its inception. Although dupli- cate requests from the same developer can occur, the figure is still substantial. Certainly not all Eclipse developers use JUnit, nor do all JUnit adopters use TDD, but it is likely that the popularity of XP, JUnit, and Eclipse combined implies a certain degree of TDD adoption.<marker type="block"/> EVALUATIVE TDD RESEARCH Since the introduction of XP, many practitioner articles and books on applying TDD have been written. There has been relatively little evaluative research on the benefits and effects of TDD, however. Research on TDD can be categorized broadly by context. In particular, TDD research is classified as industry if the study or research was conducted primarily with professional software practitioners. It is classified as academia if the practitioners are primarily students and the work takes place in the context of an academic setting. Academic research also includes studies in which students work on a project for a company but in the context of an academic course.<marker type="column" number="2"/><marker type="block"/> TDD in industry A few evaluative research studies have been conducted on TDD with professional practitioners. North Carolina State University seems to be the only source of such a study to date. Researchers at NCSU have performed at least three empirical studies on TDD in industry settings involving fairly small groups in at least four different companies. 8-10 These studies examined defect density as a measure of software quality, although some survey data indicated that programmers thought TDD promoted simpler designs. In one study, programmers’ experience with TDD varied from novice to expert, while programmers new to TDD participated in the other studies. These studies showed that programmers using TDD produced code that passed 18 percent to 50 percent more external test cases than code produced by corresponding control groups. The studies also reported less time spent debugging code developed with TDD. Further, they reported that applying TDD had an impact that ranged from minimal to a 16 percent decrease in programmer productivity—which shows that applying TDD sometimes took longer. In the case that took 16 percent more time, researchers noted that the control group wrote far fewer tests than the TDD group. <xref ref-type="table" rid="T1" id="58" class="deo:Reference">Table 1</xref> summarizes these studies and labels each experiment as either a case study or a controlled experiment.<marker type="block"/> TDD in academia Several academic studies have examined XP as a whole, but a few focused on TDD. Although many of the TDD studies published in academic settings are anecdotal, the five studies shown in <xref ref-type="table" rid="T2" id="60" class="deo:Reference">Table 2</xref> specifically report on empirical results. When refer- ring to software quality, all but one 11 study focused on the ability of TDD to detect defects early. Two of the five studies reported significant improvement in software quality and programmer productivity. 11,12 One reported a correlation between the number of tests written and productivity. 13 In this particular study, students using test-first methods wrote more tests and were significantly more pro- ductive. The remaining two studies 14,15 reported no significant improvement in either defect density or productivity. All five of these relatively small studies lasted a semester or less and involved programmers who had little or no previous experience with TDD.</region>
      <region class="unknown" id="17" page="2" column="1">Test-driven development (TDD) is the craft of pro- ducing automated tests for production code, and using that process to drive design and programming. For every tiny bit of functionality in the production code, you first develop a test that specifies and validates what the code will do. You then produce exactly as much code as will enable that test to pass. Then you refactor (simplify and clarify) both the production code and the test code.</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="23" page="2" column="2">44</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="24" page="2" column="2">Computer</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="27" page="3" column="2">September 2005</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="28" page="3" column="2">45</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="33" page="4" column="2">46</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="34" page="4" column="2">Computer</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="38" page="5" column="2">September 2005</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="39" page="5" column="2">47</outsider>
      <region class="DoCO:TableBox" id="T1">
        <caption class="deo:Caption" id="40" page="6" column="1">Table 1. Summary of TDD research in industry.</caption>
      </region>
      <region class="unknown" id="41" page="6" column="1">Number of</region>
      <outsider class="DoCO:TextBox" type="header" id="42" page="6" column="1">Study</outsider>
      <outsider class="DoCO:TextBox" type="header" id="43" page="6" column="1">Type</outsider>
      <outsider class="DoCO:TextBox" type="header" id="44" page="6" column="1">companies</outsider>
      <outsider class="DoCO:TextBox" type="header" id="45" page="6" column="1">Number of programmers</outsider>
      <outsider class="DoCO:TextBox" type="header" id="46" page="6" column="1">Quality effects</outsider>
      <outsider class="DoCO:TextBox" type="header" id="47" page="6" column="1">Productivity effects</outsider>
      <outsider class="DoCO:TextBox" type="header" id="48" page="6" column="1">George 8</outsider>
      <outsider class="DoCO:TextBox" type="header" id="49" page="6" column="1">Controlled experiment</outsider>
      <outsider class="DoCO:TextBox" type="header" id="50" page="6" column="1">3</outsider>
      <outsider class="DoCO:TextBox" type="header" id="51" page="6" column="1">24</outsider>
      <outsider class="DoCO:TextBox" type="header" id="52" page="6" column="1">TDD passed 18% more tests</outsider>
      <outsider class="DoCO:TextBox" type="header" id="53" page="6" column="1">TDD took 16% longer</outsider>
      <region class="unknown" id="54" page="6" column="1">Maximilien 9 Case study 1 9 50% reduction in defect density Williams 10 Case study 1 9 40% reduction in defect density</region>
      <region class="unknown" id="57" page="6" column="2">Minimal impact No change</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="62" page="6" column="2">48</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="63" page="6" column="2">Computer</outsider>
      <region class="DoCO:TableBox" id="T2">
        <caption class="deo:Caption" id="64" page="7" column="1">Table 2. Summary of TDD research in academia.</caption>
        <content>
          <table class="DoCO:Table" number="2" page="7">
            <thead class="table"/>
            <tbody>
              <tr class="table">
                <td class="table"></td>
                <td class="table"> Number of</td>
              </tr>
              <tr class="table">
                <td class="table"> Controlled experiment</td>
                <td class="table"> programmers Quality effects</td>
              </tr>
            </tbody>
          </table>
        </content>
        <region class="TableInfo" id="65" confidence="possible" page="7" column="1">Number of Controlled experiment programmers Quality effects</region>
      </region>
      <outsider class="DoCO:TextBox" type="header" id="66" page="7" column="1">Productivity effects</outsider>
      <outsider class="DoCO:TextBox" type="header" id="67" page="7" column="1">Kaufmann 11</outsider>
      <outsider class="DoCO:TextBox" type="header" id="68" page="7" column="1">8</outsider>
      <outsider class="DoCO:TextBox" type="header" id="69" page="7" column="1">Improved information flow</outsider>
      <outsider class="DoCO:TextBox" type="header" id="70" page="7" column="1">50% improvement</outsider>
      <region class="DoCO:TextChunk" id="71" confidence="possible" page="7" column="1">Edwards 12 59 54% fewer defects Erdogmus 13 35 No change Müller 14 19 No change, but better reuse Pan ˇ ur 15 38 No change</region>
      <region class="DoCO:TextChunk" id="79" page="7" column="1">FACTORS IN SOFTWARE PRACTICE ADOPTION A variety of factors play into the widespread adoption of a software practice. These include motivation for change, economics, availability of tools, training and instructional materials, a sound theoretical basis, empirical and anecdotal evidence of success, time, and even endorsements of the practice by highly regarded individuals or groups. These factors complicate TDD’s current state. Current software development practice provides a clear motivation for change and thus TDD seems poised for growth. Software development involves a complex mix of people, processes, technology, and tools that struggle to find consistency and pre- dictability. Projects continue to go over schedule and budget, which makes practitioners eager to find improved methods. Tool support for TDD is strong and improving for most modern languages. Tools such as JUnit, MockObjects, and Cactus are mature and widely available. Much of this tool development has tar- geted Java, an increasingly popular language in both commercial applications and academia. Economic models have noted the potential for positive improvements for XP and TDD, but rec- ognized that additional research is needed. This is especially true regarding speed and defects and when TDD is combined with pair programming. The interplay between academic and industry practitioners for acceptance is an interesting one. Research indicates that it takes five to 15 years for academic development to succeed in commercial practice—and the reverse holds true. Research shows how TDD can improve programming pedagogy, yet few instructional resources exist. The JUnit incorporation into BlueJ and the corresponding programming textbook indicates improvement may be on its way, however. The adoption of TDD faces many challenges. First, TDD requires a good deal of discipline on the programmer’s part. Hence, programmers may require compelling reasons before they try it. Second, TDD is still widely misunderstood, per- haps because of its name, but many still think erro- neously that TDD addresses testing only, and not design. Third, TDD doesn’t fit every situation. Developers and managers must determine when to apply TDD and when to do something else. <marker type="column" number="2"/><marker type="block"/> Additional research and the availability of training and instructional materials will likely play an important role in determining how widespread TDD will become.<marker type="block"/> UNDERSTANDING TDD’S EFFECTS Further research must be done to determine and understand TDD’s effects. To date, research has focused on TDD as a testing technique to lower defect density. Empirical studies should be conducted to evaluate TDD’s effect on software design quality and to examine characteristics such as extensibility, reusability, and maintainability. Even with the focus on defect density, there have been only a small number of studies conducted, and those on only small samples. One industry study with more than 10 participants involved a small application that took only one day to complete. The results were suspect because the control group wrote a minimal number of tests. The few academic studies that have examined defect density produced inconsistent results. The largest study reported a 54 percent reduction in defect density with beginning programmers. Two other reasonably large studies with advanced programmers did not provide any significant reduction in defect density. One study hinted at better designs. Future studies should consider the effectiveness of TDD at varying levels in the curriculum and the programmer’s maturity. The studies can also examine how TDD compares to test-last methods that fix the design ahead of time, as well as iterative test- last methods that build an emergent design. XP-EF, 16 a framework for consistently conducting and assessing case studies on XP projects, currently under development, seems appropriate for adapta- tion into a TDD case studies framework. Given such a framework, researchers can conduct multiple case studies and controlled studies. Programmer productivity and software quality should be examined. Adoption issues such as learning curves, suitability and fit, and motivation must be addressed. Additionally, research is needed to examine the effects of combining TDD with other practices such as pair programming and code inspection. Both industry and academic settings can benefit from more research. In particular, academic stud-<marker type="page" number="8"/><marker type="column" number="1"/><marker type="block"/> ies need to examine whether TDD improves or at least does not hinder learning. Can TDD be incor- porated into the undergraduate curriculum in a way that improves students’ ability to design and test? If so, then TDD must be written into appropriate student texts and lab materials.<marker type="block"/> ven if XP fades in popularity, TDD may persist. E Additional to improve research software is quality needed and on on TDD’s its place ability in undergraduate computer science and software engineering curricula. If TDD finds its way into academia, students could enter software organizations with increased discipline and improved software design and testing skills, increasing the software engineering community’s ability to reliably produce, reuse, and maintain quality software. I</region>
      <region class="unknown" id="73" page="7" column="2">n/a Improved productivity No change No change</region>
      <outsider class="DoCO:TextBox" type="footer" id="76" page="7" column="2">September 2005</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="77" page="7" column="2">49</outsider>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="80" confidence="possible" page="8" column="1">References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="81" page="8" column="1">1. C. Larman and V.R. Basili, “Iterative and Incremen- tal Development: A Brief History,” Computer, June 2003, pp. 47-56.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="82" page="8" column="1">2. K. Beck, “Aim, Fire,” IEEE Software, Sept./Oct. 2001, pp. 87-89.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="83" page="8" column="1">3. K. Beck, Extreme Programming Explained: Embrace Change, Addison-Wesley, 1999.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="84" page="8" column="1">4. P. Abrahamsson et al., “New Directions on Agile Methods: A Comparative Analysis,” Proc. 25th Int’l Conf. Software Eng. (ICSE 03), IEEE CS Press, 2003, pp. 244-254.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="85" page="8" column="1">5. D.J. Reifer, “How Good are Agile Methods?” IEEE Software, July/Aug. 2002, pp. 16-18.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="86" page="8" column="1">6. Shine Technologies, “Agile Methodologies Survey Results,” 17 Jan. 2003; www.shinetech.com/ download/attachments/98/ShineTechAgileSurvey2003- 01-17.pdf?version=1.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="87" page="8" column="1">7. J. Rasmusson, “Introducing XP into Greenfield Projects: Lessons Learned,” IEEE Software, May/June, 2003, pp. 21-28.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="88" page="8" column="1">8. B. George and L. Williams, “A Structured Experiment of Test-Driven Development,” Information and Software Technology, vol. 46, no. 5, 2004, pp. 337-342.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="89" page="8" column="1">9. E.M. Maximilien and L. Williams, “Assessing Test- Driven Development at IBM,” Proc. 25th Int’l Conf. Software Eng. (ICSE 03), IEEE CS Press, 2003, pp. 564-569.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="90" page="8" column="1">10. L. Williams, E.M. Maximilien, and M. Vouk, “Test- Driven Development as a Defect-Reduction Prac- tice,” Proc. 14th Int’l Symp. Software Reliability Eng. (ISSRE 03), IEEE Press, 2003, pp. 34-45.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="92" page="8" column="1">11. R. Kaufmann and D. Janzen, “Implications of Test- Driven Development: A Pilot Study,” Companion of <marker type="column" number="2"/><marker type="block"/> the 18th Ann. ACM Sigplan Conf. Object-Oriented Programming, Systems, Languages, and Applications, ACM Press, 2003, pp. 298-299.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="94" page="8" column="2">12. S.H. Edwards, “Using Test-Driven Development in the Classroom: Providing Students with Automatic, Concrete Feedback on Performance.” Proc. Int’l Conf. Education and Information Systems: Technologies and Applications (EISTA 03), Aug. 2003; <ext-link ext-link-type="uri" href="http://web-cat.cs.vt.edu/grader/Edwards-EISTA03." id="93">http://web-cat.cs.vt.edu/grader/Edwards-EISTA03.</ext-link> pdf.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="95" page="8" column="2">13. H. Erdogmus, M. Morisio, and M. Torchiano, “On the Effectiveness of Test-First Approach to Programming,” IEEE Trans. Software Eng., Mar. 2005, pp. 226-237.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="96" page="8" column="2">14. M.M. Müller and O. Hagner, “Experiment about Test-First Programming,” IEE Proc. Software, IEE Publications, 2002, pp. 131-136.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="97" page="8" column="2">15. M. Panc ˇur et al., “Towards Empirical Evaluation of Test-Driven Development in a University Environ- ment,” Proc. Eurocon 2003: The Computer as a Tool, The IEEE Region 8, vol. 2, 2003, pp. 83-86.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="98" page="8" column="2">16. L. Williams, L. Layman, and W. Krebs, Extreme Programming Evaluation Framework for Object-Oriented Languages, v. 1.4, tech. report TR-2004-18, North Carolina State Univ., 2004.</ref>
        </ref-list>
        <region class="DoCO:TextChunk" id="100" confidence="possible" page="8" column="2">David Janzen is the president of Simex LLC, a con- sulting and training firm, and an associate professor of computer science at Bethel College. He is currently a PhD candidate at the University of Kansas, where he received an MS in computer science. Janzen is a member of the IEEE Computer Society and the ACM. Contact him at david@ simexusa.com. Hossein Saiedian is a professor and associate chair in the Department of Electrical Engineering and Computer Science, and a member of the Information and Telecommunication Technology Center at the University of Kansas. His research interests include software process improvement, object technology, and software architecture. Saiedian received a PhD in computer science from Kansas State University. He is a senior member of the IEEE, and a member of the IEEE Computer Society and the ACM. Contact him at <email id="99">saiedian@eecs.ku.edu</email>.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="101" page="8" column="2">50</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="102" page="8" column="2">Computer</outsider>
      </section>
    </body>
  </article>
</pdfx>
