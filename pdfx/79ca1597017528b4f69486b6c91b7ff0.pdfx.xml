<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>82130ecf347873db45a1e130cdf5156538449c16a87e519a451ec4390d7b3a94</job>
    <base_name>62ea</base_name>
    <doi>http://dx.doi.org/10.1109/icdim.2011.6093363</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="1">Minimizing Ambiguity in Natural Language Software Requirements Specification</article-title>
      </title-group>
      <region class="DoCO:TextChunk" id="3" confidence="possible">Ashfa Umber Department of Computer Science &amp; IT The Islamia University of Bahawalpur Bahawalpur, Pakistan <email id="2">ashfaumber@yahoo.com</email></region>
      <abstract class="DoCO:Abstract" id="4">Abstract—Software requirements are typically captured in natural languages (NL) such as English and then analyzed by software engineers to generate a formal software design/model (such as UML model). However, English is syntactically ambiguous and semantically inconsistent. Hence, the English specifications of software requirements can not only result in erroneous and absurd software designs and implementations but the informal nature of English is also a main obstacle in machine processing of English specification of the software requirements. To address this key challenge, there is need to introduce a controlled NL representation for software requirements to generate accurate and consistent software models. In this paper, we report an automated approach to generate Semantic of Business Vocabulary and Rules (SBVR) standard based controlled representation of English software requirement specification. The SBVR based controlled representation can not only result in accurate and consistent software models but also machine process able because SBVR has pure mathematical foundation. We also introduce a java based implementation of the presented approach that is a proof of concept.</abstract>
      <region class="unknown" id="5">Keywords-Software Requirement Specifications; Natural Lanaague Processing; SBVR</region>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="7">Imran Sarwar Bajwa</name>
        </contrib>
      </contrib-group>
    </front>
    <body class="DoCO:BodyMatter">
      <region class="DoCO:TextChunk" id="6" page="1" column="1">I. I NTRODUCTION It is a typical practice that software requirements are specified in natural languages (NL). It is a common knowledge that 71.80% of the software requirements specifications are captured in NL [1]. However, the natural languages are intrinsically ambiguous. For automated software modeling, impervious and explicit software requirements are a primary necessity as computers cannot accurately process ambiguous requirements. A few scientists have proposed various approaches to identify and measure the typical ambiguities in NL based software requirements specifications (SRS) e.g. Kiyavitskaya et al. [3] presented a couple of tools to identify ambiguous sentence in a NL SRS document and find the reason of ambiguity. Similarly, Popescu et al. presented a tool Dowser [4] to identify ambiguous and inconsistent sentences in a NL SRS. However, a drawback of the used approach is that input should be in a constrained language, and this pitfall makes the approach impractical. According to our knowledge, there is no appropriate approach</region>
      <region class="DoCO:TextChunk" id="9" confidence="possible" page="1" column="2">School of Computer Science University of Birmingham Birmingham, UK <email id="8">i.s.bajwa@cs.bham.ac.uk</email></region>
      <region class="DoCO:TextChunk" id="37" page="1" column="2">or tool that can provide an automatic procedure of minimizing or removing ambiguity in NL SRS. In this paper, we aim to present an approach capable of automatically generating an unambiguous and semantically consistent representation of SRS specified in English language. To achieve a semantically controlled representation, we propose the use of Semantic of Business Vocabulary and Rules (SBVR) 1.0 [4]. SBVR is an OMG standard, initially presented to assist business requirements specifiers and analyzers. In [5], we presented that similar to business requirement, the software requirements can be captured and specified using SBVR syntax. In this paper, we propose the use of SBVR to overcome the typical ambiguities in a natural language. The SBVR incorporate not only ability of generating accurate and consistent software representation but also provides capability of machine processing as SBVR is based on mathematical or higher order logic [4]. The presented approach is also implemented in Java. The performance of the tool is evaluated by solving a case study, presented in section 4. The remaining paper is structured into the following sections: Section 2 states preliminaries of the presented research. Section 3 presents the framework for translation of English to SBVR representation. Section 4 presents a case study. The evaluation of our approach is presented in section 5. Finally, the paper is concluded to discuss the future work. <marker type="block"/> II. S EMANTIC B USINESS V OCABULARY AND R ULES In 2008, OMG presented a new standard Semantic Business Vocabulary and Rules (SBVR) [4]. SBVR supports capturing of requirement in a controlled natural language. There are various controlled natural languages such as Attempto but we have used SBVR due to following reasons: • SBVR is a standard. Latest available version is 1.0. • SBVR is easy to read and understand for human beings as SBVR uses syntax of natural languages e.g. English. • SBVR is easy to machine translate as it is based on higher logic such as First Order Logic (FOL).<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> A typical SBVR representation is based on a set of SBVR business vocabulary and SBVR business rules in particular business domain. 1) SBVR Business Vocabulary: A business vocabulary [4] (section: 8.1) consists of all the specific terms and definitions of concepts used by an organization or community in course of business. In SBVR, A concept can be a noun concept or fact type. Noun concepts can be further categorized into object type, individual concept, and characteristic. Hence we have four key elements in SBVR: • In SBVR, an object type is a general concept that exhibits a set of characteristics to distinguishes that object type from all other object types” [3] (section: 8.1) e.g. robot, user, etc. • In SBVR, an individual noun is a qualified noun that corresponds to only one object [3] (section: 8.1) e.g. ‘Robby’, a famous robot. • In SBVR, characteristic is an abstraction of a property of an object [4] (section: 8.1) e.g. name of robot is Robby, here name is characteristic. • In SBVR, a fact type or a verb concept [4] (section: 8.1) specifies the relationships among noun concepts e.g. car has wheels. A fact type can be binary fact type e.g. “customer places orders”. 2) SBVR Business Rules: A SBVR business rule is a formal representation under business jurisdiction ‘Under business jurisdiction’ [4]. Each SBVR business rule is based on at least one fact type. Business rules The SBVR rules can be a structural business rule [4] (section: 12.1) to define an organization’s setup or a behavioural business rule [4] (section: 12.1) to express the conduct of a business entity. B. SBVR based Controlled Representation SBVR was originally presented to assist business people in creating clear and unambiguous business policies and rules in their native language [4]. The following characteristics of SBVR can help in generating a controlled representation of English: 1) Rule-based Conceptual Formalization: SBVR standard provides a rule-based conceptual formalization that can be employed to generate a syntactically formal representation of English. SBVR contains a vocabulary for conceptual modeling and captures expressions based on this vocabulary as formal logic structures. The SBVR vocabulary can be used to formally specify representations of concepts, definitions, instances, and rules of any knowledge domain in natural language. These features make SBVR well suited for describing business domains and software requirements to implement software models. 2) Natural Language Semantic Formulation: SBVR is typically proposed for business modeling in NL. However, we are using the formal logic based nature of SBVR to semantically formulate the English software requirements statements. A set of logic structures called semantic formulations are provided in SBVR to make English statements controlled such as atomic formulation, instantiate<marker type="column" number="2"/><marker type="block"/> formulation, logical formulation, quantification, and modal formulation. 3) SBVR Formal Notation : Structured English is one of the possible SBVR notations, given in SBVR 1.0 document, Annex C [4], is applied by prefixing rule keywords in a SBVR rules. The other possible SBVR notation is Rulespeak, given in SBVR 1.0 document, Annex F [4], uses mixfixing keywords in propositions. SBVR formal notations help in expressing propositions with equivalent semantics that can be captured and formally represented as logical formulations.<marker type="block"/> III. T RANSLATING NL TO SBVR This section briefly explains how English text is mapped to SBVR representation and object oriented information is extracted from SBVR representation. <xref ref-type="fig" rid="F1" id="16" class="deo:Reference">Figure 1</xref> show the used approach that works in three phases:<marker type="block"/> A. Parsing NL Software Requirement Text The first phase of SR-Elicitor is NL parsing that involves a number of processing units (organized in a pipelined architecture) to process complex English statements. The NL parsing phase lexically, syntactically and semantically processes the English text as following: 1) Lexical Processing: The NL parsing starts with the lexical processing of a plain text file containing English software requirements specification. The lexical processing initiates with the tokenization of the input English text.The tokenized text is further passed to Stanford parts-of- speech (POS) [13] tagger v3.0 to identify the basic POS tags e.g.<marker type="page" number="3"/><marker type="column" number="1"/><marker type="block"/> 2) Syntactic Processing: We have used an enhanced version of a rule-based bottom-up parser for the syntactic analyze of the input text used in [11]. English grammar rules are base of used parser. The text is syntactically analyzed and a parse tree is generated for further semantic processing as shown in <xref ref-type="fig" rid="F2" id="24" class="deo:Reference">Figure 2</xref>.<marker type="block"/> 3) Semantic Interpretation: In this semantic interpretation phase, role labeling [12] is performed. The desired role labels are actors (nouns used in subject part), co-actor (additional actors conjuncted with ‘and’), action (action verb), thematic object (nouns used in object part), and a beneficiary (nouns used in adverb part) if exists. These roles assist in identifying SBVR vocabulary. B. Extracting SBVR Vocabulary In this phase, the basic SBVR elements e.g. noun concept, individual concept, object type, verb concepts, etc are identified from the English input that is preprocess in the previous phase. The extraction of various SBVR elements is described below: 1) Extracting Object Types: All common nouns (actors, co-actors, thematic objects, or beneficiaries) are represented as the object types or general concept (see <xref ref-type="fig" rid="F3" id="28" class="deo:Reference">figure 3</xref>) e.g. belt, user, cup, etc. In conceptual modelling, the object types are mapped to classes. 2) Extracting Individual Concepts: All proper nouns (actors, co-actors, thematic objects, or beneficiaries) are represented as the individual concepts. 3) Extracting Fact Types: The auxiliary and action verbs are represented as verb concepts. To constructing a fact types, the combination of an object type/individual concept + verb forms a unary fact type e.g. “vision system senses”. Similarly, the combination of an object type/individual concept + verb + object type forms a binary fact type e.g. belt conveys part is a binary fact type.<marker type="column" number="2"/><marker type="block"/> 4) Extracting Characteristics: In English, the characteristic or attributes are typically represented using is- property-of fact type e.g. “name is-property-of customer”. Moreover, the use of possessed nouns (i.e. pre-fixed by’s or post-fixed by of) e.g. student’s age or age of student is also characteristic. 5) Extracting Quantifications: All indefinite articles (a and an), plural nouns (prefixed with s) and cardinal numbers (2 or two) represent quantifications. 6) Extracting Associative Fact Types: The associative fact types [4] (section 11.1.5.1) are identified by associative or pragmatic relations in English text. In English, the binary fact types are typical examples of associative fact types e.g. “The belt conveys the parts”. In this example, there is a binary association in belt and parts concepts. This association is one- to-many as ‘parts’ concept is plural. In conceptual modeling of SBVR, associative fact types are mapped to associations. 7) Extracting Partitive Fact Type: The partitive fact types [4] (section 11.1.5.1) are identified by extracting structures such as “is-part-of”, “included-in” or “belong-to” e.g. “The user puts two-kinds-of parts, dish and cup”. Here ‘parts’ is generalized form of ‘dish’ and ‘cup’. In conceptual modeling of SBVR, categorization fact types are mapped to aggregations. 8) Extracting Categorization Fact Types: The categorization fact types [4] (section 11.1.5.2) are identified by extracting structures such as “is-category-of” or “is-type- of”, “is-kind-of” e.g. “The user puts two-kinds-of parts, dish and cup”. Here ‘parts’ is generalized form of ‘dish’ and ‘cup’. In conceptual modeling of SBVR, categorization fact types are mapped to generalizations. All the extracted information shown in <xref ref-type="fig" rid="F4" id="30" class="deo:Reference">figure 4</xref> is stored in an arraylist for further analysis.<marker type="block"/> C. Generating SBVR Rules In this phase, a SBVR representation such as SBVR rule is generated from the SBVR vocabulary in previous phase. SBVR rule is generated in three phases as following: 1) Extracting SBVR Requirements: To generate a rule from an English statement, it is primarily analyzed that it is a structural requirement or a behavioural requirement. Following mapping rules are used to classify a constraint type. 2) Extracting Structural Requirements: The use of auxiliary verbs such as ‘can’, ‘may’, etc is identified to classify co requirement as a structural requirement. The sentences representing state e.g. “Robby is a robot” or possession e.g. “robot has two arms” can be categorized as structural requirements. Moreover, the general use of action<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> verbs e.g. consists, composed, equipped, etc also represent a structural requirement. 3) Extracting Behavioural Requirements: The use of auxiliary verbs such as ‘should’, ‘must’ are identified to classify requirement as a behavioural rule. Moreover, the use of action verb can be categorized as a behavioural rule e.g. “robot picks up parts”. 4) Applying Semantic Formulation: A set of semantic formulations are applied to each fact type to construct a SBVR rule. There are five basic semantic formulations proposed in SBVR version 1.0 [12] but we are using following three with respect to the context of the scope of proposed research: a) Logical Formulation: A SBVR rule can be composed of multiple fact types using logical operators e.g. AND, OR, NOT, implies, etc. For logical formulation, the tokens ‘not’ or ‘no’ are mapped to negation ( a ). Similarly, the tokens ‘that’ and ‘and’ are mapped to conjunction ( a b ). The token ‘or’ is mapped to disjunction ( a b ) and the tokens ‘imply’, ‘suggest’, ‘if, ‘infer’ are mapped to implication ( a b ). b) Quantification: Quantification [13] is used to specify the scope of a concept. Quantifications are applied by mapping tokes like “more than” or “greater than” to at least n quantification; token “less than” is mapped to at most n quantification and token “equal to” or a positive statement is mapped to exactly n quantification. c) Modal Formulation: In SBVR, the modal formulation [13] specifies seriousness of a constraint. Modal verbs such as ‘can’ , ‘’ or ‘may’ are mapped to possibility formulation to represent a structural requirement and the modal verbs ‘should’, ‘must’ or verb concept “have to” are mapped to obligation formulation to represent a behavioural requirement.<marker type="block"/> IV. A C ASE S TUDY To demonstrate the potential of our tool SR-Elicitor, a small case study is discussed from the domain of office time management system. This case study is online available. Following is a part of the problem statement for the case study, solved in the thesis to test SR-Elicitor.</region>
      <outsider class="DoCO:TextBox" type="footer" id="12" page="1" column="2">978-1-4577-1539-6/11/$26.00 ©2011 IEEE</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="13" page="1" column="2">102</outsider>
      <region class="unknown" id="18" page="2" column="2">Process NL Specification Text Preprocessing NL Syntactic Parsing Semantic Text Interpretation Generate Extracting Object Types Extracting SBVR Verb Concepts Extracting Individual Vocabular Concepts Extracting Characteristics Generate Generating SBVR Fact Types Apply SBVR Semantic Formulation Applying Rules Structured English Notation</region>
      <region class="DoCO:FigureBox" id="F1">
        <caption class="deo:Caption" id="19" page="2" column="2">Figure 1. A Framework used for English to SBVR Translation</caption>
      </region>
      <region class="unknown" id="21" page="2" column="2">A task is a component of the schedule with a start and end date. [A/ ] [task/ ] [is/ ] [a/ ] [component/ ] [of/ ] [the/ ] [schedule/ ] [with/ ] [a/ ] [start/ ] [and/ ] [end/ ] [date/ ]. A Framework used for English to SBVR Translation</region>
      <region class="DoCO:FigureBox" id="F2">
        <caption class="deo:Caption" id="22" page="2" column="2">Figure 2.</caption>
      </region>
      <outsider class="DoCO:TextBox" type="page_nr" id="23" page="2" column="2">103</outsider>
      <region class="unknown" id="26" page="3" column="1">A task is a component of the schedule with a start and end date. ( ( ( ( A) ( task)) ( ( is) ( ( ( a) ( component)) ( ( of) ( ( ( the) ( schedule)) ( ( with) ( ( a) ( start) ( and) ( end) ( date)))))))))</region>
      <region class="DoCO:FigureBox" id="F3">
        <caption class="deo:Caption" id="27" page="3" column="1">Figure 3. Parsing English text using Stanford Parser</caption>
      </region>
      <region class="unknown" id="32" page="3" column="2">A task is a component of the schedule with a start and end date. [A] [task/ ] [is/ ] [a] [component/ ] [of] [the] [schedule/ ] [with] [a] [start/object_type] [and] [end_date/ ].</region>
      <region class="DoCO:FigureBox" id="F4">
        <caption class="deo:Caption" id="33" page="3" column="2">Figure 4. Semantic interpretation of English text</caption>
      </region>
      <outsider class="DoCO:TextBox" type="page_nr" id="35" page="3" column="2">104</outsider>
      <region class="DoCO:TextChunk" id="38" confidence="possible" page="4" column="1">“ The two main functions of the time Monitor software system are to allow the developers to use a www browser to store timestamp records in a database, and to allow a manager to analyze these timestamp records. A timestamp record consists of the time duration consists of the duration of a specific activity with the unique identification. The unique identification is made of three components: the project, the user and the date when the activity is taken place. The description of an activity is divided into three components: a task name, an activity, and an artefact. For managerial purpose it is often useful to define the date in term of the current week. The current week is defined as the week starting on the Monday immediately preceding the current day of the week, and ending on the Sunday immediately following the current day of the week, inclusively. A task is unit of work defined by the manager</region>
      <region class="DoCO:TextChunk" id="40" page="4" column="2">and for which the developer is accountable. A task is a component of the schedule with a start and end date. Examples of task are Implement module A, Design library XYZ. Developers usually work with on assigned tasks. One developer may work on many tasks and a given task may involve many developers .” The problem statement of the second case study was given as input (NL specification) to the SR-Elicitor tool. The SR- Elicitort parses the text first, which includes lexical processing (Tokenization, Sentence splitting, POS tagging and Morphological analysis) syntactic analysis and semantic analysis. It extracts the SBVR vocabulary from the case study during syntactic analysis of text which includes noun concept, individual concept, object type, verb concept, characteristics, quantifications, associative fact types and partitive fact types etc. as shown in <xref ref-type="table" rid="TI" id="39" class="deo:Reference">table I</xref>:</region>
      <region class="DoCO:TextChunk" id="41" confidence="possible" page="4" column="2">Category Count Details Object Types 18 time_monitor_software, developer, user, task, www_browser, component, date, timestamp_record, activity, identification, project, name, artifact, week, manager, schedule, start_date, end_date Verb Concepts 15 allow, use, store, analyse, consist, made, taken_place, define, decided, preceding, starting, ending, assign, work, involve Individual 04 Monday , Sunday , Implement Module A, Concepts Design library XYZ, Characteristics 06 function, duration, description, term, day, unit, component, Quantifications 04 Two, three, one, many Unary Fact 03 activity take place, define date, current Types week defined, Associative 07 time monitor software allow developer, Fact Types developer use www browser, www browser store timestamp record, manager analyze timestamp record, week start on Monday, identification made of components, work defined by manager, developer work with task, Partitive fact 02 timestamp record consists of time duration, Types time duration consists of activity, activity divided in components, task is unit Categorization 03 of work, task is component of schedule, Fact Types</region>
      <region class="DoCO:TextChunk" id="51" page="4" column="2">The <xref ref-type="table" rid="TI" id="42" class="deo:Reference">table I</xref> show the extracted SBVR elements such as 18 object types, 15 verb concepts, 4 characteristics, 3 unary fact types, 7 associative fact types, 2 partitive fact type and 3 categorization fact type. In the used case study’s problem statement, there were 06 requirements as shown in <xref ref-type="table" rid="TII" id="43" class="deo:Reference">table II</xref>:<marker type="page" number="5"/><marker type="column" number="1"/><marker type="block"/> There are 11 requirements processed by the SR-Elicitor (see <xref ref-type="table" rid="TII" id="47" class="deo:Reference">table II</xref>). According to SBVR structured English the object types are underlined e.g. system , developer , manager , time etc. the verb concepts are italicized e.g. allow , analyze etc. the SBVR keywords are bolded e.g. at least, It is possibility etc. the individual concepts are double underlined e.g. Monitor , Design library , etc. The characteristics are also italicized but with different colour: e.g. consist of , made of etc .<marker type="block"/> V. E VALUATION We have done performance evaluation to evaluate that how accurately the English specification of the software requirements has been translated into the SBVR based controlled representation by our tool ER-Elicitor. An evaluation methodology, for the performance evaluation of NLP tools, is used that was originally proposed by Hirschman and Thompson [14]. The used performance evaluation is typically based on three aspects:<marker type="column" number="2"/><marker type="block"/> There were seven sentences in the used case study problem. The largest sentence was composed of 39 words and the smallest sentence contained 10 words. The average length of all sentences is 24. The major reason to select this case study was to test our tool with the complex examples. The correct, incorrect, and missing SBVR elements are shown in <xref ref-type="table" rid="TIII" id="50" class="deo:Reference">table III</xref>.</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="45" page="4" column="2">105</outsider>
      <region class="unknown" id="46" page="5" column="1"># Details 1 It is permissible that the two main function of the time monitor software are to allow the developer to use a www browser to store timestamp record in a database, and to allow a manager to analyze these timestamp record. /. 2 It is necessity that a timestamp record consists of the time duration consists of the duration of a specific activity with the unique identification./. 3 It is necessity that the unique identification is made of three component: the project, the user and the date when the activity is taken place ./. 4 It is necessity that the description of an activity is divided into three component a task name, an activity, and an artefact./. 5 It is necessity that for managerial purpose, it is often useful to define the date in term of the current week./. 6 It is necessity that the current week is defined as the week starting on the 'Monday' immediately preceding the current day of the week, and ending on the 'Sunday' immediately following the current day of the week, inclusively ./. 7 It is necessity that a task is unit of work defined by the manager and for which the developer is accountable ./. 8 It is necessity that a task is a component of the schedule with a start and end date./. 9 It is permissible that the example of task are 'Implement module A', 'Design library XYZ' ./. 10 It is possibility that the developer usually work with one assigned task ./. 11 It is possibility that One may work on many task and a given task may involve many developer./.</region>
      <section class="deo:Results">
        <h1 class="DoCO:SectionTitle" id="52" page="5" column="2">Results of NL to SBVR Translation by</h1>
        <region class="DoCO:TextChunk" id="53" confidence="possible" page="5" column="2">SR-Elicitor</region>
        <region class="unknown" id="54" page="5" column="2"># Type/Metrics N sample N correct N incorrect N missing 1 Object Types 18 16 1 1 2 Verb Concepts 16 14 2 0 3 Individual Concepts 05 04 1 0 4 Characteristics 07 06 2 1 5 Quantifications 06 04 0 2 6 Unary Fact Types 03 03 0 0 7 Associative Fact 08 07 1 0 8 Partitive fact Types 02 02 0 0 9 Categorization Fact 03 03 0 0 Total 68 59 7 4</region>
        <region class="DoCO:TextChunk" id="56" page="5" column="2">Results of each SBVR element describe in above table separately. According to our evaluation methodology, table shows sample elements are 68 in which 59 are correct, 7 are incorrect and 4 are missing SBVR elements. The following table describes the Recall and precision of SR-elicitor for NL- software requirements. In <xref ref-type="table" rid="TIV" id="55" class="deo:Reference">table IV</xref>, the average recall for SBVR software requirement specification is calculated 86.76% while average precision is calculated 89.39%. Considering the lengthy input English sentences including complex linguistic structures, the results of this initial performance evaluation are very encouraging and support both the approach adopted in this paper and the potential of this technology in general.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="57" page="5" column="2">Recall and Precision of SR-Elicitor for NL software requirements for case stud:</h1>
        <region class="unknown" id="58" page="5" column="2">Type/Metrics N sample N correct N incorrect N missing Rec% Prec% F-Value Software Requirements 68 59 7 4 86.76 89.39 88.05</region>
        <region class="DoCO:TextChunk" id="60" page="5" column="2">Four other case studies were solved in addition to the case study presented in section 4. All the case studies were unseen. The solved case studies were of different lengths. The largest case study was composed of 143 words and 13 sentences. The smallest case study was composed of 97 words and 8 sentences. Calculated recall, precision and f-values of the solved case studies are shown in <xref ref-type="table" rid="TV" id="59" class="deo:Reference">table V</xref>.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="61" page="5" column="2">106</outsider>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="62" page="6" column="1">Evaluatin results of SR-Elicitor</h1>
        <region class="DoCO:TextChunk" id="63" confidence="possible" page="6" column="1">Input N sample N correct N incorrect N missing Rec% % Prec% F-Value C1 48 37 8 3 77.08 8 82.22 79.65 C2 43 33 8 2 76.74 4 80.48 78.61 C3 39 31 5 3 79.48 8 86.11 82.79 C4 36 29 3 4 80.55 5 90.62 85.58 C5 68 59 7 4 86.76 6 89.39 88.05 Average 80.12 85.76 82.94</region>
        <region class="DoCO:TextChunk" id="70" page="6" column="1">The average F-value is calculated 82.94 th hat is encouraging for initial experiments. We cannot compare our results to any other tool as no other tool is available that t can generate SBVR-based SRS from NL specification. However, we can note that other language processing techn nologies, such as information extraction systems, and ma achine translation systems, have found commercial applicatio ons with precision and recall figure well below this level. Thus, , the results of this initial performance evaluation are very encouraging and support both ER-Elicitor approach and the e potential of this technology in general. Graph 7.0 shows the evaluation results of SR-Elicitor. <marker type="block"/>  <xref ref-type="fig" rid="F5" id="66" class="deo:Reference">Figure 5</xref> materializes the results of Recall l, precision and F- Value that is obtained by five differe ent case studies.<marker type="block"/> According to our results C5 has high Recall l, precision and F- Value. Moreover, C1 has lowest Recall, precision and F- Value. VI. C ONCLUSION AND F UTURE E W ORK The primary objective of the paper wa as to address the ambiguous nature of natural languages (suc ch as English) and generate a controlled representation of En nglish so that the accuracy of machine processing can be imp proved. To address this challenge we have presented a NL based automated approach to parse English software requirem ment specifications and generated a controlled representatio on using SBVR. Automated object oriented analysis of SBV VR based software requirements specifications using SR-Eli icitor provides a higher accuracy as compared to other av vailable NL-based tools.<marker type="column" number="2"/><marker type="block"/> The future work is to extract t the object-oriented information from SBVR specification of software requirements such as classes, instances and their re espective attributes, operations, associations, aggregations, an nd generalizations. Automated extraction of such information n can be helpful in automated conceptual modelling of natural language software requirement specification.</region>
        <region class="DoCO:FigureBox" id="F5">
          <caption class="deo:Caption" id="65" page="6" column="1">Figure 5. Semantic interpretation of Eng glish text</caption>
        </region>
        <region class="DoCO:FigureBox" id="Fx68">
          <image class="DoCO:Figure" src="62ea.page_006.image_01.png" thmb="62ea.page_006.image_01-thumb.png"/>
        </region>
        <region class="DoCO:TextChunk" id="72" confidence="possible" page="6" column="2">R EFE ERENCES [1] Mich Luisa , Franch Mariangela a , Inverardi Pierluigi, Market research for requirements analysis us sing linguistic tools, Requirements Engineering, v.9 n.1, p.40-56, Fe bruary 2004 [2] Kiyavitskaya, N., Zeni, N., Mic ch, L., Berry, D.M., Requirements for tools for ambiguity identification n and measurement in natural language requirements specifications, Requ uirements Engineering, v.13 n.3, p.207- 239, August 2008 [3] Popescu, D., Rugaber, S., Medvi idovic, N. et al., Reducing Ambiguities in Requirements Specifications s Via Automatically Created Object- Oriented Models, Innovations s for Requirement Analysis. From Stakeholders' Needs to Formal Designs: D 14th Monterey Workshop 2007, Monterey, CA, USA, September 10-13, 2007. [4] OMG. Semantics of Business vo ocabulary and Rules. (SBVR) Standard v.1.0. Object Mana agement Group, Available: <ext-link ext-link-type="uri" href="http://www.omg.org/spec/SBVR/" id="71">http://www.omg.org/spec/SBVR/</ext-link> /1.0/, 2008 [5] To insert individual citation int to a bibliography in a word-processor, select your preferred citation sty yle below and drag-and-drop it into the document. [6] Spreeuwenberg, S., and Healy y, K.A. 2010. SBVR’s Approach to Controlled Natural Language. C CNL 2009 Workshop, LNCS Volume 5972/2010:155-169 [7] Ilieva, M.G., Ormandjieva, O. 20 005. Automatic Transition of Natural Language Software Requirem ments Specification into Formal [ Presentation. In NLDB 2005: 392 2-397. [9] Toutanova. K., Manning, C.D. 2 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In the Joint SIGDAT Conference on Emp pirical Methods in Natural Language Processing and Very Large Corpo ora. 63-70. Hong Kong. [10] Fuchs, N.E., Kaljurand, K., and d Kuhn, T. 2008. Attempto Controlled English for Knowledge Represen ntation. In: Reasoning Web, LNCS, vol. 5224/2008:104–124. [11] White, Colin and Rolf Schwitter r. 2009. An Update on PENG Light. In: Proceedings of ALTA 2009, pp. 80–88. [12] Clark, P., Harrison, P., Murray, W.R, Thompson, J. 2009. Naturalness vs. Predictability: A Key De ebate in Controlled Languages. In: Proceedings 2009 Workshop on Controlled Natural Languages (CNL’09). [13] Martin, P. 2002. Knowledge r representation in CGLF, CGIF, KIF, Frame-CG and Formalized-Eng glish. In: Proceedings of ICCS 2002, LNAI, vol.2393, pp. 77–91. [14] Schwitter, R. 2010. Controlled d Natural Languages for Knowledge Representation, Coling 2010: Poster P Volume, Beijing, August 2010: 1113–1121 [15] Huijsen, W.O. 1998. Controll led Language –An Introduction. In: Proceedings of CLAW 98:1–15. [16] Hirschman, L., Thompson, H.S. 1995. Chapter 13 evaluation: Overview of evaluation in speech and natu ural language processing. In Survey of the State of the Art in Human Lan nguage Technology.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="73" page="6" column="2">107</outsider>
      </section>
    </body>
  </article>
</pdfx>
