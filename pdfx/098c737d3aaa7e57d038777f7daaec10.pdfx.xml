<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>02c7588fc2f0ef39ae1ec6dfadb83a4e6c4ffbddc661217f7accd3210fdab301</job>
    <base_name>62k5</base_name>
    <doi>10.1049/ip-sen:20060034</doi>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="1">Identifying tacit knowledge-based requirements</article-title>
      </title-group>
      <region class="unknown" id="2">A. Stone and P. Sawyer</region>
      <abstract class="DoCO:Abstract" id="3">Abstract: Requirements may be derived from a number of sources. Determining the source of a given requirement is known as pre-requirements tracing. Typically, some requirements appear that have no clear source, yet stakeholders will attest to the necessity of these requirements. However, such requirements are likely to be based on tacit or tacit-like knowledge embedded in the problem domain. A tool called Prospect that retrospectively identifies pre-requirement traces is presented. This tracing is achieved by working backwards from requirements to the documented records of the elicitation process, such as interview transcripts or ethnographic reports. A vector- space technique, latent semantic analysis, is shown to be useful to perform pre-requirements tracing. The identification of badly sourced requirements naturally leads to the inference that further investigation of these requirements is necessary, whether or not the requirements turn out to be based on tacit knowledge.</abstract>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="4" page="1" column="1">1 Introduction</h1>
      </section>
      <region class="DoCO:TextChunk" id="5" page="1" column="1">The factors that prevent the complete discovery of the processes and information that comprise a problem domain from which requirements have to be synthesised have been well documented [1]. The effectiveness of techniques that address these sources of non-determinism are limited by the nature of the knowledge embedded in the problem domain [2]. We argue that instances of knowledge that are particularly problematic, such as tacit knowledge, may be identified by examining the provenance of a requirement from its source material. Examining the provenance of a requirement is a form of pre-requirements tracing, a process which concerns the identification of contributors to each requirement. Pre-requirements tracing has proven to be useful during situations such as legacy systems engineering, where it is necessary to retrospectively determine the reasoning behind design decisions. We argue that pre-requirements tracing may be used to determine tacit knowledge or infer its existence in the domain. We show that pre-requirements tracing can be determined progra- matically using a semantic level comparison enabling technique. The applicability of these techniques is demonstrated on a corpus of ethnographic reports and a corresponding concept of operations document from the air traffic control (ATC) domain.</region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="6" page="1" column="1">2 Tacit knowledge</h1>
      </section>
      <region class="DoCO:TextChunk" id="7" page="1" column="1">Tacit knowledge was first extensively explored by Polanyi in his book ‘The Tacit Dimension’ [3]. Although tacit knowledge had been discussed by Ryle [4], Polanyi was the first person to approach the subject with the intent of deriving practical knowledge about it. Polanyi briefly</region>
      <region class="DoCO:TextChunk" id="9" confidence="possible" page="1" column="1"># The Institution of Engineering and Technology 2006 IEE Proceedings online no. 20060034 doi:10.1049/ip-sen:20060034 Paper first received 6th July and in revised form 24th October 2006 The authors are with the Computing Department, Infolab21, Lancaster University, Lancaster LA1 4WA, UK E-mail: <email id="8">a.stone@comp.lancs.ac.uk</email></region>
      <region class="DoCO:TextChunk" id="18" page="1" column="2">defines tacit knowledge as ‘knowing more than you can tell’. Explication of this knowledge that you ‘cannot tell’ is already of vital importance to researchers in subjects such as artificial intelligence, where the inexplicability of everyday tasks such as facial recognition prevents scientific discovery. This is not to say that requirements engineering would not benefit from tacit knowledge investigation. Ryan [5] presents a modern corollary in his statement ‘neither informal speech nor natural language text is capable of expressing unambiguously the myriad facts and behaviours that are included in large scale systems’. Tacit knowledge has been defined as knowledge that cannot be known or that cannot be communicated. The problem with this definition is that it is anti-intellectual, as it prevents discovery of types of knowledge that appear to be tacit. If the presence of apparently tacit knowledge can be identified, then in some cases identification may enable the knowledge to become explicit. Nonaka and Takeuchi [6] distinguish between technical tacit knowledge and cognitive tacit knowledge. Technical tacit knowledge is that which is acquired by doing over a long period by, for example, skilled craftsmen. Cognitive tacit knowledge is transmitted by language and forms part of our way of viewing the world. It can be internalised by exposure to knowledge, perhaps in the form of written text. However, much cognitive tacit knowledge is socially acquired [7] and shared within a group. This can make it hard to recognise and explain, perhaps leading a stakeholder to fail to reveal knowledge that they mistakenly assume is common knowledge. Since cognitive tacit knowledge can be acquired from explicit knowledge, it is feasible, if not necessarily easy, to articulate the knowledge. Technical tacit knowledge, by contrast, is much harder to articulate. Parnas presents a modern application of the effect of tacit knowledge in requirements engineering [8]. Faced with the bizarre requirement that the altimeter needle of the A7 aircraft should display a fixed altitude value when both the barometric and the ground following radar altimeters fail, Parnas set out to determine if it was really correct, and if so, why. He was able to determine that this requirement was in fact correct behaviour, although none of the stakeholders interviewed were able to explain why. Eventually a pilot explained that it was the aircraft’s cruising altitude, <marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> so this was most likely to be the aircraft’s altitude at the time of the altimeter failure! The knowledge that gave rise to the failed altimeter requirement appeared to be tacit because of a failure in organisational communication. The example allows us to derive some characteristics of such tacit-like knowledge:<marker type="block"/> 1. Tacit-like knowledge may exhibit a presence in the requirements specification. 2. The behaviour associated with tacit-like knowledge may already exist within the organisation in a physical or pro- cedural way. 3. The identification of tacit-like knowledge may impact on other requirements.<marker type="block"/> Point 3 is illustrated by the effect on the flight profile of the aircraft caused by the failed altimeter requirement. If A7 pilots are flying at the aircraft’s cruising altitude, then how do they determine if their altimeter is broken? The advice given to pilots is to pull back gently on the control column, causing the aircraft’s nose to rise, and see if the altimeter needle moves accordingly. Therefore A7 pilots routinely increase their altitudes, then subsequently return to cruising altitude throughout the flight. The provenance of requirements that are consequent on tacit knowledge may be much harder to establish than in the altimeter example. A previous study on the use of ethnography in systems engineering [9] analysed the working practises of air traffic controllers. Embedded within the ethnographic field reports are examples of tacit knowledge. For example, when confronted with a slow aeroplane about to enter a busy sector in which all flight levels (permitted altitudes of flight) will shortly be filled, the sector chief rerouted the slow aeroplane to another sector as shown in the extract from the ethnographic field report in <xref ref-type="fig" rid="F1" id="17" class="deo:Reference">Fig. 1</xref>. The ethnographer explicitly identified the sector chief’s behaviour as an example of tacit knowledge as at no point were any details about the aircraft in question mentioned, not even the originating sector, yet the chief was still able to identify and reroute the aircraft. When questioned later, the chief replied that he knew which aircraft was in question just by looking at the radar. Plausibly, therefore, an analyst experienced in the air traffic control domain might syn- thesise a requirement about the radar display that provided the information used by the sector chief. Since the nature of the information used by the sector chief is only implicit in the ethnographic report, the provenance of the radar display requirement would be difficult to trace. There are other benefits that can be expected from tacit knowledge identification. Finkelstein has identified the inability of requirements engineers to accurately and clearly define a system’s boundary as one of the remaining</region>
      <outsider class="DoCO:TextBox" type="footer" id="11" page="1" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="12" page="1" column="2">211</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="14" page="1" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="13">http://ietdl.org/copyright.jsp</ext-link></outsider>
      <region class="unknown" id="19" page="2" column="1">10.56 Wing writes a height revision on a controller’s livestrip following a telephone call. (Inbound from Scottish. Much of this co-ordination is done on the wings.) 11.05 Controller PH to Controller IS: ‘you can track Mac9025 to me, ....’ [Controller IS is on the telephone]: ‘pardon?’ Chief: ‘J...’ll take 9025’ Controller IS: ‘oh ... OK ...’ 11.17 SA: ‘Chief theres this he wants ’ Chief: ‘all levels are blocked through there ’ Spends a moment thinking Chief: ‘no, he’s a slow one there’s no way he’ll be clear then so we’ll take him through Liffy’</region>
      <region class="DoCO:FigureBox" id="F1">
        <caption class="deo:Caption" id="20" page="2" column="1">Fig. 1</caption>
      </region>
      <region class="DoCO:TextChunk" id="21" confidence="possible" page="2" column="1">An example of tacit knowledge embedded in a typical air traffic control scenario</region>
      <region class="DoCO:TextChunk" id="22" page="2" column="2">problems in RE [10]. Finkelstein comments that ad hoc decisions are then made as to the required scope of the requirements discovery process. If many of the domain’s processes are held as tacit knowledge in the domain, then defining a reasonable system boundary can never take place unless this tacit knowledge is somehow exposed. Exposure of tacit or tacit-like knowledge may be achieved by an examination of the origin of each requirement. Methods for exposing a requirement’s origins are discussed in the following section.</region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="23" page="2" column="2">3 Pre-requirements tracing and linguistic engineering</h1>
        <region class="DoCO:TextChunk" id="51" page="2" column="2">Gotel and Finkelstein [11] identify both the need for and the difficulties associated with requirements tracing. They divide tracing into two classes: pre- and post-requirements specification tracing, which are analogous to high-end and low-end tracing [12]. Post-requirement specification tracing deals with a requirement’s life after inclusion in the specification. Pre-requirement specification tracing, which we term pre-requirements tracing, is concerned with a requirement’s derivation. The difference between these two methods of tracing is illustrated in <xref ref-type="fig" rid="F2" id="24" class="deo:Reference">Fig. 2</xref>. Support for pre-requirements specification tracing is under- developed compared to support for post-requirement specification tracing. One reason for this lack of support is that requirement synthesis often involves much more than a simple transformation process in which information elicited from stakeholders is re-written. As the size of both the source material and the corresponding specification increases, it becomes unfeasible for pre-requirements tracing to be performed by hand. A method of programmatic pre-requirements tracing is therefore required. By searching for traces between requirements and their respective sources, it should be possible to determine which requirements are not firmly derived from the source material. Badly sourced requirements may reflect an instance of either:<marker type="block"/> 1. Poorly sourced knowledge, that is knowledge which is not clearly defined and should therefore be subject to further investigation. 2. A form of tacit knowledge, whose presence in the requirements specification demonstrates a description of the external behaviour of a tacit process.<marker type="block"/> Statement 1 covers the general case of all poorly sourced requirements in the requirements specification. We suppose that fewer requirements satisfy the criteria laid down in statement 2 than in statement 1. We are able to suppose statement 2 because of the anecdotal evidence of the occurrence of tacit knowledge demonstrated in Section 2. It is important to note at this stage that we are not seeking to measure requirements’ completeness. Establishing the absence of requirements that represent information that is explicit in the source material or (even harder) that is implicit from tacit knowledge is outside the scope of our work. In order to determine the origins of requirements and hence whether they are based on tacit knowledge, it is necessary to process the requirements and the available source material. Both requirements and source material are typically expressed using natural language, and we therefore assume that natural language is the medium used in both the requirements specification and the source material. Our goal is to process the text in such a way as to derive human information functions from it, such as<marker type="page" number="3"/><marker type="column" number="1"/><marker type="block"/> semantic or pragmatic meaning. We determine ‘meaning’ via a corpus linguistics technique. Corpus linguistics gave rise to the notion that language is understandable by obser- vation, rather than the classical approach of forming a linguistic theory and attempting to fit instances of natural language to the theory in order to support it. A body of language, or corpus, is composed of examples of natural language, normally in the scale of millions of words [13]. This corpus enables statistical methods of comparison which have been successfully used to determine patterns in language at the syntactic and semantic level. The applicability of corpus linguistics to document processing in requirements engineering has been shown in several problem domains and at different levels <xref ref-type="bibr" rid="R15" hidden="1" id="34" class="deo:Reference">15</xref>, <xref ref-type="bibr" rid="R16" hidden="1" id="35" class="deo:Reference">16</xref> – 17]. One technique that has been used to determine semantic equivalence between seemingly unrelated documents by exploiting similarities in their semantic structure is latent semantic analysis (LSA). LSA is a computationally inten- sive technique designed to allow intelligent document query and retrieval. Its principal advantage over other techniques is its ability to overcome the problems of polysemy (multiple meanings per word) and synonymy (multiple words that mean the same thing). Synonymy in particular appears to be a characteristic problem of pre-requirements tracing. For instance, multiple documents will refer to the same aspect of the system in different ways. For example VDU, monitor and interface could all be used to refer to a user display. This capability of LSA was demonstrated when we used it to process a sample corpus of newspaper articles from the Guardian and Observer newspapers. LSA determined that two stories were related even though a log-likelihood score on their content showed they had shared few common terms. Upon inspection of their content their semantic relationship became apparent. The two articles were entitled ‘M&amp;S Turns its Back on Youth’ and ‘Game Boys (sic) Prepare for Bloody War’. They were semantically related because both articles focused on the efforts of retailers to drop and gain respectively the attention of the youth market. LSA is commonly accepted to be a shallow technique that accurately manages to approximate human expectations of linguistic comparison. LSA has been successfully used to<marker type="column" number="2"/><marker type="block"/> trace between other artefacts found in software projects, such as between use cases, interaction diagrams and test cases [18] and between documentation and source code [19]. LSA has also been used to reverse engineer requirements coverage views from design and test artefacts [20]. LSA results in the formation of a multidimensional document-word space. The number of occurrences of each word in a document determines the document’s magnitude in that dimension, thereby determining the position of the document in the space. Similar documents appear to cluster together in the space. This clustering can be heigh- tened by reduction of the space to fewer dimensions by singular value decomposition (SVD). Similarity can therefore be determined via a variety of algorithms, such as simple Euclidean distance or a cosine score between vectors. A document – word space technique based on lexical similarity measures, rather than LSA, has been used by Natt och Dag et al. [21] to determine the linguistic equivalence between two different sources of requirements: market requirements and business requirements. The technique used resulted in more than 50% of the correct business – market requirement equivalence relationships being identified. Natt och Dag et al. estimate that there is potential for up to 63% of similar business – market requirement equivalence relationships to be identified using their technique. However, lexical similarity measures are unable to cope with polysemy and synonymy, and this motivated our selection of LSA as the basis of our work. We make three assumptions during the course of our work:<marker type="block"/> † Both a clearly separable specification (e.g. a user requirements specification) and a set of source materials (e.g. interview transcripts, standards, user manuals, law) exist. † Both source and specification materials are transcribed in text. † Both source and specification materials are recorded in natural language.<marker type="block"/> The starting structure for LSA is a term – document frequency matrix. The columns of A comprise the documents<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> whose similarities are being investigated. The rows of A comprise the terms used in the documents. LSA depends crucially on the ability to identify appropriate terms to add to the matrix as rows. While it is possible to add every word found in every document to the matrix as a row, it is not a practical use of computer memory, as stop words such as ‘and’, ‘of’ and ‘that’ do not contribute towards semantic meaning in this context. Additionally, there is already a degree of semantic connectedness between words such as ‘jump’, ‘jumps’, ‘jumping’ and ‘jumped’. Such words can be stemmed so that they retain only their root meaning. In the example given, all the ‘jump’ words would be reduced to just ‘jump’. Not only does stemming reinforce the notion of semantic connectedness between words that share a common root, it also increases computational efficiency. Therefore before construction of the matrix, it is necessary to remove stop words and to stem the remaining words in order to determine their root. Robust algorithms to achieve stemming exist and these operations may be undertaken in one step. Stemming and stop word removal are essential to identify the terms. It is also necessary to determine what constitutes a document. The word ‘document’ does not necessarily refer to a whole document in this case. It is possible for a document to be split into parts to enable semantic comparison. To make this distinction clear, the word ‘document’ will be replaced by ‘chunk’ whenever referring to a piece of text that constitutes a column of the matrix. The splitting of documents into chunks is essential. This means that sections of the specification and of the source documents from which they are derived must be isolated as chunks of the specification and source documents, respectively. Failure to split the documents into chunks would mean that the granularity of the relationships that LSA could identify would be too course to be useful. <xref ref-type="fig" rid="F3" id="44" class="deo:Reference">Fig. 3</xref> illustrates the process of splitting the documents. The appropriate size of a chunk can be determined by a built-in heuristic such as 10 consecutive sentences per chunk. If a heuristic is used, it must be appropriate for large sections of free flowing text that are typically found in source material. For example, 10 consecutive<marker type="column" number="2"/><marker type="block"/> sentences in the requirements specification could be too granular since it might force different requirements into the same chunk. This would impede the determination of each requirement’s origin. Alternatively, the analyst may determine the start and end boundaries of chunks in the requirement specification by manual inspection. As currently implemented, the size and content of chunks are determined by a heuristic boundary detection algorithm [22]. Our tool, Prospect, implements three distinct phases of analysis, discussed subsequently.<marker type="block"/> Collation: All source documentation and the requirements specification are prepared here. Several steps are performed, such as collating all the documents into a single logical col- lection for easier processing, tokenisation, stemming and the removal of syntactic elements of speech. Both the source material and the specification are then split into chunks to enable comparison. Comparison: The semantic equivalence of chunks is determined by the use of LSA. Chunks of source material are then compared against chunks of the requirements specification and the similarities are noted. The application of LSA that we propose requires the construction of a document similarity matrix. Analysis: The results are presented to the analyst, who evaluates the results by sampling them. If the analyst can verify a selection of matches between requirements chunks and source text, particularly where little common vocabulary is apparent, this will build confidence that other requirements that Prospect has identified as being badly sourced are rooted in tacit knowledge.</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="28" page="2" column="2">212</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="29" page="2" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="31" page="2" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="30">http://ietdl.org/copyright.jsp</ext-link></outsider>
        <region class="DoCO:FigureBox" id="F2">
          <image class="DoCO:Figure" src="62k5.page_003.image_01.png" thmb="62k5.page_003.image_01-thumb.png"/>
          <caption class="deo:Caption" id="33" page="3" column="1">Fig. 2 Overview of pre- and post-requirements specification tracing, adapted from Gotel and Finkelstein [11]</caption>
        </region>
        <outsider class="DoCO:TextBox" type="footer" id="40" page="3" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="41" page="3" column="2">213</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="43" page="3" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="42">http://ietdl.org/copyright.jsp</ext-link></outsider>
        <region class="DoCO:FigureBox" id="F3">
          <image class="DoCO:Figure" src="62k5.page_004.image_02.png" thmb="62k5.page_004.image_02-thumb.png"/>
          <caption class="deo:Caption" id="49" page="4" column="1">Fig. 3 Identification of sources of requirements Black lines represent the splitting of source and specification into chunks Grey lines show the equivalences found by using LSA Here chunks t(<xref ref-type="bibr" rid="R6" id="47" class="deo:Reference">6</xref>) and t(<xref ref-type="bibr" rid="R7" id="48" class="deo:Reference">7</xref>) would be flagged by the system as examples of poorly sourced knowledge, potentially tacit knowledge, as their source is not known</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="52" page="4" column="2">4 Operation of LSA</h1>
        <region class="DoCO:TextChunk" id="66" page="4" column="2">LSA depends on the construction of a term – chunk space given by matrix A filled with the frequencies of terms in the given chunks. Essentially, A is a count of how many times term i occurs in chunk j, that is A 1⁄4 1⁄2a ij ð1Þ <marker type="page" number="5"/><marker type="column" number="1"/><marker type="block"/> where a ij is the frequency of term i in chunk j. Berry et al. [23] note that A is normally sparse. A is then subjected to two functions, L(i, j) and G(i) which determine local and global weights, respectively, to be applied to frequencies in A. L(i, j) is used to boost the importance of selected terms in selected chunks. G(i) is a global transform applied to the whole of matrix A. Typically, G(i) is a logar- ithm that increases the separation between the most fre- quently used terms in A. Appropriate algorithms for L(i, j) and G(i) are given by Dumais [24]. The construction of A is now complete. The pivotal step in LSA is the decomposition of A via SVD into three matrices, that is A 1⁄4 U S V T ð2Þ The SVD results in the formation of three decomposed matrices of A that demonstrate the underlying semantic structure of the terms and chunks in A. These three matrices represent the breakdown of any relationship between chunks via their terms into linearly independent vectors. The SVD is essentially a method of rotating matrix A such that the largest variation between the chunks runs along the first axis of the n-dimensions space that A is in, the second largest variation runs along the second axis, and so on [22]. This rotation is the first contribution of SVD to LSA. The second contribution is that performing the SVD of A allows the best least-squared error approximation of A for a given dimension space to be computed, where the dimensionality of that space is less than the dimensionality of matrix A. Essentially, the SVD is a way of projecting A into a lower dimension space in the best possible manner. This step is the essential part of LSA, as similar vectors are collapsed closer to each other than in their previously higher dimensional space. This collapse allows words that have similar patterns of occurrence across chunks to pull their respective document vectors closer to each other in the reduced space, inferring similarity of meaning. To use SVD to project A into a lower dimensional space, the rank-k approximation of A must be computed, written as A k . By restricting A to its first k rows, and restricting the matrices U, S and V accordingly, we compute A k 1⁄4 U mÂk S kÂk V kÂn T ð3Þ Graphically, the reduction of A to A k may be depicted as shown in <xref ref-type="fig" rid="F4" id="58" class="deo:Reference">Fig. 4</xref>. The proof that A k is the best rank-k approximation of A is given by Berry et al. [23]. Determining the appropriate number of dimensions to reduce A into is a difficult problem. Deerwester et al. provide evidence that k 1⁄4 100 is appropriate for most information retrieval problems [25]. An example of the reduction of A to A k is shown in <xref ref-type="fig" rid="F5" id="59" class="deo:Reference">Fig. 5</xref>. Here, a value of k 1⁄4 2 was chosen to enable a<marker type="column" number="2"/><marker type="block"/> simple vector plot to be drawn. Two documents are present in <xref ref-type="fig" rid="F5" id="63" class="deo:Reference">Fig. 5</xref>, an ethnographic report and a concept of operations that we refer to in Section 5. Each vector represents a chunk of a document, defined for this example as five continuous sentences of each document. The similarity of parts of each document may be determined by the angle between their associated vectors. The reduction of A into its best rank-k form, A k , allows the construction of matrix B which may be defined as<marker type="block"/> B kÂn 1⁄4 S kÂk V kÂn ð4Þ from which the document correlation matrix E T E may be computed where E is matrix B with length normalised columns. The document correlation matrix is not the tra- ditional output of the LSA process. Normally, a query vector is represented in the reduced space of A k and the similarity between the query vector and the chunk vectors in the Cartesian plane is noted. Chunk vectors that are similar to the query vector are returned as candidate matches. However, the application of LSA that we propose requires only the comparison of chunks to each other, hence the calculation of E T E. The document similarity matrix contains numbers in the range [21, 1], where 21 represents content that is semantically divergent, and 1 represents content that is semantically identical. It is also symmetric about the identity line. An example document similarity matrix is shown subsequentles. For clarity, only the lower portion of the matrix is shown. 0 c 1 c 2 c 3 c 4 c 5 1 E T E 1⁄4 B B @ B B B B c c c c 1 2 3 4 1:00 0:59 0:97 0:11 À0:11 0:39 1:00 0:86 1:00 1:00 C C C C A C C ð5Þ c 5 0:00 À0:21 0:81 0:99 1:00<marker type="block"/> The matrix is now in an easy to read form. For example, the relatedness of c 1 to c 3 is 0.59. The closest match a chunk can have is itself, hence the repeated value 1.0 along the identity line. To eliminate poor matches, it is necessary to determine a minimum threshold value, a .</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="54" page="4" column="2">214</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="55" page="4" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="57" page="4" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="56">http://ietdl.org/copyright.jsp</ext-link></outsider>
        <region class="DoCO:FigureBox" id="F4">
          <image class="DoCO:Figure" src="62k5.page_005.image_03.png" thmb="62k5.page_005.image_03-thumb.png"/>
          <caption class="deo:Caption" id="62" page="5" column="1">Fig. 4 Graphical representation of the reduction of A to A k Taken from Berry et al. [23]</caption>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="67" page="5" column="2">5 Applicability to pre-requirements tracing</h1>
        <region class="DoCO:TextChunk" id="68" page="5" column="2">In order to test the validity of our approach, LSA was used to trace between a concept of operations for a new system and an ethnographic report of the existing system. The ethnographic reports relate to a UK ATC system. The concept of operations was developed by Bentley [26] for a tool to prototype displays for command and control systems where the requirements were informed by the ATC ethnographic reports. The ethnographic report was scanned from a printed document using optical character recognition techniques. It contained scanning errors that resulted in spelling and grammatical mistakes that we left uncorrected in order to better approximate real world documents. When determining an appropriate size of each document chunk, a trade-off becomes immediately apparent. Small chunk sizes (e.g. single sentences) can lead to difficulty in analysts accurately interpreting results as there are too many chunks and relations to concurrently track. Larger chunk sizes are too granular to enable effective comparison. We decided to use five sentences per chunk for this exper- iment. The current version of Prospect uses variable size chunks, so for example, the analyst can investigate individ- ual requirements clauses or steps in a scenario.</region>
        <outsider class="DoCO:TextBox" type="footer" id="69" page="5" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="70" page="5" column="2">215</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="72" page="5" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="71">http://ietdl.org/copyright.jsp</ext-link></outsider>
        <region class="DoCO:FigureBox" id="F5">
          <image class="DoCO:Figure" src="62k5.page_006.image_04.png" thmb="62k5.page_006.image_04-thumb.png"/>
          <caption class="deo:Caption" id="74" page="6" column="1">Fig. 5 Reduction of A to A k where k 1⁄4 2 Light grey represents vectors of an ethnographic report and dark grey represents a concept of operation document</caption>
        </region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="75" page="6" column="1">5.1 Evaluation</h2>
          <region class="DoCO:TextChunk" id="91" page="6" column="1">Linguistic analysis techniques may be evaluated by a number of metrics, such as accuracy, precision, recall, error and fallout. Accuracy, error and fallout are measures of completeness, used to show the proportion of correct results that a retrieval system will output. As the document similarities provided by LSA are normally used in conjunc- tion with a threshold to eliminate weak matches, the accuracy of the system will become high and the error will become low as the threshold increases. These measures do not tell us if the document similarities provided are correct, that is that they match human expectation. Two measures that can be used to provide meaningful results are recall and precision. In order to calculate these measures, it is first necessary to manually determine the correct links between the concept of operations and the ethnographic reports. The recall and precision may then be calculated as follows: <marker type="block"/> 1. Compute the similarities between chunks; 2. Select a threshold, a , in the range [21, 1]; 3. Select a chunk of the concept of operations, i; 4. Manually compare i to all chunks of the ethnographic report to produce a set of matches, r man ; 5. For chunk i, determine all the chunks of the ethnographic report that have a similarity value greater than a to produce a set of matches r lsa ; 6. Calculate the recall as recall 1⁄4 jr man &gt; r lsa j ð6Þ jr man j 7. Calculate the precision as precision 1⁄4 jr man &gt; r lsa j ð7Þ jr lsa j<marker type="block"/> Essentially, recall can be seen as the percentage of correct associations in the current list with respect to the total number of correct associations, that is how many correct associations<marker type="column" number="2"/><marker type="block"/> have been discovered. Precision is the percentage of correct associations with respect to the size of the associations list, that is how many of the results are correct. It is therefore expected that the recall of LSA will be high when the threshold is low. By setting the threshold to 21, the lowest threshold possible, all documents will be included in R lsa and will therefore result in total recall. In other words, every chunk in the concept of operations will appear to be derived from every chunk in the ethnographic report. A threshold of 21 will also result in poor precision as the number of incorrect associations in R lsa is high. However, as the threshold tends towards 1 precision should increase as the weak and noisy candidate matches are eliminated. In order to test that LSA can be used to perform semantic level comparison on these sorts of documents, the associations between 4 of the 25 chunks of the concept of operations and the 85 chunks of the ethnographic report were determined manually. The manual associations were then used to plot the recall and precision against threshold, as shown in <xref ref-type="fig" rid="F6" id="79" class="deo:Reference">Fig. 6</xref>. This figure is made from a population sample. Corresponding confidence interval plots are presented in Figs. 7 and 8. The confidence interval plots show the 95% confidence interval for each sampled point. The range in which 95% of all members of the population<marker type="page" number="7"/><marker type="column" number="1"/><marker type="block"/> are contained within assuming a p normally distributed sample is calculated as x + 1.96( s / n). <xref ref-type="fig" rid="F6" id="89" class="deo:Reference">Fig. 6</xref> clearly shows that as the minimum threshold of relatedness increases, the recall decreases and the precision increases. <xref ref-type="fig" rid="F6" id="90" class="deo:Reference">Fig. 6</xref> provides evidence that LSA is approximat- ing human expectations of semantic equivalence for the documents being considered. If LSA were providing the opposite of human expectation, we would expect to see the precision drop as a function of threshold. If LSA were producing random results, we would expect to see no trend at all in the precision and recall curves.</region>
          <region class="DoCO:FigureBox" id="F6">
            <image class="DoCO:Figure" src="62k5.page_006.image_05.png" thmb="62k5.page_006.image_05-thumb.png"/>
            <caption class="deo:Caption" id="82" page="6" column="2">Fig. 6 Recall and precision as a function of threshold</caption>
          </region>
          <outsider class="DoCO:TextBox" type="page_nr" id="83" page="6" column="2">216</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="84" page="6" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
          <outsider class="DoCO:TextBox" type="footer" id="86" page="6" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="85">http://ietdl.org/copyright.jsp</ext-link></outsider>
          <region class="DoCO:FigureBox" id="F7">
            <image class="DoCO:Figure" src="62k5.page_007.image_06.png" thmb="62k5.page_007.image_06-thumb.png"/>
            <caption class="deo:Caption" id="88" page="7" column="1">Fig. 7 Recall and associated 95% confidence intervals</caption>
          </region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="92" page="7" column="1">5.2 Badly sourced material</h2>
          <region class="DoCO:TextChunk" id="99" page="7" column="1">We define any chunk as being badly sourced if it has no relatedness to chunks belonging to other documents for a . 0.1. Chunks of the concept of operations that were identified as being poorly sourced were examined. These chunks fell into two main categories: <marker type="block"/> 1. A detailed description of the semantics of shared user displays. These were requirements invented by Bentley as part of his work on shared displays. 2. Chunks for which Bentley had used knowledge from his own field work at the ATC centre and knowledge elicited by him from the ethnographer. Neither type of information was explicitly represented in the ethnographic report.<marker type="block"/> Other less significant examples of poorly sourced text were due to our erroneously scanning too much of one of the leading pages in the document that contained the concept of operations, but was unrelated to the concept of operations. LSA correctly identified this material as not being associated with the ethnographic report.<marker type="column" number="2"/><marker type="block"/> The results include examples of Prospect correctly iden- tifying poorly sourced chunks of Bentley’s concept of operations as potentially tacit in nature. One example of a poorly sourced chunk from the concept of operations contains the lexical term ‘strip’. Strip is a common word in both documents, but despite its high frequency of occurrence, the chunk is correctly identified as poorly sourced. The chunk deals primarily with a description of the pragmatics of different views of the airspace, such as a radar view or a view based upon a ‘rack’ of paper strips, where each strip represents information about an aircraft. Similarly, despite many instances of the word ‘radar’ in the ethnographic report no strong link is made with the concept of operations chunk, as LSA has correctly identified that the chunk is primarily concerned with a concept not covered in the ethnography. The tacit- like knowledge identified in this chunk is the general HCI knowledge Bentley incorporated into his description of how concepts in the air traffic control domain form part of his system. This example appears to be an instance of cognitive tacit knowledge on the part of the analyst, and contrasts with the technical tacit knowledge seen in <xref ref-type="fig" rid="F1" id="98" class="deo:Reference">Fig. 1</xref>.</region>
          <region class="DoCO:FigureBox" id="F8">
            <image class="DoCO:Figure" src="62k5.page_007.image_07.png" thmb="62k5.page_007.image_07-thumb.png"/>
            <caption class="deo:Caption" id="97" page="7" column="1">Fig. 8 Precision and associated 95% confidence intervals</caption>
          </region>
        </section>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="100" page="7" column="2">6 Conclusion</h1>
        <region class="DoCO:TextChunk" id="101" page="7" column="2">We propose a method to highlight requirements that are potentially based on tacit or tacit-like knowledge. This identification is made possible by examining the provenance of each requirement, effectively showing the source material that contributes to it. We demonstrate that a semantic-level comparison enabling technique is appropriate for this purpose. By splitting up requirements specifica- tions and the source material from which they were derived into chunks and comparing their semantic similarities, it is possible to determine likely sources for each chunk of the requirements specification. Further, this identification process permits us to identify requirements not firmly derived from the supplied source material. We argue that these requirements represent either poorly sourced knowledge or instances of tacit knowledge embedded in the problem domain or the analyst’s mind. We have demonstrated that LSA, a linguistic technique designed to overcome the problems of polysemy and synonymy, can approximate human expectations of semantic relatedness between chunks of source material and their resulting specification. Planned extensions to this work include an examination of the most effective chunking strategies to use, as well as an examination of the effect of chunk size on the ability of LSA to determine semantic equivalence that meets human expectation. Additionally, there exists the possibility of exploiting other lexical-level techniques, such as incor- porating log-likelihood scores into Prospect to further aid requirement engineers in determining candidate matches.</region>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="102" page="7" column="2">7 References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="103" page="7" column="2">1 Nuseibeh, B., and Easterbrook, S.: ‘Requirements engineering: a roadmap’. Proc. 23rd Int. Conf. Software Engineering (ICSE 2000), 2000, pp. 35– 46</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="105" page="7" column="2">2 Takeuchi, H.: ‘Beyond knowledge management: lessons from Japan’ <ext-link ext-link-type="uri" href="http://www.sveiby.com/articles/LessonsJapan.htm," id="104">http://www.sveiby.com/articles/LessonsJapan.htm,</ext-link> June 1998, last accessed August 2005</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="106" page="7" column="2">3 Polanyi, M.: ‘The tacit dimension’ (Paul Smith Publishing, 1983), ISBN 0-8446-5999-1</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="107" page="7" column="2">4 Ryle, G.: ‘The concept of mind’ (University of Chicago Press, 1984), ISBN 0-2267-3296-7</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="108" page="7" column="2">5 Ryan, K.: ‘The role of natural language in requirements engineering’. Proc. IEEE Int. Symposium RE, 1993, pp. 80– 82</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="113" page="8" column="1">6 Nonaka, I., and Takeuchi, H.: ‘The knowledge-creating company’ (Oxford University Press, 1995)</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="114" page="8" column="1">7 Collins, H.M.: ‘What is tacit knowledge’ in ‘The practice turn in contemporary theory’ (Routledge, London, 2001), pp. 107– 119</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="116" page="8" column="1">8 Parnas D.: ‘Preparing requirements documents using the four variable model’. A presentation to the British Computer Society’s Requirements Engineering Specialist Group, <ext-link ext-link-type="uri" href="http://www.resg.org.uk/" id="115">http://www.resg.org.uk/</ext-link> parnastutorial.pdf, accessed June 2006</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="117" page="8" column="1">9 Bentley, R., Hughes, J.A., Randall, D., Rodden, T., Sawyer, P., Shapiro, D., and Sommerville, I.: ‘Ethnographically-informed systems design for air traffic control’. Proc. ACM CSCW’92 Conf. Computer-Supported Cooperative Work, Ethnographically-Informed Design, 1992, pp. 123– 129</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="119" page="8" column="1">10 Finkelstein, A.: ‘Unsolved Problems in Requirements Engineering’. A presentation to the British Computer Society’s Requirements Engineering Specialist Group, <ext-link ext-link-type="uri" href="http://www.resg.org.uk/unsolved.pdf," id="118">http://www.resg.org.uk/unsolved.pdf,</ext-link> accessed January 2005</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="120" page="8" column="1">11 Gotel, O.C.Z., and Finkelstein, A.C.W.: ‘An analysis of the requirements traceability problem’ in ‘First International Conference on Requirements Engineering (ICRE)’, 1994, pp. 94–101 (IEEE Computer Society Press)</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="121" page="8" column="1">12 Ramesh, B., and Jarke, M.: ‘Toward reference models of requirements traceability’, IEEE Trans. Softw. Eng., 2001, 27, (1), pp. 58– 93</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="123" page="8" column="1">13 Oxford University Computing: ‘British National Corpus’, <ext-link ext-link-type="uri" href="http://www" id="122">http://www</ext-link>. natcorp.ox.ac.uk, accessed February 2005</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="124" page="8" column="1">14 Sawyer, P., Rayson, P., and Cosh, K.: ‘Shallow knowledge as an aid to deep understanding in early phase requirements engineering’, IEEE Trans. Softw. Eng., 2005, 31, (11), pp. 969–981</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="125" page="8" column="1">15 Gervasi, V., and Nuseibeh, B.: ‘Lightweight validation of natural language requirements’, Softw. Pract. Exp., 2002, 32, (2), pp. 113–133</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="127" page="8" column="1">16 Maarek, Y.S., and Berry, D.M.: ‘The use of lexical affinities in requirements extraction’ in Greenspan, S. (Ed.): ‘Proceedings of the 5th International Workshop on Software Specification and Design’, <marker type="column" number="2"/><marker type="block"/> (IEEE Computer Society Press), Pittsburgh, PA, May 1989, pp. 196– 202</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="128" page="8" column="2">17 Lecoeuche, R.: ‘Finding comparatively important concepts between texts’. Proc. 15th IEEE Int. Conf. Automated Software Eng. (ASE’00), 2000, pp. 55– 63</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="129" page="8" column="2">18 Antoniol, G., Canfora, G., Casazza, G., De Lucia, A., and Merlo, E.: ‘Recovering traceability links between code and documentation’, IEEE Trans. Softw. Eng., 2002, 28, (10), pp. 970 –983</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="130" page="8" column="2">19 Marcus, A., and Maletic, J.I.: ‘Recovering documentation- to-source-code traceability links using latent semantic indexing’ in ‘International conference on software engineering’ (IEEE Computer Society Press, 2003), pp. 125– 134</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="131" page="8" column="2">20 Lormans, M., and van Deursen, A.: ‘Reconstructing requirements coverage views from design and test using traceability recovery via lsi’ in ‘TEFSE’05: Proceedings of the 3rd International Workshop on Traceability in emerging forms of software engineering, New York, NY, USA, 2005, pp. 37– 42, ACM Press</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="132" page="8" column="2">21 Natt och Dag, J., Gervasi, V., Brinkkemper, S., and Regnell, B.: ‘A linguistic-engineering approach to large-scale requirements management’, IEEE Softw., 2005, 22, (1), pp. 32– 39</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="133" page="8" column="2">22 Manning, C.D., and Schu  ̈rtze, H.: ‘Foundations of statistical natural language processing’ (The MIT Press, Cambridge, England, 2000)</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="134" page="8" column="2">23 Berry, M.W., Dumais, S.T., and O’Brien, G.W.: ‘Using linear algebra for intelligent information retrieval’, SIAM Rev., 1995, 37, (4), pp. 573–595</ref>
          <ref rid="R24" class="deo:BibliographicReference" id="135" page="8" column="2">24 Dumais, S.T.: ‘Improving the retrieval of information from external sources’, Behav. Res. Methods, Instrum. Comput., 1991, 23, pp. 229–236</ref>
          <ref rid="R25" class="deo:BibliographicReference" id="136" page="8" column="2">25 Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R.: ‘Indexing by latent semantic analysis’, J. Am. Soc. Inf. Sci., 1990, 41, (6), pp. 391–407</ref>
          <ref rid="R26" class="deo:BibliographicReference" id="137" page="8" column="2">26 Bentley, R.: ‘Supporting multi-user interface development for cooperative systems’. PhD Thesis, 1994 Lancaster University</ref>
        </ref-list>
        <outsider class="DoCO:TextBox" type="footer" id="109" page="7" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="110" page="7" column="2">217</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="112" page="7" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="111">http://ietdl.org/copyright.jsp</ext-link></outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="138" page="8" column="2">218</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="139" page="8" column="2">IEE Proc.-Softw., Vol. 153, No. 6, December 2006</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="141" page="8" column="2">Downloaded 12 Jan 2007 to 194.80.32.9. Redistribution subject to IET licence or copyright, see <ext-link ext-link-type="uri" href="http://ietdl.org/copyright.jsp" id="140">http://ietdl.org/copyright.jsp</ext-link></outsider>
      </section>
    </body>
  </article>
</pdfx>
