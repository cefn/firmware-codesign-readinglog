<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>a377efff868933d3699cb8b7884edcd92007b68cdef58e341eac2d2f6fa07a8d</job>
    <base_name>62eu</base_name>
    <doi>http://dx.doi.org/10.1145/122672.122685</doi>
    <warning>Original PDF was found to be an image-based/possible OCR document. Output quality may be degraded.</warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="header" id="1"> </outsider>
      <title-group>
        <article-title class="DoCO:Title" id="2">THE USE OF THINK-ALOUD EVALUATION METHODS I N DESIGN</article-title>
      </title-group>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="3">PETER C . WRIGH T ANDREW F . MON K</name>
        </contrib>
      </contrib-group>
      <abstract class="DoCO:Abstract" id="4">Abstract: This paper reports on two studies in which team s of two or three trainee designers evaluated a use r interface by observing a user working throug h some set tasks . These users were instructed to think aloud as they worked . The instruction received by the designers took the form of a brief how-to-do-it manual . Study 1 demonstrate s that this method is effective. Study 2 found that more problems were detected by the designers of the system than other groups . Also, designers cannot predict the problems users wil l experience in advance of user testing .</abstract>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="5" page="1" column="1">Introductio n</h1>
      </section>
      <region class="DoCO:TextChunk" id="9" page="1" column="1">The design of a good interface is difficult . It involves getting feedback from users early in the design process an d iterating through several versions of the system . This i s potentially time consuming . In the development of the STAR interface ( <xref ref-type="bibr" rid="R1" id="6" class="deo:Reference">Bewley, Roberts, Schroit &amp; Verplank 1983</xref>) experiments were performed to compare different icon sets and schemes for selecting text. A software developer following this model would have to brief huma n factors specialists about the design. The human factors specialists would then recruit users to take part in th e experiments, analyse results and report back to designers . Lawrence Tesler, Vice President of Advanced Technolog y Apple Computers INC ., in his Plenary Address to the CHI '89 conference, commented that, given the pressures o f commercial software development a two week period ma y well be all that is available for user testing in th e development programme of a moderately large softwar e application . It is difficult to see how all this could b e achieved in such a small time.<marker type="column" number="2"/><marker type="block"/> One solution to this problem is to have designers carry ou t their own evaluation . In this way the time consumin g process of designer briefing evaluator and evaluato r communicating results to designer is eliminated . <xref ref-type="bibr" rid="R3" id="8" class="deo:Reference">Gould and Lewis (1985)</xref> point out that this approach also serves t o involve the user directly in design . But this solution i s workable only if the evaluation methods used are of th e sort that can be understood by designers who may have little training in human factors. Think aloud methods are being used increasingly for evaluation and they ar e plausible candidates for such a role .</region>
      <section class="deo:Methods">
        <h1 class="DoCO:SectionTitle" id="10" page="1" column="2">Study 1 - how effective are think alou d methods ?</h1>
      </section>
      <region class="DoCO:TextChunk" id="20" page="1" column="2">Trainee software engineers were taught a think alou d evaluation method and their success with the method wa s compared with that of more experienced evaluators . Training was limited to reading a short manual ( <xref ref-type="bibr" rid="R4" id="11" class="deo:Reference">Wright, Monk &amp; Carey 1989</xref>) . The time available for evaluatio n and report writing was also limited. In this way we hoped to obtain an idea of just how much could be achieved b y designers, in a time representative of commercial settings. In our version of the think aloud method (<xref ref-type="bibr" rid="R4" id="12" class="deo:Reference">Wright an d Monk 1989</xref>), the users are told to think of themselves as co-evaluators of the system . They are occasionally asked questions such as What will the system do if. . . .?, Why did you do that ? When the user asks questions, about what to do next for example, the evaluator asks further questions of the user to find out about their understanding of th e operations available, their interpretation of screen and s o on . We call this 'co-operative evaluation'. The system evaluated was the menu-based bibliographi c database running on an IBM PC described by Wright &amp;<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/>  <xref ref-type="bibr" rid="R4" id="18" class="deo:Reference">Monk (1989)</xref> . The system keeps a time-stamped log of th e user's key depressions and the state of the display windows . The trainees were provided with a 1st-year undergraduate psychologist who acted as their user . They had not used REF before and had virtually no experience with desk-top computing or human factors testing. Each team tested their user for about 2 hours . The trainees noted the problem s that the user experienced. They audio-taped the user's talk , and a time-stamped log of the session was available . Each trainee wrote a report independently for the purpose of assessment. All of them were read individually and th e problems each trainee reported were listed . This listin g took the form of an inventory of scenarios describin g situations in which the behaviour of the system caused th e user problems. This classification process yielded a total o f 29 different problems. On average each team identified 9 . 6 of the 29 problems . There was some variance between the teams (standard deviation= 3 .04), with one team detectin g 14 problems and two others detecting only 5 . The 29 problems varied in character. Many were relativel y trivial while others were not . In order to identify the more serious ones, the 29 problems were classified according t o whether they were persistent and whether they had consequences. Persistent problems were those which the user could do nothing to avoid even when they were full y aware of the problem . Problems with consequences were those which result in unwanted changes to the data or i n intended changes not being carried out . In this way 10 of the complete set of 29 problems detected were identified a s serious . The average team detected 4 .5 of these. Thus despite the very limited training and time for testing mos t of the teams were able to converge on four significan t usability problems . The question remains whether the 29 problems detected b y the trainees as a group represents an exhaustive list of th e problems with this system . The REF system had bee n evaluated in a previous study (<xref ref-type="bibr" rid="R4" id="19" class="deo:Reference">Wright &amp; Monk 1989</xref>), it i s thus possible to compare the problems identified by th e trainees in this study with those identified in an evaluation by specialists during a much fuller procedure . The author' s evaluation yielded 40 problems including all 29 of the problems reported by the trainees. This leaves 11 problem s that the trainees failed to detect. Of these 11 however , seven were associated with aspects of REF's functionalit y the trainees were not required to evaluate (e.g., printing an d inserting references) . This leaves only four problems tha t the trainees might have been expected to detect but missed .</region>
      <outsider class="DoCO:TextBox" type="footer" id="14" page="1" column="2">SIGCHI Bulletin January 1991</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="15" page="1" column="2">55</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="16" page="1" column="2">Volume 23, Number 1</outsider>
      <outsider class="DoCO:TextBox" type="header" id="17" page="2" column="1"> </outsider>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="21" page="2" column="1">Study 2 - Is It easier to spot problems wit h your own or someone else's design ?</h1>
        <region class="DoCO:TextChunk" id="24" page="2" column="1">The trainees in study 1, were not evaluating their ow n design . The REF system was chosen because it had bee n studied previously and its problems were known an d documented . There may however be a number of difference s between evaluating your own design and evaluating that of someone else. It may be more difficult to see your ow n design in objective terms and this may result in les s problems being detected . Alternatively the more thoroug h understanding that comes from designing the system ma y make it easier for designers to spot problems . In this <marker type="column" number="2"/><marker type="block"/> second study we compare two new groups of trainees . Group A designed and tested their own prototype and grou p B designed a prototype but tested the prototypes of grou p A . In addition before testing took place, we asked the designers in group A to make some predictions about what problems they would observe during user testing . 16 pairs of trainees first designed a paper mock-up of a n electronic touch-screen data-entry system, either a syste m for collecting data from applicants to be the first Britis h cosmonaut, or a similar system for applicants to a computer dating agency . The information to be sought was specified as a set of ADA data structures along with constraints on the display and the stylus input to be used . Four `computer dating' and four 'British cosmonaut ' designs were selected randomly for evaluation. These wer e all evaluated by their designers one week after completin g the designs by testing a naive user as in Study 1 . I n addition each of the computer dating designs were evaluate d by a team who had previous designed a cosmonaut interfac e and each of the cosmonaut designs were evaluated by a team who had designed a dating interface . Thus eight team s evaluated their own design and eight teams evaluated a design that was new to them . Considering all problems, the mean number of problem s predicted by each design team was 5 .8, and the mean number observed by designers was 10 .1 and by non designers was 6 .5 . But many of these problem whe n considered in detail were discovered to be problems related to the data structures rather than the screens designed . For example users complained that the system had not aske d them for information which they considered to be relevant . Since we are concerned primarily with the ability designers to detect problems with the part of the system they actuall y designed, the authors analysed the problems and discarde d any they considered to be unrelated to the design of th e interface. We also discarded problems due to the fact that a paper and pencil mock-up had been used. This reduced the number of problems for further analysis by approximatel y 40% . The problems left were those which would have led to data being wrongly updated. Ignoring the discarded problems, the mean number of problems detected by th e designers of the prototype was 3 .9 (s=1 .36) compared with 1 .9 (s=1 .62) for evaluators not involved in the design . Thi s difference was significant with a Mann-Whitney U tes t (U=8, p=0.005) . Thus it would appear that designers are better at detecting problems in their own design than in someone else's . Designers are not however very accurate at predicting what problems they will observe . Designers predicted a mean o f 3 .6 problems but of those problems only 0 .9 were actually observed. So the problems observed were quite unexpected . The non-designer teams performed similarly . Of th e problems observed by the non-designers, few had bee n predicted by the designers.</region>
        <region class="DoCO:FigureBox" id="Fx23">
          <image class="DoCO:Figure" src="62eu.page_002.image_02.png" thmb="62eu.page_002.image_02-thumb.png"/>
        </region>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="25" page="2" column="2">Conclusion s</h1>
        <region class="DoCO:TextChunk" id="26" confidence="possible" page="2" column="2">It is concluded that user testing with think-aloud method s not only is an effective evaluation technique for designers, but also that there are significant gains to be had fro m designers carrying out their own evaluations .</region>
        <outsider class="DoCO:TextBox" type="footer" id="27" page="2" column="2">SIGCHI Bulletin January 1991</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="28" page="2" column="2">56</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="29" page="2" column="2">Volume 23, Number 1</outsider>
        <outsider class="DoCO:TextBox" type="header" id="30" page="3" column="1"> </outsider>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="31" page="3" column="1">Reference s</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="32" page="3" column="1">Bewley, W .L ., Roberts, T.L . Schroit, D. &amp; Verplank, W .L . (1983) Human factors testing in the design of the Xerox's 8010 "Star" office workstation . Proceedings of CHI `83, pp . 72-77 .</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="33" page="3" column="1">Gould, J .D ., Boies, S .J ., Levy, S ., Richards, J .T . &amp; Schoonard, J . (1987) The 1984 Olympic message system : A test of behavioural principles of syste m design, Communications of the ACM, 30, 758-769 .</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="34" page="3" column="1">Gould, J .D ., &amp; Lewis, C ., (1985) Designing for usability: Key principles and what designers think . Communications of the ACM, 28, pp. 300-31 1</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="35" confidence="possible" page="3" column="1">Wright, P.C . &amp; Monk, A.F. (1989) Evaluation for design . In Sutcliffe, A. &amp; Macaulay, L . (Eds .), People and computers V, Cambridge University Press, pp . 345-358 .</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="37" page="3" column="2">Wright, P.C . Monk, A .F. &amp; Carey, T. (1989) Cooperative evaluation : The York manual . Technical report, Dept of Psychology, York UK . *</ref>
          <ref class="deo:BibliographicReference" id="38" confidence="possible" page="3" column="2">*This manual can be obtained by writing to the authors a t the address given above</ref>
        </ref-list>
        <region class="DoCO:FigureBox" id="Fx36">
          <image class="DoCO:Figure" src="62eu.page_003.image_03.png" thmb="62eu.page_003.image_03-thumb.png"/>
        </region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="39" page="3" column="2">About the Author s</h1>
        <region class="DoCO:TextChunk" id="40" page="3" column="2">Peter Wright has been a postdoctoral research fellow in th e Psychology Department at the University of York sinc e 1988. He is part of a team of researchers on joint projects with colleagues in the Department of Computer Science . He has published several papers with Andrew Monk (se e `about the author ' for Monk and Wright) in the area o f evaluation methods for HCI and also on formal approache s to interface design .</region>
        <outsider class="DoCO:TextBox" type="footer" id="41" page="3" column="2">SIGCHI Bulletin January 1991</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="42" page="3" column="2">57</outsider>
        <outsider class="DoCO:TextBox" type="footer" id="43" page="3" column="2">Volume 23, Number 1</outsider>
      </section>
    </body>
  </article>
</pdfx>
