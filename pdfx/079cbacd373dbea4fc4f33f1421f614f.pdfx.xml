<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>9c66dd08b83a250c89b3177df2b56abee58aca315e19f3b7c92e0e393ef8110a</job>
    <base_name>62qk</base_name>
    <doi>http://dx.doi.org/10.1109/isre.1993.324825</doi>
    <warning>Original PDF was found to be an image-based/possible OCR document. Output quality may be degraded.</warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="1">Analyzing Software Requirements Errors in Safety-Critical, Embedded Systems</article-title>
      </title-group>
      <contrib-group class="DoCO:ListOfAuthors">
        <contrib contrib-type="author">
          <name id="2">Robyn R. Lutz' Jet Propulsion Laboratory California Institute of Technology Pasadena</name>
        </contrib>
        <contrib contrib-type="author">
          <name id="3">CA</name>
        </contrib>
      </contrib-group>
      <abstract class="DoCO:Abstract" id="4">This paper analyzes the root causes of safety-related software errors i n safety-critical, embedded systems. The results show that software errors identified as potentially hazardous t o the s y s t e m tend t o be produced by different error mechanisms than non-safety- related software errors. Safety-related software errors are shown t o arise most commonly f r o m (1) discrep- ancies between the documented requirements specifications and the requirements needed f o r correct func- tioning of the s y s t e m and (2) misunderstandings of the soflware's interface with the rest of ihe system. The paper uses these results t o identify methods by which requirements errors can be prevented. The goal is t o reduce safety-related software errors and i o enhance the safety of complex, embedded systems.</abstract>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="5" confidence="possible" page="1" column="1">I. Introduction</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="22" confidence="possible" page="2" column="1">111. Analysis of Safety-Related Software Errors</h2>
          <region class="unknown" id="23" page="2" column="1">A. Overview of Classification Scheme</region>
          <region class="DoCO:TextChunk" id="25" page="2" column="1">An overview of the classification scheme follows, ad- justed to the needs of safety-critical, embedded software. See [<xref ref-type="bibr" rid="R16" id="24" class="deo:Reference">16</xref>] for additional details on how errors are categorized. An ongoing, multi-project investigation will address the issue of repeatability (do different analysts classify a given error in the same way?).</region>
          <region class="DoCO:TextChunk" id="26" confidence="possible" page="2" column="1">0 Program Faults (Documented Software Errors)</region>
          <region class="DoCO:FigureBox" id="Fx27">
            <image class="DoCO:Figure" src="62qk.page_002.image_02.png" thmb="62qk.page_002.image_02-thumb.png"/>
          </region>
          <region class="unknown" id="28" page="2" column="2">A. B.</region>
          <region class="unknown" id="29" page="2" column="2">C.</region>
          <region class="unknown" id="30" page="2" column="2">A. B1.</region>
          <region class="unknown" id="31" page="2" column="2">B2.</region>
          <region class="unknown" id="32" page="2" column="2">C1.</region>
          <region class="unknown" id="33" page="2" column="2">C2.</region>
          <region class="unknown" id="34" page="2" column="2">0</region>
          <region class="unknown" id="35" page="2" column="2">A.</region>
          <region class="unknown" id="36" page="2" column="2">B1.</region>
          <region class="unknown" id="37" page="2" column="2">B2.</region>
          <region class="unknown" id="38" page="2" column="2">C1.</region>
          <region class="DoCO:TextChunk" id="39" confidence="possible" page="2" column="2">Internal Faults (e.g., syntax) Interface Faults (interactions with other s y 5 tem components, such as transfer of data or control) Functional Faults (operating faults: omission or unnecessary operations; conditional faults: incorrect condition or limit values; behavioral faults: incorrect behavior, not conforming to requirements) 0 Human Errors (Root Causes) Coding or Editing Errors Communication Errors Within a Team (misunderstanding S/W interface specifications) Communication Errors Between Teams (misunderstanding H/W interface specifications or other team’s S/W specifications) Errors in Recognizing Requirements (misunderstanding specifications or problem dG main) Errors in Deploying Requirements (problems implementing or translating requirements into a design) Process Flaws (Flaws in Control of System Com- plexity + Inadequacies in Communication or De- velopment Methods) Inadequate Code Inspection and Testing Methods Inadequate Interface Specifications + Inadequate Communication (among S/W developers) Inadequate Interface Specifications + Inadequate Communication (between S/W and H/W developers) Requirements Not Identified or Understood + Incomplete Documentation Requirements Not Identified or Understood Inadequate Design</region>
          <region class="unknown" id="40" page="2" column="2">C2. +</region>
          <region class="DoCO:TextChunk" id="48" page="2" column="2">Clearly, the attribution of a key human error and a key process flaw to each software error oversimpli- fies the cause/effect relationship. However, the identification of these factors allows the characterization of safety-related software errors in a way that relates features of the development process and of the system under development to the safety consequences of those features. Similarly the association of each software error with a human error, while unrealistic (in <marker type="page" number="3"/><marker type="column" number="1"/><marker type="block"/> what sense is a failure to predict details of system behavior an error?), allows a useful association between human factors (such as misunderstanding the requirements or the underlying physical realities) and their safety-related consequences.<marker type="block"/> Safety-related software errors account for 56% of the total software errors for Voyager and 48% of the total software errors for Galileo discovered during integration and system testing. (The reader is referred to [<xref ref-type="bibr" rid="R14" id="45" class="deo:Reference">14</xref>] for the tables containing the error data.) Few internal faults (e.g., coding errors internal to a software module) were uncovered during integration and system testing. An examination of software errors found later during operations also shows few internal errors. It appears that these coding errors are being detected and corrected before system testing begins. They are thus not discussed further in this paper. At a high level of detail, safety-related and non- safety-related software errors display similar propor- tions of interface and functional faults. Functional faults (operating, conditional, or behavioral discrep- ancies with the functional requirements) are the most common kind of software error. Behavioral faults account for about half of all the functional faults on both spacecraft (52% on Voyager; 47% on Galileo). On Voyager fully half of the safety-related functional faults are attributable to behavioral faults (the software behaving incorrectly). On Galileo, a slightly greater percentage is due to operating faults (nearly always a required but omitted operation in the software) than to behavioral faults. Often the omitted operation involves the failure to perform adequate reasonableness checks on data input to a module. This frequently results in an error-recovery routine being called inappropriately. Conditional faults (nearly always an erroneous value on a condition or limit) tend to be safety-related on both spacecraft (73% total). Erroneous values (e.g., of deadbands or delay timers) often involve risk to the spacecraft by causing inappropriate triggering of an error-recovery response or by failing to trigger a needed response. The association between conditional faults and safety-related software errors emphasizes the importance of specifying the correct values for any data used in control decisions in safety-critical, embedded software. The analysis also identifies interface faults (incorrect interactions with other system components, such as the timing or transfer of data or control) as a significant problem (36% of the safety-related program<marker type="column" number="2"/><marker type="block"/> faults on Voyager; 19% on Galileo). Sect. IV below describes how the high incidence of interface faults in these complex, embedded systems contrasts with the low incidence of interface faults in earlier studies on simpler, standalone software.</region>
          <outsider class="DoCO:TextBox" type="footer" id="42" page="2" column="2">127</outsider>
          <region class="unknown" id="44" page="3" column="1">B. Program Faults</region>
          <region class="DoCO:FigureBox" id="Fx47">
            <image class="DoCO:Figure" src="62qk.page_003.image_03.png" thmb="62qk.page_003.image_03-thumb.png"/>
          </region>
          <region class="unknown" id="49" page="3" column="2">C. Program</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="50" confidence="possible" page="3" column="2">Relationships Between Faults and Root Causes</h2>
          <region class="unknown" id="51" page="3" column="2">hardware</region>
          <region class="unknown" id="52" page="3" column="2">as-</region>
          <region class="DoCO:TextChunk" id="64" page="3" column="2">The second step in the cause/effect analysis is to trace backwards in time to the human factors involved in the program faults that were discovered during integration and system testing. For interface faults, the major human factors are either communication errors within a development team or communication errors between a development team and other teams. In the latter case, a further distinction is made between misunderstanding hardware interface specifications and misunderstanding the interface specifications of other software Components. Communication errors between development teams (rather than within teams) is the leading cause of interface faults (93% on Voyager, 72% on G slileo). Safety-related interface faults are associated oterwhelmingly with communication errors between a development team and others (often between software developers and systems engineers), rather than with communication errors within a team. Significant differences in the distribution of fault causes between safety-related and non-safety-related interface faults appear. The primary cause of safety- related interface faults is misunderstood interface specifications (67% on Voyager; 48% on Galileo). Examples are faults caused by wrong sumptions about the initial state of relays or by unexpected heartbeat timing patterns in a particular operating mode. (On the other hand, the root causes of non-safety-related interface faults are distributed more evenly between misunderstood hardware specifications and misunderstood software specifications.) The profile of safety-related interface faults assembled here emphasizes the importance of understanding the software as a set of embedded components in a larger system. The primary cause of safety-related functional faults is errors in recognizing (understanding) the requirements (62% on Voyager, 79% on Galileo). Safety- related conditional faults, for example, are almost always caused by errors in recognizing requirements. (On the other hand, non-safety-related functional faults are more often caused by errors in deploying- implementing-the requirements.) <marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> In summary, difficulties with requirements is the key root cause of the safety-related software errors which have persisted until integration and system testing.<marker type="block"/> In tracing backwards from the program faults to their sources, features of the system-development process can be identified which facilitate or enable the occurrence of errors. Discrepancies between the difficulty of the problem and the means used to solve it may permit hazardous software errors to occur [<xref ref-type="bibr" rid="R4" id="58" class="deo:Reference">4</xref>]. The third step of the error analysis therefore ass* ciates a pair of process flaws with each program fault [<xref ref-type="bibr" rid="R16" id="59" class="deo:Reference">16</xref>]. The first element in the pair identifies a process flaw or inadequacy in the control of the system complexity (e.g., requirements which are not discovered until system testing). The second element of the pair identifies an associated process flaw in the communication or development methods used (e.g., impre- cise or unsystematic specification methods). The two elements of the process-flaw pair are closely related. Frequently, as will be seen in Sect. V, a solution to one flaw will provide a solution to the related flaw. For safety-related interface faults, the most common complexity-control flaw is interfaces not ade- quately identified or understood (56% on Voyager; on Galileo). The most common safety-related flaw in the communication or development methods used on Voyager is hardware behavior not documented (44%). On Galileo the most common safety-related flaws are lack of communication between H/W and S/W teams (35%) and interface specificaiions known but not documented or communicated (35%). Anomalous hardware behavior is a more significant factor in safety-related than in non-safety-related interface faults. It is often associated with interface design during system testing, another indication of a un- stable software product. There are significant variations in the process flaws that cause errors between the two spacecraft. Interface design during testing is involved in almost one- fifth of the safety-critical interface faults on Voyager, but in none of them on Galileo. This is because on Voyager a set of related hardware problems generated nearly half of the safety-related interface faults. On the other hand, the problem of interface specifications that are known but not documented is more common on Galileo. This is perhaps due to the increased complexity of the Galileo interfaces.<marker type="column" number="2"/><marker type="block"/> For functional faults, requirements not identified and requirements not understood are the most common complexity-control flaws. Safety-related functional faults are more likely than non-safety-related functional faults to be caused by requirements which<marker type="block"/> With regard to flaws in the communication or development methods, missing requirements are involved in nearly half the safety-related errors that involve recognizing requirements. Inadequate design is the most common flaw leading to errors in deploying requirements on Voyager. On Galileo, incomplete documenta- tion of requirements is as important a factor for safety- related errors, but not for non-safety-related errors. Imprecise or unsystematic specifications are more than twice as likely to be associated with safety- related functional faults as with non-safety-related functional faults. Similarly, unknown, undocumented, or wrong requirements are a greater cause of safety- related than of non-safety-related errors. These results suggest that the sources of safety- related software errors lie farther back in the software development process-in inadequate requirements- whereas the sources of non-safety-related errors more commonly involve inadequacies in the design phase.</region>
          <outsider class="DoCO:TextBox" type="footer" id="54" page="3" column="2">128</outsider>
          <region class="unknown" id="56" page="4" column="1">D. Relationships Between Root Causes and Process Flaws</region>
          <region class="unknown" id="57" page="4" column="1">87%</region>
          <region class="DoCO:FigureBox" id="Fx61">
            <image class="DoCO:Figure" src="62qk.page_004.image_04.png" thmb="62qk.page_004.image_04-thumb.png"/>
          </region>
          <region class="unknown" id="63" page="4" column="2">have not been identified.</region>
        </section>
      </section>
      <region class="DoCO:TextChunk" id="21" page="1" column="1">This paper examines 387 software errors uncovered during integration and system testing of two spacecraft, Voyager and Galileo. A software error is defined to be a software-related discrepancy between a com- puted, observed, or measured value or condition and the true, specified, or theoretically correct value or condition [ <xref ref-type="bibr" rid="R6" id="6" class="deo:Reference">6</xref>]. Each of these software errors was documented at the time of discovery by a form describing the problem or failure. The form also recorded the subsequent analysis and the corrective actions taken. As part of the standard procedure for correcting each reported software error, the failure effect of each is classified as negligible, significant, or catastrophic. Those classified as significant or catastrophic are investigated by a systems safety analyst as representing *Author's mailing address is Dept. of Computer Science, Iowa State University, Ames, IA 50011. The research described in this paper was carried out by the Jet Propulsion Laboratory, California Institute of Technology, under a contract with NASA.<marker type="column" number="2"/><marker type="block"/> potential safety hazards [<xref ref-type="bibr" rid="R13" id="8" class="deo:Reference">13</xref>]. For this study the 87 software errors on Voyager and 122 software errors on Galileo documented as having potentially significant or catastrophic effects are classified as safety-related.<marker type="block"/> The spacecrafts' software is safety-critical in that it monitors and controls components that can be involved in hazardous system behavior [ll]. The software must execute in a system context without con- tributing unacceptable risk.<marker type="block"/> Each spacecraft involves embedded software distributed on several different flight computers. Voyager has roughly 18,000 lines of source code; Galileo has over 22,000 [<xref ref-type="bibr" rid="R18" id="11" class="deo:Reference">18</xref>]. Embedded software is software that runs on a computer system that is integral to a larger system whose primary purpose is not computational [<xref ref-type="bibr" rid="R6" id="12" class="deo:Reference">6</xref>]. The software on both spacecraft is highly interactive in terms of the degree of message-passing among system components, the need to respond in real-time to monitoring of the hardware and environment, and the complex timing issues among parts of the system. The software development teams for each spacecraft involved multiple teams working for a period of years.<marker type="block"/> The purpose of this paper is to identify the ex- tent and ways in which the cause/effect relationships of safety-related software errors differ from the cause/effect relationships of non-safety-related software errors. In particular, the analysis shows that errors in identifying or understanding functional and interface requirements frequently lead to safety-related software errors. This distinction is used to identify methods by which the common causes of safety-related software errors can be targeted during development. The goal is to improve system safety by understanding and, where possible, removing, the prevalent sources of safety-related software errors.<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> The study described here characterizes the root causes of the safety-related software errors discovered during integration and system testing. The re- cent work by Nakajo and Kume on software error cause/effect relationships offers an appropriate frame- work for classifying the software errors [<xref ref-type="bibr" rid="R16" id="20" class="deo:Reference">16</xref>]. Their work is extended here to account for the additional complexities operative in large, safety-critical, embedded systems with evolving requirements driven by hardware and environmental issues. Nakajo and Kume’s classification scheme analyzes three points in the path from a software error backwards to its sources. This approach allows classification not only of the documented software error (called the program fault), but also of the earlier human error (the root cause, e.g., a misunderstanding of an interface specification), and, before that, of the process flaws that contribute to the likelihood of the error’s occurrence (e.g., inadequate communication between systems engineering and software development teams). The classification scheme thus leads backwards in time from the evident software error to an analysis of the root cause (usually a communication error or an error in recognizing or deploying requirements), to an analysis of the software development process. By comparing common error mechanisms for the software errors identified as potentially hazardous with those of the other software errors, the prevalent root causes of the safety-related errors are isolated. The classification of the sources of error is then applied here to determine countermeasures which may prevent similar error occurrences in other safety-critical, embedded systems. This paper thus uses the classification scheme to assemble an error profile of safety-related software errors and to identify development methods by which these sources of error can be controlled.</region>
      <outsider class="DoCO:TextBox" type="footer" id="15" page="1" column="2">126</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="16" page="1" column="2">0-8186-3120-1/92 $3.00 0</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="17" page="1" column="2">1992</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="18" page="1" column="2">IEEE</outsider>
      <region class="unknown" id="19" page="2" column="1">11. Methodology</region>
      <section class="deo:Results">
        <h1 class="DoCO:SectionTitle" id="65" page="4" column="2">Results with Previ-</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="66" confidence="possible" page="4" column="2">IV. Comparison of ous Work</h2>
          <region class="DoCO:TextChunk" id="81" page="4" column="2">Although software errors and their causes have been studied extensively, the current work differs from most of the prior investigations in the four following ways: 1) The software chosen to analyze in most studies is not embedded in a complex system as it is here. The consequence is that the role of interface specifications in controlling software hazards has been underesti- mated. 2) Unlike the current paper, most studies have analyzed fairly simple systems in familiar and well- understood application domains. Consequently, few software errors have occurred during system testing in most studies, leading to a gap in knowledge regarding the sources of these more-persistent and often more hazardous errors. 3 ) Most studies assume that the requirements specification is correct. On the spacecraft, as in many large, complex systems, the requirements evolve as knowledge of the system’s behavior and the problem domain evolve. Similarly, most studies assume that requirements are fixed by the time that systems testing begins. This leads to a underestimation of the impact <marker type="page" number="5"/><marker type="column" number="1"/><marker type="block"/> of unknown requirements on the scope and schedule of the later stages of the software development process. 4) The distinction between causes of safety-critical and non-safety-critical software errors has not been ade- quately investigated. Efforts to enhance system safety by specifically targeting the causes of safety-related errors, as distinguished from the causes of all errors, can take advantage of the distinct error mechanisms, as described in Sect. 5. A brief description of the scope and results of some related work is given below and compared with the results presented in this paper for safety-critical, embedded computer systems. Nakajo and Kume categorized 670 errors found during the software development of two firmware products for controlling measuring instruments and two software products for instrument measurement programs [<xref ref-type="bibr" rid="R16" id="69" class="deo:Reference">16</xref>]. Over 90% of the errors were either interface or functional faults, similar to the results reported here. While the key human error on the spacecraft involved communication between teams, the key human error in their study involved communication within a development team. Finally, the key process flaw that they identified was a lack of methods to record known interfaces and describe known functions. In the safety- critical, embedded software on the spacecraft, the flaw was more often a failure to identify or to understand the requirements. Ostrand and Weyuker categorized 173 errors found during the development and testing of an editor system [<xref ref-type="bibr" rid="R19" id="70" class="deo:Reference">19</xref>]. Only 2% of the errors were found during system testing, reflecting the simplicity and stability of the interfaces and requirements. Most of the errors (61%) were found-instead during function testing. Over half of these errors were caused by omissions, confirming the findings of the present study that omissions are a major cause of software errors. Schneidewind and Hoffmann [<xref ref-type="bibr" rid="R21" id="71" class="deo:Reference">21</xref>] categorized 173 errors found during the development of four small programs by a single programmer. Again, there were no significant interfaces with hardware and little system testing. The most frequent class of errors, other than coding and clerical, was design errors. All three of the most common design errors-extreme conditions neglected, forgotten cases or steps, and loop control errors- are also common functional faults on the spacecraft. Both the findings presented in [19, 211 and in this paper confirm the common experience that early in- sertion and late discovery of software errors maximizes the time and effort that the correction takes. Errors inserted in the requirements and design phases take<marker type="column" number="2"/><marker type="block"/> longer to find and correct than those inserted in later phases (because they tend to involve complex software structures). Errors discovered in the testing phase take longer to correct (because they tend t o be more complicated and difficult t o isolate). This is consistent with the results in [<xref ref-type="bibr" rid="R18" id="74" class="deo:Reference">18</xref>] indicating that more severe errors take longer t o discover than less severe errors during system-level testing. Furthermore, this effect was found to be more pronounced in more complex (as measured by lines of code) software. The work done by Endres is a direct forerunner of Nakajo and Kume’s in that Endres backtracked from the error type to the technical and organizational causes which led to each type of error [<xref ref-type="bibr" rid="R4" id="75" class="deo:Reference">4</xref>]. Moreover, because he studied the system testing of an operating system, the software’s interaction with the hardware was a source of concern. Endres noted the difficulty of precisely specifying functional demands on the systems before the programmer had seen their effect on the dynamic behavior of the system. His conclusion that better tools were needed to attack this problem still holds true eighteen years after he published his study. Of the 432 errors that Endres analyzed, 46% were errors in understanding or communicating the problem, or in the choice of a solution, 38% were errors in implementing a solution, and the remaining 16% were coding errors. These results are consistent with the finding here that software with many system interfaces displays a higher percentage of software errors involving understanding requirements or the system iniplications of alternative solutions. Eckhardt et al., in a study of software redundancy, analyzed the errors in twenty independent versions of a software component of an inertial navigation system [<xref ref-type="bibr" rid="R3" id="76" class="deo:Reference">3</xref>]. He found that inadequate understanding of the specifications or the underlying coordinate system was a major contributor to the program faults causing coincident failures. Addy, looking a t the types of errors that caused safety problems in a large, real-time control system, concluded that the design complexity inherent in such a system requires hidden interfaces which allow errors in non-critical software to affect safety-critical software [l]. This is consistent with Selby and Basili’s results when they analyzed 770 software errors during the updating of a library tool [<xref ref-type="bibr" rid="R22" id="77" class="deo:Reference">22</xref>]. Of the 46 errors documented in trouble reports, 70% were categorized as “wrong” and 28% as “missing.” They found that subsystems that were highly interactive with other subsystems had proportionately more errors than less interactive subsystems.<marker type="page" number="6"/><marker type="column" number="1"/><marker type="block"/> Leveson listed a set of common assumptions that are often false for control s y s t e m , resulting in software errors [ll]. Among these assumptions are that the software specification is correct, that it is possible to predict realistically the software’s execution environment (e.g., the existence of transients), and that it is possible to anticipate and specify correctly the software’s behavior under all possible circumstances. These assumptions tend to be true for the simple sys- t e m in which software errors have been analyzed to date and false for spacecraft and other large, safety- critical, embedded systems. Thus, while studies of software errors in simple systems can assist in understanding internal errors or some functional errors, they are of less help in understanding the causes of safety- related software errors, which tend heavily t o involve interfaces or recognition of complex requirements. Similarly, standard measures of the internal complexity of modules have limited usefulness in antici- pating software errors during system testing It is not the internal complexity of a module but the complexity of the module’s connection to its environment that yields the persistent, safety-related errors seen in the embedded systems here [<xref ref-type="bibr" rid="R8" id="80" class="deo:Reference">8</xref>].</region>
          <outsider class="DoCO:TextBox" type="footer" id="68" page="4" column="2">129</outsider>
          <region class="DoCO:FigureBox" id="Fx73">
            <image class="DoCO:Figure" src="62qk.page_005.image_05.png" thmb="62qk.page_005.image_05-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="footer" id="79" page="5" column="2">130</outsider>
          <region class="unknown" id="82" page="6" column="1">V.</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="83" confidence="possible" page="6" column="1">Conclusion A.</h2>
          <region class="unknown" id="84" page="6" column="1">Recommendations</region>
          <region class="DoCO:TextChunk" id="96" page="6" column="1">The results in Sect. I11 indicate that safety-related software errors tend to be produced by different error mechanisms than non-safety-related software errors. This means that system safety can be directly enhanced by targeting the causes of safety-related errors. Specifically, the following six recommendations emerge from our analysis of safety-related errors in complex, embedded systems. <marker type="block"/> 1. Focus on ihe interfaces between the software and the system i n analyzing the problem domain, since these interfaces are a m a j o r source of safety-related software errors. The traditional goal of the requirements analysis phase is the specification of the software’s external interface to the user. This definition is inadequate when the software is deeply embedded in larger systems such as spacecraft, advanced aircraft, air-traffic control units, or manufacturing process-control facil- ities. In such systems, the software is often physi- cally and logically distributed among various hardware components of the system.<marker type="column" number="2"/><marker type="block"/> Specifying the external behavior of the software (its transformation of software inputs into software outputs) only makes sense if the interfaces between the system inputs (e.g., environmental conditions, power transients) and the software inputs (e.g., mon- itor data) are also specified. Similarly, specifying the interfaces-specially the timing and dependency relationships-between the software outputs (e.g., star identification) and system outputs (e.g., closing the shutter on the star scanner) is necessary. [5, 101 System-development issues such as timing (real- time activities, interrupt handling, frequency of sen- sor data), hardware capabilities and limitations (stor- age capacity, power transients, noise characteristics), communication links (buffer and interface formats), and the expected operating environment (temper+ ture, pressure, radiation) need t o be reflected in the software requirements specifications because they are frequently sources of safety-critical software interface errors. Timing is a particularly difficult source of safety- related software interface faults since timing issues are so often integral to the functional correctness of safety-critical, embedded systems. Timing dependen- cies (e.g., how long input data is valid for making control decisions) should be included in the software interface specifications. Analytical models or simulations to understand system interfaces are particularly useful for complex, embedded systems.<marker type="block"/> 2. Identify safety-critical hazards early in the requirements analysis. These hazards are constraints on the possible designs and factors in any contemplated tradeoffs between safety (which tends to encourage software simplicity) and increased functionality (which tends to encourage software complexity) [lo, 221. Many of the safety-related software errors reported in Sect. I11 involve data objects or processes that would be targeted for special attention using hazard-detection techniques such as those described in [7, 111. Early detection of these safety-critical objects and increased attention to software operations involving them might forestall safety-related software errors involving them.<marker type="block"/> 3. Use formal specificofion techniques in addition to natural-language software requirements specifications. Lack of precision and incomplete requirements led to many of the safety-related software errors seen here. Enough detail is needed to cover all circumstances that can be envisioned (component failures, timing con-<marker type="page" number="7"/><marker type="column" number="1"/><marker type="block"/> straint violations, expired data) as well as to document all environmental assumptions (e.g., how close to the sun an instrument will point) and assumptions about other parts of the system (maximum transfer rate, consequences of race conditions or cycle slippage). The capability to describe dynamic events, the timing of process interactions in distinct computers, decen- tralized supervisory functions, etc., should be consid- ered in chooosing a formal method [2, 4, 5, 15, 20, 231.<marker type="block"/> Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member(s) of another development team knew. The goal is to be able to modularize responsibility in a development project without modularizing communication about the system under development. The identification and tracking of safety hazards in a system, for example, is clearly best done across team boundaries.<marker type="block"/> 5. A s requirements evolve, communicate the changes t o the development and test teams. This is both more important (because there are more requirements changes during design and testing) and more difficult (because of the number and size of the teams and the length of the development process) in a large, embedded system than in simpler systems. In analyzing the safety-related software errors, it is evident that the determination as to who needs to know about a change is often made incorrectly. Frequently, changes that appear to involve only one team or system component end up affecting other teams or components at some later date (sometimes as the result of incompatible changes in distinct units). There is also a need for faster distribution of changes that have been made, with the update stored so as to be fingertip accessible. CASE tools offer a possible solution to the difficulty of promulgating change without increasing paperwork. The prevalence of safety-related software errors involving misunderstood or missing requirements points up the inadequacy of consistency checks of requirements and code as a means of demonstrating system correctness [<xref ref-type="bibr" rid="Rlo" id="95" class="deo:Reference">lo</xref>]. Code that implements incorrect requirements is incorrect if it fails to provide needed system behavior. Similarly, generating test cases from misunderstood or missing requirements will not test system correctness. Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems. Alternative</region>
          <region class="DoCO:FigureBox" id="Fx87">
            <image class="DoCO:Figure" src="62qk.page_006.image_06.png" thmb="62qk.page_006.image_06-thumb.png"/>
          </region>
          <outsider class="DoCO:TextBox" type="footer" id="91" page="6" column="2">131</outsider>
          <region class="unknown" id="93" page="7" column="1">4. Promote informal communication among ieams.</region>
          <region class="DoCO:FigureBox" id="Fx97">
            <image class="DoCO:Figure" src="62qk.page_007.image_07.png" thmb="62qk.page_007.image_07-thumb.png"/>
          </region>
          <region class="unknown" id="98" page="7" column="2">as</region>
          <region class="DoCO:TextChunk" id="99" confidence="possible" page="7" column="2">validation and testing methods such those described in [9, 1 1 1 offer greater coverage.</region>
          <region class="unknown" id="101" page="7" column="2">6. Include requirements for “defensive design” [<xref ref-type="bibr" rid="R17" id="100" class="deo:Reference">17</xref>].</region>
          <region class="DoCO:TextChunk" id="103" page="7" column="2">Many of the safety-related software errors involve inadequate software responses to extreme conditions or extreme values. Anomalous hardware behavior, unanticipated states, events out of order, and obsolete data are all causes of safety-related software errors on the spacecraft. Run-time safety checks on the validity of input data, watchdog timers, delay timers, software fil- ters, software-imposed initialization conditions, additional exception handling, and assertion checking can be used to combat the many safety-critical software errors involving conditional and omission faults [ll]. Requirements for error-handling, overflow pro- tection, signal saturation limits, heartbeat and pulse frequency, maximum event duration, and system behavior under unexpected conditions can be added and traced into the design. Many safety-related functional faults involve error-recovery routines being invoked in- alppropriately because of erroneous limit values or bad data. Backward analysis from critical failures to possible ciiuses offers one check of how defensive the requirements and design are [<xref ref-type="bibr" rid="R12" id="102" class="deo:Reference">12</xref>]. Requirements specifications that account for worst-case scenarios, models that can predict the range of possible (rather than allowable) values, and simulations that can discover unexpected interactions before system testing contribute to the system’s defense against hazards.</region>
          <region class="unknown" id="104" page="7" column="2">E. Summary and Future</region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="105" confidence="possible" page="7" column="2">Work</h2>
          <region class="DoCO:TextChunk" id="108" page="7" column="2">In large, embedded systems such as the two spacecraft in this study, the software requirements change tiiroughout the software development process, even d tiring system testing. This is largely due to unantici- piited behavior, dynamic changes in the operating environment, and complex software/hardware and soft- ware/software interactions in the systems being devel- oped. Controlling requirement changes (and, hence, the scope and cost of development) is difficult since the changes are often prompted by an improved understanding of the software’s necessary interfaces with the physical components of the spacecraft in which it is embedded. Complex timing issues and hardware id- iosyncrasies often prompt changes to requirements or to design solutions. The analysis presented here of the cause/effect relationships of safety-related software errors pinpoints aspects of system complexity which merit additional <marker type="page" number="8"/><marker type="column" number="1"/><marker type="block"/> attention. Specifically, the results have shown that conditional faults (e.g., condition or limit values) are highly correlated with safety-related software errors. Operating faults (especially the omission of run-time reasonableness checks on data) are also highly correlated with safety-related software errors. Unknown, undocumented, or erroneous requirements frequently are associated with safety-related software errors as well. Hardware/software interfaces have been shown t o be a frequent trouble spot because of the lack of communication between teams. The results presented in this paper indicate a need for better methods to confront the real-world issues of developing safety-critical, embedded software in a complex, distributed system. Future work will be di- rected at incorporating knowledge of the distinct error mechanisms that produce safety-related software errors into the requirements analysis and validation processes. Work is also needed on specifying how these results can be used t o predict more precisely what features or combinations of factors in a safety-critical, embedded system are likely t o cause time-consuming and hazardous software errors.</region>
          <outsider class="DoCO:TextBox" type="footer" id="107" page="7" column="2">132</outsider>
        </section>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="109" confidence="possible" page="8" column="1">References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="110" confidence="possible" page="8" column="1">[I] E. A. Addy, “A Case Study on Isolation of Safety- Critical Software,“ in Proc 6th Annual Conf on Computer Assurance. NIST/IEEE, 1991, pp. 75-83.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="111" confidence="possible" page="8" column="1">[2] A. M. Davis, Software Requirements, Analysis and Specification. Englewood Cliffs, N.J.: Prentice Hall, 1990.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="112" confidence="possible" page="8" column="1">[3] D. E. Eckhardt, et al., “An Experimental Evaluation of Software Redundancy as a Strategy for Improving Reliability,” IEEE Trans Software Eng, 17, 7, July 1991, pp. 692-702.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="113" confidence="possible" page="8" column="1">[4] A. Endres, “An Analysis of Errors and Their Causes in Systems Programs,” IEEE Trans Software Eng, SE-1, 2, June 1975, pp. 140-149.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="114" confidence="possible" page="8" column="1">[5] E. M. Gray and R. H. Thayer, “Requirements,” in Aerospace Software Engineering, A Collection of Concepts. Ed. C. Anderson and M. Dorfman. Washington: AIAA, 1991, pp. 89-121.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="115" confidence="possible" page="8" column="1">[6] ANSI/IEEE Standard Glossary of Software Engineering Terminology. New York: IEEE, 1983.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="116" confidence="possible" page="8" column="1">[7] M. S. Jaffe et al., “Software Requirements Analysis for Real-Time Process-Control Systems,” IEEE Trans Software Eng, 17, 3, March 1991, pp. 241- 258.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="117" confidence="possible" page="8" column="1">[8] P. Jalote, An Integrated Approach to Software Engineering. New York: Springer-Verlag, 1991.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="119" confidence="possible" page="8" column="2">[9] J. C. Knight, ”Testing,” in Aerospace Software Engineering, A Collection of Concepts. Ed. c. Anderson and M. Dorfman. Washington: AIAA, 1991, pp. 135-1 59.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="120" confidence="possible" page="8" column="2">[lo] N. G . Leveson, “Safety,” in Aerospace Software Engineering, A Collection of Concepts. Ed. c. Anderson and M. Dorfman. Washington: AIAA, 1991, pp. 319-336.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="121" confidence="possible" page="8" column="2">[11] N. G. Leveson, “Software Safety in Embedded Computer Systems,” Commun ACM, Vol. 34, No. 2, Feb 1991, pp. 35-46.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="122" confidence="possible" page="8" column="2">[12] N. G . Leveson and P. R. Harvey, “Analyzing Software Safety,” IEEE Transactions on Software Engineering, SE-9, 5, Sept 1983, pp. 569-579.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="123" confidence="possible" page="8" column="2">[13] Karan L’Heureux, “Software Systems Safety Program RTOP, Phase A Report,” Internal Document, Jet Propulsion Laboratory, April 19, 1991.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="124" confidence="possible" page="8" column="2">[14] R. Lutz, “Analyzing Software Requirements Errors in Safety-Critical, Embedded Systems,” TR92-27, Dept. of Comp. Sci, Iowa State University.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="125" confidence="possible" page="8" column="2">[15] R. Lutz and J. S. K. Wong, “Detecting Unsafe Error Recovery Schedules,” IEEE Trans Software Eng, 18, 8, Aug, 1992, pp. 749-760.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="126" confidence="possible" page="8" column="2">[16] T. Nakajo and H. Kume, “A Case History Analysis of Software Error Cause-Effect Relationships,” IEEE Trans Software Eng 17, 8, Aug 1991, pp. 830-838.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="127" confidence="possible" page="8" column="2">[17] P. G. Neumann, ”The Computer-Related Risk of the Year: Weak Links and Correlated Events,” in Proc 6th Annual Conf on Computer Assurance. NIST/IEEE, 1991, pp. 5-8.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="128" confidence="possible" page="8" column="2">[18] A. P. Nikora, “Error Discovery Rate by Severity Category and Time to Repair Software Failures for Three JPL Flight Projects,” Internal Document, Jet Propulsion Laboratory, 1991.</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="129" confidence="possible" page="8" column="2">[19] T. J. Ostrand and E. J. Weyuker, “Collecting and Categorizing Software Error Data in an Industrial Environment,” The Journal of Systems and Software, 4, 1984, pp. 289-300.</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="130" confidence="possible" page="8" column="2">[20] Proc Berkeley Workshop on Tempoml and Real- Time Specification. Eds. P. B. Ladkin and F. H. Vogt. Berkeley, CA: International Computer Science Institute, 1990, TR-90-060.</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="131" confidence="possible" page="8" column="2">[21] N. F. Schneidewind and H.-M. Hoffmann, “An Ex- periment in Software Error Data Collection and Analysis,” IEEE Trans Software Eng, S E 5 , 3, May 1979, pp. 276-286.</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="132" confidence="possible" page="8" column="2">[22] R. W. Selby and V. R. Basili, “Analyzing Error- Prone System Structure,” IEEE Trans Software Eng 17, 2, Febr 1991, pp. 141-152.</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="133" confidence="possible" page="8" column="2">[23] J. M. Wing, “A Specifier’s Introduction to Formal Methods,” Computer, Vol. 23, Sept 1990, pp. 8-26.</ref>
        </ref-list>
        <region class="DoCO:FigureBox" id="Fx118">
          <image class="DoCO:Figure" src="62qk.page_008.image_08.png" thmb="62qk.page_008.image_08-thumb.png"/>
        </region>
        <outsider class="DoCO:TextBox" type="footer" id="134" page="8" column="2">133</outsider>
      </section>
    </body>
  </article>
</pdfx>
